{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cats-Dogs VGG16 with RMSprop Opt (BEST MODEL NEW).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee3Kh4do_phs"
      },
      "source": [
        "## **Cats-Dogs VGG16 with RSMProp Optimizer**\n",
        "\n",
        "**Benedictus Bayu Pramudhito**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2BSFWW6TG8A",
        "outputId": "c7b7c31f-b998-4877-ee57-dcea9eff7fb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1hRjyLbTLO5"
      },
      "source": [
        "zip_path = '/content/drive/My\\ Drive/cats-dogs.zip'\n",
        "\n",
        "!cp {zip_path} /content/\n",
        "\n",
        "!cd /content/\n",
        "\n",
        "!unzip -q /content/cats-dogs.zip -d /content\n",
        "\n",
        "!rm /content/cats-dogs.zip"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp-KicWtTXSE"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop, Adadelta, Adagrad, Adam, Adamax\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHya6gG-Tf3U"
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POs6BVnW_2Gi"
      },
      "source": [
        "**Image Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5ap0U5FTkxJ",
        "outputId": "dbfdb9a1-e36e-446b-9b7e-0fe5d44065ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "dataset_dir = '/content/'\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "train_datagen = ImageDataGenerator(rescale=1.0/255.0, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "\n",
        "train_augmented_iteration = train_datagen.flow_from_directory(os.path.join(dataset_dir, 'train'), class_mode='binary', batch_size=128, target_size=(224, 224))\n",
        "test_augmented_iteration = test_datagen.flow_from_directory(os.path.join(dataset_dir, 'test'), class_mode='binary', batch_size=128, target_size=(224, 224))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 12749 images belonging to 2 classes.\n",
            "Found 2252 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsZi10F1TmNK"
      },
      "source": [
        "from tensorflow.keras.applications import vgg16\n",
        "from tensorflow.keras.layers import Dropout"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDEBtQOKTn5D",
        "outputId": "2de51fab-b167-4cfd-d885-a1a6fe475707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "vgg_conv = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "for layer in vgg_conv.layers[:]:\n",
        "  layer.trainable = False\n",
        "\n",
        "fine_tuned_model = Sequential()\n",
        "\n",
        "fine_tuned_model.add(vgg_conv)\n",
        "\n",
        "fine_tuned_model.add(Flatten())\n",
        "fine_tuned_model.add(Dense(1024, activation='relu'))\n",
        "fine_tuned_model.add(Dropout(0.5))\n",
        "fine_tuned_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "fine_tuned_model.summary()\n",
        "\n",
        "opt = RMSprop(learning_rate=0.0001, momentum=0.9)\n",
        "\n",
        "fine_tuned_model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              25691136  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 40,406,849\n",
            "Trainable params: 25,692,161\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuqR0B8HTrkR",
        "outputId": "1c9d8bd8-d4cb-44e1-ef85-8f65d7154309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history_deep_model = fine_tuned_model.fit_generator(train_augmented_iteration, steps_per_epoch=len(train_augmented_iteration), validation_data=test_augmented_iteration, validation_steps=len(test_augmented_iteration), epochs=20)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-8-a996c6331420>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/50\n",
            "100/100 [==============================] - 155s 2s/step - loss: 0.6988 - accuracy: 0.8074 - val_loss: 0.1958 - val_accuracy: 0.9179\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.2570 - accuracy: 0.8900 - val_loss: 0.1863 - val_accuracy: 0.9165\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.2258 - accuracy: 0.9020 - val_loss: 0.1654 - val_accuracy: 0.9290\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.2140 - accuracy: 0.9095 - val_loss: 0.1593 - val_accuracy: 0.9334\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.2067 - accuracy: 0.9129 - val_loss: 0.1834 - val_accuracy: 0.9307\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.2020 - accuracy: 0.9136 - val_loss: 0.1580 - val_accuracy: 0.9325\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.1996 - accuracy: 0.9140 - val_loss: 0.1819 - val_accuracy: 0.9285\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.1898 - accuracy: 0.9203 - val_loss: 0.1538 - val_accuracy: 0.9356\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.1861 - accuracy: 0.9241 - val_loss: 0.1476 - val_accuracy: 0.9387\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.1864 - accuracy: 0.9237 - val_loss: 0.1609 - val_accuracy: 0.9369\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.1802 - accuracy: 0.9244 - val_loss: 0.1454 - val_accuracy: 0.9378\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.1834 - accuracy: 0.9251 - val_loss: 0.1516 - val_accuracy: 0.9378\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.1686 - accuracy: 0.9310 - val_loss: 0.1469 - val_accuracy: 0.9352\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 155s 2s/step - loss: 0.1782 - accuracy: 0.9260 - val_loss: 0.1597 - val_accuracy: 0.9334\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 155s 2s/step - loss: 0.1750 - accuracy: 0.9292 - val_loss: 0.1497 - val_accuracy: 0.9325\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 155s 2s/step - loss: 0.1642 - accuracy: 0.9322 - val_loss: 0.1784 - val_accuracy: 0.9276\n",
            "Epoch 17/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.1674 - accuracy: 0.9324 - val_loss: 0.1601 - val_accuracy: 0.9356\n",
            "Epoch 18/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.1599 - accuracy: 0.9327 - val_loss: 0.1578 - val_accuracy: 0.9378\n",
            "Epoch 19/50\n",
            "100/100 [==============================] - 155s 2s/step - loss: 0.1626 - accuracy: 0.9351 - val_loss: 0.1539 - val_accuracy: 0.9374\n",
            "Epoch 20/50\n",
            "100/100 [==============================] - 155s 2s/step - loss: 0.1606 - accuracy: 0.9324 - val_loss: 0.1524 - val_accuracy: 0.9374\n",
            "Epoch 21/50\n",
            "100/100 [==============================] - 155s 2s/step - loss: 0.1605 - accuracy: 0.9348 - val_loss: 0.1472 - val_accuracy: 0.9374\n",
            "Epoch 22/50\n",
            "100/100 [==============================] - 155s 2s/step - loss: 0.1554 - accuracy: 0.9357 - val_loss: 0.1554 - val_accuracy: 0.9374\n",
            "Epoch 23/50\n",
            "100/100 [==============================] - 155s 2s/step - loss: 0.1529 - accuracy: 0.9343 - val_loss: 0.1619 - val_accuracy: 0.9347\n",
            "Epoch 24/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.1519 - accuracy: 0.9363 - val_loss: 0.1650 - val_accuracy: 0.9392\n",
            "Epoch 25/50\n",
            "100/100 [==============================] - 155s 2s/step - loss: 0.1446 - accuracy: 0.9408 - val_loss: 0.1565 - val_accuracy: 0.9405\n",
            "Epoch 26/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.1484 - accuracy: 0.9408 - val_loss: 0.1680 - val_accuracy: 0.9325\n",
            "Epoch 27/50\n",
            "100/100 [==============================] - 154s 2s/step - loss: 0.1581 - accuracy: 0.9358 - val_loss: 0.1667 - val_accuracy: 0.9387\n",
            "Epoch 28/50\n",
            "100/100 [==============================] - 154s 2s/step - loss: 0.1500 - accuracy: 0.9387 - val_loss: 0.1616 - val_accuracy: 0.9356\n",
            "Epoch 29/50\n",
            "100/100 [==============================] - 154s 2s/step - loss: 0.1506 - accuracy: 0.9393 - val_loss: 0.1486 - val_accuracy: 0.9383\n",
            "Epoch 30/50\n",
            "100/100 [==============================] - 154s 2s/step - loss: 0.1499 - accuracy: 0.9393 - val_loss: 0.1589 - val_accuracy: 0.9347\n",
            "Epoch 31/50\n",
            "100/100 [==============================] - 154s 2s/step - loss: 0.1540 - accuracy: 0.9378 - val_loss: 0.1610 - val_accuracy: 0.9401\n",
            "Epoch 32/50\n",
            "100/100 [==============================] - 154s 2s/step - loss: 0.1429 - accuracy: 0.9416 - val_loss: 0.1668 - val_accuracy: 0.9383\n",
            "Epoch 33/50\n",
            "100/100 [==============================] - 154s 2s/step - loss: 0.1442 - accuracy: 0.9402 - val_loss: 0.1743 - val_accuracy: 0.9387\n",
            "Epoch 34/50\n",
            "100/100 [==============================] - 154s 2s/step - loss: 0.1480 - accuracy: 0.9401 - val_loss: 0.1676 - val_accuracy: 0.9418\n",
            "Epoch 35/50\n",
            "100/100 [==============================] - 153s 2s/step - loss: 0.1442 - accuracy: 0.9423 - val_loss: 0.1705 - val_accuracy: 0.9392\n",
            "Epoch 36/50\n",
            "100/100 [==============================] - 153s 2s/step - loss: 0.1365 - accuracy: 0.9425 - val_loss: 0.1569 - val_accuracy: 0.9401\n",
            "Epoch 37/50\n",
            "100/100 [==============================] - 153s 2s/step - loss: 0.1332 - accuracy: 0.9481 - val_loss: 0.1668 - val_accuracy: 0.9409\n",
            "Epoch 38/50\n",
            "100/100 [==============================] - 153s 2s/step - loss: 0.1308 - accuracy: 0.9470 - val_loss: 0.1698 - val_accuracy: 0.9387\n",
            "Epoch 39/50\n",
            "100/100 [==============================] - 153s 2s/step - loss: 0.1293 - accuracy: 0.9477 - val_loss: 0.1706 - val_accuracy: 0.9352\n",
            "Epoch 40/50\n",
            "100/100 [==============================] - 153s 2s/step - loss: 0.1357 - accuracy: 0.9442 - val_loss: 0.1742 - val_accuracy: 0.9418\n",
            "Epoch 41/50\n",
            "100/100 [==============================] - 153s 2s/step - loss: 0.1266 - accuracy: 0.9489 - val_loss: 0.1667 - val_accuracy: 0.9418\n",
            "Epoch 42/50\n",
            "100/100 [==============================] - 153s 2s/step - loss: 0.1263 - accuracy: 0.9500 - val_loss: 0.1490 - val_accuracy: 0.9414\n",
            "Epoch 43/50\n",
            "100/100 [==============================] - 153s 2s/step - loss: 0.1341 - accuracy: 0.9448 - val_loss: 0.1570 - val_accuracy: 0.9392\n",
            "Epoch 44/50\n",
            "100/100 [==============================] - 154s 2s/step - loss: 0.1316 - accuracy: 0.9463 - val_loss: 0.1540 - val_accuracy: 0.9423\n",
            "Epoch 45/50\n",
            "100/100 [==============================] - 154s 2s/step - loss: 0.1320 - accuracy: 0.9460 - val_loss: 0.1773 - val_accuracy: 0.9423\n",
            "Epoch 46/50\n",
            "100/100 [==============================] - 154s 2s/step - loss: 0.1246 - accuracy: 0.9482 - val_loss: 0.1711 - val_accuracy: 0.9445\n",
            "Epoch 47/50\n",
            "100/100 [==============================] - 154s 2s/step - loss: 0.1247 - accuracy: 0.9515 - val_loss: 0.1988 - val_accuracy: 0.9387\n",
            "Epoch 48/50\n",
            "100/100 [==============================] - 153s 2s/step - loss: 0.1257 - accuracy: 0.9487 - val_loss: 0.1555 - val_accuracy: 0.9423\n",
            "Epoch 49/50\n",
            "100/100 [==============================] - 154s 2s/step - loss: 0.1245 - accuracy: 0.9499 - val_loss: 0.1772 - val_accuracy: 0.9374\n",
            "Epoch 50/50\n",
            "100/100 [==============================] - 154s 2s/step - loss: 0.1192 - accuracy: 0.9522 - val_loss: 0.1843 - val_accuracy: 0.9436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zXjL5H-Tw7E",
        "outputId": "7ec43571-2d7b-4dce-b687-a614b4918906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plot_loss(history_deep_model)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCq6LSD18x1N"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "history_dataframe = pd.DataFrame(history_deep_model.history)\n",
        "history_dataframe['epoch'] = history_deep_model.epoch"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNDUnYK_8_Iz",
        "outputId": "db3e8642-74f4-4663-e63e-ea78338f9fdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history_dataframe.sort_values(by='val_loss', ascending=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "      <th>epoch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.180159</td>\n",
              "      <td>0.924386</td>\n",
              "      <td>0.145402</td>\n",
              "      <td>0.937833</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.168574</td>\n",
              "      <td>0.930975</td>\n",
              "      <td>0.146876</td>\n",
              "      <td>0.935169</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.160531</td>\n",
              "      <td>0.934818</td>\n",
              "      <td>0.147175</td>\n",
              "      <td>0.937389</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.186091</td>\n",
              "      <td>0.924073</td>\n",
              "      <td>0.147609</td>\n",
              "      <td>0.938721</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.150565</td>\n",
              "      <td>0.939289</td>\n",
              "      <td>0.148595</td>\n",
              "      <td>0.938277</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.126340</td>\n",
              "      <td>0.950035</td>\n",
              "      <td>0.148986</td>\n",
              "      <td>0.941385</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.175032</td>\n",
              "      <td>0.929249</td>\n",
              "      <td>0.149725</td>\n",
              "      <td>0.932504</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.183445</td>\n",
              "      <td>0.925092</td>\n",
              "      <td>0.151578</td>\n",
              "      <td>0.937833</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.160598</td>\n",
              "      <td>0.932387</td>\n",
              "      <td>0.152396</td>\n",
              "      <td>0.937389</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.189817</td>\n",
              "      <td>0.920307</td>\n",
              "      <td>0.153757</td>\n",
              "      <td>0.935613</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.162583</td>\n",
              "      <td>0.935132</td>\n",
              "      <td>0.153929</td>\n",
              "      <td>0.937389</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.131592</td>\n",
              "      <td>0.946270</td>\n",
              "      <td>0.154007</td>\n",
              "      <td>0.942274</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.155412</td>\n",
              "      <td>0.935681</td>\n",
              "      <td>0.155436</td>\n",
              "      <td>0.937389</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.125739</td>\n",
              "      <td>0.948702</td>\n",
              "      <td>0.155468</td>\n",
              "      <td>0.942274</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.144578</td>\n",
              "      <td>0.940780</td>\n",
              "      <td>0.156540</td>\n",
              "      <td>0.940497</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.136452</td>\n",
              "      <td>0.942505</td>\n",
              "      <td>0.156869</td>\n",
              "      <td>0.940053</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.134055</td>\n",
              "      <td>0.944780</td>\n",
              "      <td>0.156966</td>\n",
              "      <td>0.939165</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.159863</td>\n",
              "      <td>0.932701</td>\n",
              "      <td>0.157804</td>\n",
              "      <td>0.937833</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.202012</td>\n",
              "      <td>0.913562</td>\n",
              "      <td>0.158047</td>\n",
              "      <td>0.932504</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.149919</td>\n",
              "      <td>0.939289</td>\n",
              "      <td>0.158883</td>\n",
              "      <td>0.934725</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.213998</td>\n",
              "      <td>0.909483</td>\n",
              "      <td>0.159261</td>\n",
              "      <td>0.933393</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.178194</td>\n",
              "      <td>0.925955</td>\n",
              "      <td>0.159737</td>\n",
              "      <td>0.933393</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.167427</td>\n",
              "      <td>0.932387</td>\n",
              "      <td>0.160147</td>\n",
              "      <td>0.935613</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.186445</td>\n",
              "      <td>0.923680</td>\n",
              "      <td>0.160865</td>\n",
              "      <td>0.936945</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.154015</td>\n",
              "      <td>0.937799</td>\n",
              "      <td>0.160982</td>\n",
              "      <td>0.940053</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.149983</td>\n",
              "      <td>0.938740</td>\n",
              "      <td>0.161630</td>\n",
              "      <td>0.935613</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.152945</td>\n",
              "      <td>0.934348</td>\n",
              "      <td>0.161946</td>\n",
              "      <td>0.934725</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.151855</td>\n",
              "      <td>0.936309</td>\n",
              "      <td>0.165031</td>\n",
              "      <td>0.939165</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.225795</td>\n",
              "      <td>0.902032</td>\n",
              "      <td>0.165420</td>\n",
              "      <td>0.928952</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.158100</td>\n",
              "      <td>0.935760</td>\n",
              "      <td>0.166682</td>\n",
              "      <td>0.938721</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.126644</td>\n",
              "      <td>0.948937</td>\n",
              "      <td>0.166730</td>\n",
              "      <td>0.941830</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.133247</td>\n",
              "      <td>0.948074</td>\n",
              "      <td>0.166774</td>\n",
              "      <td>0.940941</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.142855</td>\n",
              "      <td>0.941642</td>\n",
              "      <td>0.166817</td>\n",
              "      <td>0.938277</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.147982</td>\n",
              "      <td>0.940074</td>\n",
              "      <td>0.167636</td>\n",
              "      <td>0.941830</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.148408</td>\n",
              "      <td>0.940780</td>\n",
              "      <td>0.168019</td>\n",
              "      <td>0.932504</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.130784</td>\n",
              "      <td>0.946976</td>\n",
              "      <td>0.169843</td>\n",
              "      <td>0.938721</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.144219</td>\n",
              "      <td>0.942270</td>\n",
              "      <td>0.170461</td>\n",
              "      <td>0.939165</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.129282</td>\n",
              "      <td>0.947682</td>\n",
              "      <td>0.170561</td>\n",
              "      <td>0.935169</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.124632</td>\n",
              "      <td>0.948153</td>\n",
              "      <td>0.171144</td>\n",
              "      <td>0.944494</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.135727</td>\n",
              "      <td>0.944152</td>\n",
              "      <td>0.174203</td>\n",
              "      <td>0.941830</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.144232</td>\n",
              "      <td>0.940231</td>\n",
              "      <td>0.174333</td>\n",
              "      <td>0.938721</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.124513</td>\n",
              "      <td>0.949878</td>\n",
              "      <td>0.177182</td>\n",
              "      <td>0.937389</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.132012</td>\n",
              "      <td>0.945957</td>\n",
              "      <td>0.177295</td>\n",
              "      <td>0.942274</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.164185</td>\n",
              "      <td>0.932152</td>\n",
              "      <td>0.178372</td>\n",
              "      <td>0.927620</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.199604</td>\n",
              "      <td>0.913954</td>\n",
              "      <td>0.181909</td>\n",
              "      <td>0.928508</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.206714</td>\n",
              "      <td>0.912856</td>\n",
              "      <td>0.183376</td>\n",
              "      <td>0.930728</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.119195</td>\n",
              "      <td>0.952232</td>\n",
              "      <td>0.184349</td>\n",
              "      <td>0.943606</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.257044</td>\n",
              "      <td>0.890031</td>\n",
              "      <td>0.186312</td>\n",
              "      <td>0.916519</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.698785</td>\n",
              "      <td>0.807436</td>\n",
              "      <td>0.195800</td>\n",
              "      <td>0.917851</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.124723</td>\n",
              "      <td>0.951526</td>\n",
              "      <td>0.198836</td>\n",
              "      <td>0.938721</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        loss  accuracy  val_loss  val_accuracy  epoch\n",
              "10  0.180159  0.924386  0.145402      0.937833     10\n",
              "12  0.168574  0.930975  0.146876      0.935169     12\n",
              "20  0.160531  0.934818  0.147175      0.937389     20\n",
              "8   0.186091  0.924073  0.147609      0.938721      8\n",
              "28  0.150565  0.939289  0.148595      0.938277     28\n",
              "41  0.126340  0.950035  0.148986      0.941385     41\n",
              "14  0.175032  0.929249  0.149725      0.932504     14\n",
              "11  0.183445  0.925092  0.151578      0.937833     11\n",
              "19  0.160598  0.932387  0.152396      0.937389     19\n",
              "7   0.189817  0.920307  0.153757      0.935613      7\n",
              "18  0.162583  0.935132  0.153929      0.937389     18\n",
              "43  0.131592  0.946270  0.154007      0.942274     43\n",
              "21  0.155412  0.935681  0.155436      0.937389     21\n",
              "47  0.125739  0.948702  0.155468      0.942274     47\n",
              "24  0.144578  0.940780  0.156540      0.940497     24\n",
              "35  0.136452  0.942505  0.156869      0.940053     35\n",
              "42  0.134055  0.944780  0.156966      0.939165     42\n",
              "17  0.159863  0.932701  0.157804      0.937833     17\n",
              "5   0.202012  0.913562  0.158047      0.932504      5\n",
              "29  0.149919  0.939289  0.158883      0.934725     29\n",
              "3   0.213998  0.909483  0.159261      0.933393      3\n",
              "13  0.178194  0.925955  0.159737      0.933393     13\n",
              "16  0.167427  0.932387  0.160147      0.935613     16\n",
              "9   0.186445  0.923680  0.160865      0.936945      9\n",
              "30  0.154015  0.937799  0.160982      0.940053     30\n",
              "27  0.149983  0.938740  0.161630      0.935613     27\n",
              "22  0.152945  0.934348  0.161946      0.934725     22\n",
              "23  0.151855  0.936309  0.165031      0.939165     23\n",
              "2   0.225795  0.902032  0.165420      0.928952      2\n",
              "26  0.158100  0.935760  0.166682      0.938721     26\n",
              "40  0.126644  0.948937  0.166730      0.941830     40\n",
              "36  0.133247  0.948074  0.166774      0.940941     36\n",
              "31  0.142855  0.941642  0.166817      0.938277     31\n",
              "33  0.147982  0.940074  0.167636      0.941830     33\n",
              "25  0.148408  0.940780  0.168019      0.932504     25\n",
              "37  0.130784  0.946976  0.169843      0.938721     37\n",
              "34  0.144219  0.942270  0.170461      0.939165     34\n",
              "38  0.129282  0.947682  0.170561      0.935169     38\n",
              "45  0.124632  0.948153  0.171144      0.944494     45\n",
              "39  0.135727  0.944152  0.174203      0.941830     39\n",
              "32  0.144232  0.940231  0.174333      0.938721     32\n",
              "48  0.124513  0.949878  0.177182      0.937389     48\n",
              "44  0.132012  0.945957  0.177295      0.942274     44\n",
              "15  0.164185  0.932152  0.178372      0.927620     15\n",
              "6   0.199604  0.913954  0.181909      0.928508      6\n",
              "4   0.206714  0.912856  0.183376      0.930728      4\n",
              "49  0.119195  0.952232  0.184349      0.943606     49\n",
              "1   0.257044  0.890031  0.186312      0.916519      1\n",
              "0   0.698785  0.807436  0.195800      0.917851      0\n",
              "46  0.124723  0.951526  0.198836      0.938721     46"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdIzacio9FZ_",
        "outputId": "cdc90458-0d4c-435c-84b8-45164325d0a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history_dataframe.sort_values(by='val_accuracy', ascending=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "      <th>epoch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.124632</td>\n",
              "      <td>0.948153</td>\n",
              "      <td>0.171144</td>\n",
              "      <td>0.944494</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.119195</td>\n",
              "      <td>0.952232</td>\n",
              "      <td>0.184349</td>\n",
              "      <td>0.943606</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.132012</td>\n",
              "      <td>0.945957</td>\n",
              "      <td>0.177295</td>\n",
              "      <td>0.942274</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.125739</td>\n",
              "      <td>0.948702</td>\n",
              "      <td>0.155468</td>\n",
              "      <td>0.942274</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.131592</td>\n",
              "      <td>0.946270</td>\n",
              "      <td>0.154007</td>\n",
              "      <td>0.942274</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.135727</td>\n",
              "      <td>0.944152</td>\n",
              "      <td>0.174203</td>\n",
              "      <td>0.941830</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.147982</td>\n",
              "      <td>0.940074</td>\n",
              "      <td>0.167636</td>\n",
              "      <td>0.941830</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.126644</td>\n",
              "      <td>0.948937</td>\n",
              "      <td>0.166730</td>\n",
              "      <td>0.941830</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.126340</td>\n",
              "      <td>0.950035</td>\n",
              "      <td>0.148986</td>\n",
              "      <td>0.941385</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.133247</td>\n",
              "      <td>0.948074</td>\n",
              "      <td>0.166774</td>\n",
              "      <td>0.940941</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.144578</td>\n",
              "      <td>0.940780</td>\n",
              "      <td>0.156540</td>\n",
              "      <td>0.940497</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.136452</td>\n",
              "      <td>0.942505</td>\n",
              "      <td>0.156869</td>\n",
              "      <td>0.940053</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.154015</td>\n",
              "      <td>0.937799</td>\n",
              "      <td>0.160982</td>\n",
              "      <td>0.940053</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.144219</td>\n",
              "      <td>0.942270</td>\n",
              "      <td>0.170461</td>\n",
              "      <td>0.939165</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.134055</td>\n",
              "      <td>0.944780</td>\n",
              "      <td>0.156966</td>\n",
              "      <td>0.939165</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.151855</td>\n",
              "      <td>0.936309</td>\n",
              "      <td>0.165031</td>\n",
              "      <td>0.939165</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.130784</td>\n",
              "      <td>0.946976</td>\n",
              "      <td>0.169843</td>\n",
              "      <td>0.938721</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.144232</td>\n",
              "      <td>0.940231</td>\n",
              "      <td>0.174333</td>\n",
              "      <td>0.938721</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.158100</td>\n",
              "      <td>0.935760</td>\n",
              "      <td>0.166682</td>\n",
              "      <td>0.938721</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.186091</td>\n",
              "      <td>0.924073</td>\n",
              "      <td>0.147609</td>\n",
              "      <td>0.938721</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.124723</td>\n",
              "      <td>0.951526</td>\n",
              "      <td>0.198836</td>\n",
              "      <td>0.938721</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.150565</td>\n",
              "      <td>0.939289</td>\n",
              "      <td>0.148595</td>\n",
              "      <td>0.938277</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.142855</td>\n",
              "      <td>0.941642</td>\n",
              "      <td>0.166817</td>\n",
              "      <td>0.938277</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.180159</td>\n",
              "      <td>0.924386</td>\n",
              "      <td>0.145402</td>\n",
              "      <td>0.937833</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.183445</td>\n",
              "      <td>0.925092</td>\n",
              "      <td>0.151578</td>\n",
              "      <td>0.937833</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.159863</td>\n",
              "      <td>0.932701</td>\n",
              "      <td>0.157804</td>\n",
              "      <td>0.937833</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.124513</td>\n",
              "      <td>0.949878</td>\n",
              "      <td>0.177182</td>\n",
              "      <td>0.937389</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.155412</td>\n",
              "      <td>0.935681</td>\n",
              "      <td>0.155436</td>\n",
              "      <td>0.937389</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.160598</td>\n",
              "      <td>0.932387</td>\n",
              "      <td>0.152396</td>\n",
              "      <td>0.937389</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.162583</td>\n",
              "      <td>0.935132</td>\n",
              "      <td>0.153929</td>\n",
              "      <td>0.937389</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.160531</td>\n",
              "      <td>0.934818</td>\n",
              "      <td>0.147175</td>\n",
              "      <td>0.937389</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.186445</td>\n",
              "      <td>0.923680</td>\n",
              "      <td>0.160865</td>\n",
              "      <td>0.936945</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.149983</td>\n",
              "      <td>0.938740</td>\n",
              "      <td>0.161630</td>\n",
              "      <td>0.935613</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.167427</td>\n",
              "      <td>0.932387</td>\n",
              "      <td>0.160147</td>\n",
              "      <td>0.935613</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.189817</td>\n",
              "      <td>0.920307</td>\n",
              "      <td>0.153757</td>\n",
              "      <td>0.935613</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.168574</td>\n",
              "      <td>0.930975</td>\n",
              "      <td>0.146876</td>\n",
              "      <td>0.935169</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.129282</td>\n",
              "      <td>0.947682</td>\n",
              "      <td>0.170561</td>\n",
              "      <td>0.935169</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.149919</td>\n",
              "      <td>0.939289</td>\n",
              "      <td>0.158883</td>\n",
              "      <td>0.934725</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.152945</td>\n",
              "      <td>0.934348</td>\n",
              "      <td>0.161946</td>\n",
              "      <td>0.934725</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.213998</td>\n",
              "      <td>0.909483</td>\n",
              "      <td>0.159261</td>\n",
              "      <td>0.933393</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.178194</td>\n",
              "      <td>0.925955</td>\n",
              "      <td>0.159737</td>\n",
              "      <td>0.933393</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.148408</td>\n",
              "      <td>0.940780</td>\n",
              "      <td>0.168019</td>\n",
              "      <td>0.932504</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.175032</td>\n",
              "      <td>0.929249</td>\n",
              "      <td>0.149725</td>\n",
              "      <td>0.932504</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.202012</td>\n",
              "      <td>0.913562</td>\n",
              "      <td>0.158047</td>\n",
              "      <td>0.932504</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.206714</td>\n",
              "      <td>0.912856</td>\n",
              "      <td>0.183376</td>\n",
              "      <td>0.930728</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.225795</td>\n",
              "      <td>0.902032</td>\n",
              "      <td>0.165420</td>\n",
              "      <td>0.928952</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.199604</td>\n",
              "      <td>0.913954</td>\n",
              "      <td>0.181909</td>\n",
              "      <td>0.928508</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.164185</td>\n",
              "      <td>0.932152</td>\n",
              "      <td>0.178372</td>\n",
              "      <td>0.927620</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.698785</td>\n",
              "      <td>0.807436</td>\n",
              "      <td>0.195800</td>\n",
              "      <td>0.917851</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.257044</td>\n",
              "      <td>0.890031</td>\n",
              "      <td>0.186312</td>\n",
              "      <td>0.916519</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        loss  accuracy  val_loss  val_accuracy  epoch\n",
              "45  0.124632  0.948153  0.171144      0.944494     45\n",
              "49  0.119195  0.952232  0.184349      0.943606     49\n",
              "44  0.132012  0.945957  0.177295      0.942274     44\n",
              "47  0.125739  0.948702  0.155468      0.942274     47\n",
              "43  0.131592  0.946270  0.154007      0.942274     43\n",
              "39  0.135727  0.944152  0.174203      0.941830     39\n",
              "33  0.147982  0.940074  0.167636      0.941830     33\n",
              "40  0.126644  0.948937  0.166730      0.941830     40\n",
              "41  0.126340  0.950035  0.148986      0.941385     41\n",
              "36  0.133247  0.948074  0.166774      0.940941     36\n",
              "24  0.144578  0.940780  0.156540      0.940497     24\n",
              "35  0.136452  0.942505  0.156869      0.940053     35\n",
              "30  0.154015  0.937799  0.160982      0.940053     30\n",
              "34  0.144219  0.942270  0.170461      0.939165     34\n",
              "42  0.134055  0.944780  0.156966      0.939165     42\n",
              "23  0.151855  0.936309  0.165031      0.939165     23\n",
              "37  0.130784  0.946976  0.169843      0.938721     37\n",
              "32  0.144232  0.940231  0.174333      0.938721     32\n",
              "26  0.158100  0.935760  0.166682      0.938721     26\n",
              "8   0.186091  0.924073  0.147609      0.938721      8\n",
              "46  0.124723  0.951526  0.198836      0.938721     46\n",
              "28  0.150565  0.939289  0.148595      0.938277     28\n",
              "31  0.142855  0.941642  0.166817      0.938277     31\n",
              "10  0.180159  0.924386  0.145402      0.937833     10\n",
              "11  0.183445  0.925092  0.151578      0.937833     11\n",
              "17  0.159863  0.932701  0.157804      0.937833     17\n",
              "48  0.124513  0.949878  0.177182      0.937389     48\n",
              "21  0.155412  0.935681  0.155436      0.937389     21\n",
              "19  0.160598  0.932387  0.152396      0.937389     19\n",
              "18  0.162583  0.935132  0.153929      0.937389     18\n",
              "20  0.160531  0.934818  0.147175      0.937389     20\n",
              "9   0.186445  0.923680  0.160865      0.936945      9\n",
              "27  0.149983  0.938740  0.161630      0.935613     27\n",
              "16  0.167427  0.932387  0.160147      0.935613     16\n",
              "7   0.189817  0.920307  0.153757      0.935613      7\n",
              "12  0.168574  0.930975  0.146876      0.935169     12\n",
              "38  0.129282  0.947682  0.170561      0.935169     38\n",
              "29  0.149919  0.939289  0.158883      0.934725     29\n",
              "22  0.152945  0.934348  0.161946      0.934725     22\n",
              "3   0.213998  0.909483  0.159261      0.933393      3\n",
              "13  0.178194  0.925955  0.159737      0.933393     13\n",
              "25  0.148408  0.940780  0.168019      0.932504     25\n",
              "14  0.175032  0.929249  0.149725      0.932504     14\n",
              "5   0.202012  0.913562  0.158047      0.932504      5\n",
              "4   0.206714  0.912856  0.183376      0.930728      4\n",
              "2   0.225795  0.902032  0.165420      0.928952      2\n",
              "6   0.199604  0.913954  0.181909      0.928508      6\n",
              "15  0.164185  0.932152  0.178372      0.927620     15\n",
              "0   0.698785  0.807436  0.195800      0.917851      0\n",
              "1   0.257044  0.890031  0.186312      0.916519      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdbqEMpz-iHC"
      },
      "source": [
        "**Kita ingin mencari loss yang terendah, loss ini terhadap val_lossnya**\n",
        "\n",
        "Nilai val_accuracy tertinggi pada model berada pada epoch 45 yaitu sebesar 0.944494 akan tetapi kita harus tinjau juga berdasarkan val_lossnya. Berdasarkan grafik epoch terhadap loss pada arsitektur CNN dengan vgg16 dan image augmentation, terlihat bahwa garis loss nya terus menurun dan val_loss nya cenderung stabil dengan epoch 50. Terlihat adanya perbedaan jarak antara loss dan val_lossnya dengan 50 epoch yang kecil yang berarti model yang kita buat ini bagus. Nilai val_loss terendah diperoleh saat epochnya sekitar 10, dimana diperoleh val_loss: 0.145402 dan val_accuracy: 0.937833, sedikit lebih kecil dibanding val_accuracy tertingginya"
      ]
    }
  ]
}