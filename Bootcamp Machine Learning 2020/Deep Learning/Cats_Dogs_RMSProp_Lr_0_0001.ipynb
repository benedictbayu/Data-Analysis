{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cats-Dogs RMSProp Lr 0.0001.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsIkJljP8KBW"
      },
      "source": [
        "## **Cats-Dogs Detection**\n",
        "\n",
        "**Benedictus Bayu Pramudhito**\n",
        "\n",
        "Sebelumnya sudah dicoba tuning hyperparameter dengan memakai optimizer Adamax dan juga SGD dengan learning rate 0.0001 dan nesterov = True. Namun kedua model ini memberikan nilai akurasi yang buruk yakni berkisar di 0.5 bahkan loss pada model Adamax bernilai negatif. Model terbaik diperoleh dengan Optimizer RMSProp dengan learning rate 0.0001 dan momentum 0.9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvF4ql1ggYDd",
        "outputId": "b0310cb6-f1dc-44a3-8229-086118cb93c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPAd5UxKkxF4"
      },
      "source": [
        "zip_path = '/content/drive/My\\ Drive/cats-dogs.zip'\n",
        "\n",
        "!cp {zip_path} /content/\n",
        "\n",
        "!cd /content/\n",
        "\n",
        "!unzip -q /content/cats-dogs.zip -d /content\n",
        "\n",
        "!rm /content/cats-dogs.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4SOzrLEk8aT"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop, Adadelta, Adagrad, Adam, Adamax\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NkAiW1e9Gaa"
      },
      "source": [
        "**Image Segmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YGWMBDSlBkV"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1.0/255.0, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255.0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glPK9tq1lG5q",
        "outputId": "1628a1c7-478c-4bec-df9e-f91f0e625d46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "dataset_dir = '/content/'\n",
        "\n",
        "train_augmented_iterator = train_datagen.flow_from_directory(os.path.join(dataset_dir, 'train'), class_mode='binary', batch_size=128, target_size=(200, 200))\n",
        "test_augmented_iterator = train_datagen.flow_from_directory(os.path.join(dataset_dir, 'test'), class_mode='binary', batch_size=128, target_size=(200, 200))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 12749 images belonging to 2 classes.\n",
            "Found 2252 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpK9W-iilJMJ",
        "outputId": "587485a5-0ed2-4bc6-9353-42a1eaf71790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "#Learning rate = 0.01\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "opt = RMSprop(learning_rate=0.0001, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 200, 200, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 100, 100, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 100, 100, 64)      18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 50, 50, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 80000)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               10240128  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 10,333,505\n",
            "Trainable params: 10,333,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYqa2kOklV2P",
        "outputId": "5f74bf35-a321-478a-defb-ad18aeca8b52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history_deep_model = model.fit_generator(train_augmented_iterator, steps_per_epoch=len(train_augmented_iterator), validation_data=test_augmented_iterator, validation_steps=len(test_augmented_iterator), epochs=50)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-8-1a0b6b7991a1>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/50\n",
            "  2/100 [..............................] - ETA: 7s - loss: 5.3863 - accuracy: 0.5273WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0507s vs `on_train_batch_end` time: 0.0973s). Check your callbacks.\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.9476 - accuracy: 0.5890 - val_loss: 0.5961 - val_accuracy: 0.6674\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 151s 2s/step - loss: 0.5721 - accuracy: 0.6971 - val_loss: 0.5216 - val_accuracy: 0.7380\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 148s 1s/step - loss: 0.5217 - accuracy: 0.7400 - val_loss: 0.4921 - val_accuracy: 0.7531\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 150s 1s/step - loss: 0.4892 - accuracy: 0.7655 - val_loss: 0.4597 - val_accuracy: 0.7766\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 149s 1s/step - loss: 0.4603 - accuracy: 0.7805 - val_loss: 0.4185 - val_accuracy: 0.8046\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 147s 1s/step - loss: 0.4516 - accuracy: 0.7896 - val_loss: 0.4188 - val_accuracy: 0.8095\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 147s 1s/step - loss: 0.4276 - accuracy: 0.8054 - val_loss: 0.4711 - val_accuracy: 0.7749\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 148s 1s/step - loss: 0.4079 - accuracy: 0.8172 - val_loss: 0.4185 - val_accuracy: 0.8042\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 146s 1s/step - loss: 0.3935 - accuracy: 0.8203 - val_loss: 0.4047 - val_accuracy: 0.8162\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 145s 1s/step - loss: 0.3829 - accuracy: 0.8260 - val_loss: 0.4064 - val_accuracy: 0.8135\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.3696 - accuracy: 0.8325 - val_loss: 0.3976 - val_accuracy: 0.8153\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 145s 1s/step - loss: 0.3529 - accuracy: 0.8426 - val_loss: 0.3666 - val_accuracy: 0.8353\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.3428 - accuracy: 0.8481 - val_loss: 0.3477 - val_accuracy: 0.8512\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 145s 1s/step - loss: 0.3420 - accuracy: 0.8474 - val_loss: 0.3459 - val_accuracy: 0.8468\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.3420 - accuracy: 0.8485 - val_loss: 0.3560 - val_accuracy: 0.8477\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.3201 - accuracy: 0.8621 - val_loss: 0.3351 - val_accuracy: 0.8539\n",
            "Epoch 17/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.3158 - accuracy: 0.8634 - val_loss: 0.3882 - val_accuracy: 0.8295\n",
            "Epoch 18/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.3138 - accuracy: 0.8602 - val_loss: 0.3057 - val_accuracy: 0.8694\n",
            "Epoch 19/50\n",
            "100/100 [==============================] - 143s 1s/step - loss: 0.3046 - accuracy: 0.8677 - val_loss: 0.3090 - val_accuracy: 0.8619\n",
            "Epoch 20/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.2914 - accuracy: 0.8755 - val_loss: 0.3238 - val_accuracy: 0.8606\n",
            "Epoch 21/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.2834 - accuracy: 0.8799 - val_loss: 0.3129 - val_accuracy: 0.8668\n",
            "Epoch 22/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.2753 - accuracy: 0.8823 - val_loss: 0.3277 - val_accuracy: 0.8517\n",
            "Epoch 23/50\n",
            "100/100 [==============================] - 145s 1s/step - loss: 0.2675 - accuracy: 0.8861 - val_loss: 0.3131 - val_accuracy: 0.8566\n",
            "Epoch 24/50\n",
            "100/100 [==============================] - 145s 1s/step - loss: 0.2595 - accuracy: 0.8883 - val_loss: 0.3036 - val_accuracy: 0.8717\n",
            "Epoch 25/50\n",
            "100/100 [==============================] - 145s 1s/step - loss: 0.2540 - accuracy: 0.8943 - val_loss: 0.3244 - val_accuracy: 0.8650\n",
            "Epoch 26/50\n",
            "100/100 [==============================] - 145s 1s/step - loss: 0.2562 - accuracy: 0.8947 - val_loss: 0.2960 - val_accuracy: 0.8712\n",
            "Epoch 27/50\n",
            "100/100 [==============================] - 148s 1s/step - loss: 0.2453 - accuracy: 0.8962 - val_loss: 0.2919 - val_accuracy: 0.8783\n",
            "Epoch 28/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.2392 - accuracy: 0.9027 - val_loss: 0.2900 - val_accuracy: 0.8801\n",
            "Epoch 29/50\n",
            "100/100 [==============================] - 145s 1s/step - loss: 0.2387 - accuracy: 0.9012 - val_loss: 0.2758 - val_accuracy: 0.8814\n",
            "Epoch 30/50\n",
            "100/100 [==============================] - 145s 1s/step - loss: 0.2303 - accuracy: 0.9086 - val_loss: 0.3170 - val_accuracy: 0.8783\n",
            "Epoch 31/50\n",
            "100/100 [==============================] - 145s 1s/step - loss: 0.2271 - accuracy: 0.9067 - val_loss: 0.2859 - val_accuracy: 0.8801\n",
            "Epoch 32/50\n",
            "100/100 [==============================] - 146s 1s/step - loss: 0.2176 - accuracy: 0.9107 - val_loss: 0.2857 - val_accuracy: 0.8770\n",
            "Epoch 33/50\n",
            "100/100 [==============================] - 145s 1s/step - loss: 0.2203 - accuracy: 0.9092 - val_loss: 0.2802 - val_accuracy: 0.8868\n",
            "Epoch 34/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.2098 - accuracy: 0.9118 - val_loss: 0.2906 - val_accuracy: 0.8854\n",
            "Epoch 35/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.2133 - accuracy: 0.9088 - val_loss: 0.2849 - val_accuracy: 0.8903\n",
            "Epoch 36/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.2016 - accuracy: 0.9175 - val_loss: 0.2761 - val_accuracy: 0.8837\n",
            "Epoch 37/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.2026 - accuracy: 0.9187 - val_loss: 0.2831 - val_accuracy: 0.8885\n",
            "Epoch 38/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.1937 - accuracy: 0.9231 - val_loss: 0.2924 - val_accuracy: 0.8752\n",
            "Epoch 39/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.1903 - accuracy: 0.9245 - val_loss: 0.2764 - val_accuracy: 0.8983\n",
            "Epoch 40/50\n",
            "100/100 [==============================] - 144s 1s/step - loss: 0.1807 - accuracy: 0.9282 - val_loss: 0.2765 - val_accuracy: 0.8890\n",
            "Epoch 41/50\n",
            "100/100 [==============================] - 143s 1s/step - loss: 0.1799 - accuracy: 0.9266 - val_loss: 0.2904 - val_accuracy: 0.8841\n",
            "Epoch 42/50\n",
            "100/100 [==============================] - 148s 1s/step - loss: 0.1729 - accuracy: 0.9292 - val_loss: 0.3120 - val_accuracy: 0.8881\n",
            "Epoch 43/50\n",
            "100/100 [==============================] - 145s 1s/step - loss: 0.1664 - accuracy: 0.9365 - val_loss: 0.2835 - val_accuracy: 0.8956\n",
            "Epoch 44/50\n",
            "100/100 [==============================] - 146s 1s/step - loss: 0.1719 - accuracy: 0.9291 - val_loss: 0.2796 - val_accuracy: 0.8877\n",
            "Epoch 45/50\n",
            "100/100 [==============================] - 146s 1s/step - loss: 0.1562 - accuracy: 0.9363 - val_loss: 0.2611 - val_accuracy: 0.8961\n",
            "Epoch 46/50\n",
            "100/100 [==============================] - 146s 1s/step - loss: 0.1621 - accuracy: 0.9356 - val_loss: 0.3066 - val_accuracy: 0.8881\n",
            "Epoch 47/50\n",
            "100/100 [==============================] - 146s 1s/step - loss: 0.1578 - accuracy: 0.9376 - val_loss: 0.2823 - val_accuracy: 0.8930\n",
            "Epoch 48/50\n",
            "100/100 [==============================] - 146s 1s/step - loss: 0.1522 - accuracy: 0.9398 - val_loss: 0.2788 - val_accuracy: 0.8956\n",
            "Epoch 49/50\n",
            "100/100 [==============================] - 146s 1s/step - loss: 0.1539 - accuracy: 0.9407 - val_loss: 0.2902 - val_accuracy: 0.8956\n",
            "Epoch 50/50\n",
            "100/100 [==============================] - 145s 1s/step - loss: 0.1487 - accuracy: 0.9418 - val_loss: 0.3064 - val_accuracy: 0.8903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDA7mc3p9V3s"
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt8UiLBrPn-u",
        "outputId": "ea3aecb4-bf3f-4bc4-b445-ec17bf414e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "history_dataframe = pd.DataFrame(history_deep_model.history)\n",
        "history_dataframe['epoch'] = history_deep_model.epoch\n",
        "history_dataframe"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "      <th>epoch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.947643</td>\n",
              "      <td>0.588987</td>\n",
              "      <td>0.596058</td>\n",
              "      <td>0.667407</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.572074</td>\n",
              "      <td>0.697074</td>\n",
              "      <td>0.521553</td>\n",
              "      <td>0.738011</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.521695</td>\n",
              "      <td>0.739980</td>\n",
              "      <td>0.492051</td>\n",
              "      <td>0.753108</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.489236</td>\n",
              "      <td>0.765472</td>\n",
              "      <td>0.459737</td>\n",
              "      <td>0.776643</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.460256</td>\n",
              "      <td>0.780532</td>\n",
              "      <td>0.418451</td>\n",
              "      <td>0.804618</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.451644</td>\n",
              "      <td>0.789552</td>\n",
              "      <td>0.418835</td>\n",
              "      <td>0.809503</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.427562</td>\n",
              "      <td>0.805396</td>\n",
              "      <td>0.471136</td>\n",
              "      <td>0.774867</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.407920</td>\n",
              "      <td>0.817162</td>\n",
              "      <td>0.418548</td>\n",
              "      <td>0.804174</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.393523</td>\n",
              "      <td>0.820300</td>\n",
              "      <td>0.404676</td>\n",
              "      <td>0.816163</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.382904</td>\n",
              "      <td>0.826026</td>\n",
              "      <td>0.406405</td>\n",
              "      <td>0.813499</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.369551</td>\n",
              "      <td>0.832536</td>\n",
              "      <td>0.397592</td>\n",
              "      <td>0.815275</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.352915</td>\n",
              "      <td>0.842576</td>\n",
              "      <td>0.366613</td>\n",
              "      <td>0.835258</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.342766</td>\n",
              "      <td>0.848145</td>\n",
              "      <td>0.347679</td>\n",
              "      <td>0.851243</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.341976</td>\n",
              "      <td>0.847439</td>\n",
              "      <td>0.345873</td>\n",
              "      <td>0.846803</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.341970</td>\n",
              "      <td>0.848459</td>\n",
              "      <td>0.355959</td>\n",
              "      <td>0.847691</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.320052</td>\n",
              "      <td>0.862107</td>\n",
              "      <td>0.335133</td>\n",
              "      <td>0.853908</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.315778</td>\n",
              "      <td>0.863440</td>\n",
              "      <td>0.388180</td>\n",
              "      <td>0.829485</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.313839</td>\n",
              "      <td>0.860224</td>\n",
              "      <td>0.305692</td>\n",
              "      <td>0.869449</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.304565</td>\n",
              "      <td>0.867676</td>\n",
              "      <td>0.309005</td>\n",
              "      <td>0.861901</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.291412</td>\n",
              "      <td>0.875520</td>\n",
              "      <td>0.323846</td>\n",
              "      <td>0.860568</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.283353</td>\n",
              "      <td>0.879912</td>\n",
              "      <td>0.312917</td>\n",
              "      <td>0.866785</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.275262</td>\n",
              "      <td>0.882265</td>\n",
              "      <td>0.327702</td>\n",
              "      <td>0.851687</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.267534</td>\n",
              "      <td>0.886109</td>\n",
              "      <td>0.313132</td>\n",
              "      <td>0.856572</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.259465</td>\n",
              "      <td>0.888305</td>\n",
              "      <td>0.303587</td>\n",
              "      <td>0.871670</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.254037</td>\n",
              "      <td>0.894345</td>\n",
              "      <td>0.324417</td>\n",
              "      <td>0.865009</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.256177</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>0.296011</td>\n",
              "      <td>0.871226</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.245260</td>\n",
              "      <td>0.896227</td>\n",
              "      <td>0.291893</td>\n",
              "      <td>0.878330</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.239210</td>\n",
              "      <td>0.902737</td>\n",
              "      <td>0.290029</td>\n",
              "      <td>0.880107</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.238666</td>\n",
              "      <td>0.901247</td>\n",
              "      <td>0.275823</td>\n",
              "      <td>0.881439</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.230305</td>\n",
              "      <td>0.908620</td>\n",
              "      <td>0.317048</td>\n",
              "      <td>0.878330</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.227051</td>\n",
              "      <td>0.906738</td>\n",
              "      <td>0.285950</td>\n",
              "      <td>0.880107</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.217583</td>\n",
              "      <td>0.910738</td>\n",
              "      <td>0.285679</td>\n",
              "      <td>0.876998</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.220343</td>\n",
              "      <td>0.909248</td>\n",
              "      <td>0.280150</td>\n",
              "      <td>0.886767</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.209756</td>\n",
              "      <td>0.911758</td>\n",
              "      <td>0.290600</td>\n",
              "      <td>0.885435</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.213324</td>\n",
              "      <td>0.908777</td>\n",
              "      <td>0.284946</td>\n",
              "      <td>0.890320</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.201624</td>\n",
              "      <td>0.917484</td>\n",
              "      <td>0.276093</td>\n",
              "      <td>0.883659</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.202590</td>\n",
              "      <td>0.918739</td>\n",
              "      <td>0.283062</td>\n",
              "      <td>0.888544</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.193660</td>\n",
              "      <td>0.923053</td>\n",
              "      <td>0.292412</td>\n",
              "      <td>0.875222</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.190260</td>\n",
              "      <td>0.924543</td>\n",
              "      <td>0.276388</td>\n",
              "      <td>0.898313</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.180728</td>\n",
              "      <td>0.928230</td>\n",
              "      <td>0.276527</td>\n",
              "      <td>0.888988</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.179936</td>\n",
              "      <td>0.926582</td>\n",
              "      <td>0.290371</td>\n",
              "      <td>0.884103</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.172866</td>\n",
              "      <td>0.929249</td>\n",
              "      <td>0.312000</td>\n",
              "      <td>0.888099</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.166418</td>\n",
              "      <td>0.936544</td>\n",
              "      <td>0.283487</td>\n",
              "      <td>0.895648</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.171907</td>\n",
              "      <td>0.929092</td>\n",
              "      <td>0.279611</td>\n",
              "      <td>0.887655</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.156161</td>\n",
              "      <td>0.936309</td>\n",
              "      <td>0.261063</td>\n",
              "      <td>0.896092</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.162054</td>\n",
              "      <td>0.935603</td>\n",
              "      <td>0.306567</td>\n",
              "      <td>0.888099</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.157808</td>\n",
              "      <td>0.937564</td>\n",
              "      <td>0.282274</td>\n",
              "      <td>0.892984</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.152172</td>\n",
              "      <td>0.939838</td>\n",
              "      <td>0.278782</td>\n",
              "      <td>0.895648</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.153886</td>\n",
              "      <td>0.940701</td>\n",
              "      <td>0.290206</td>\n",
              "      <td>0.895648</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.148738</td>\n",
              "      <td>0.941799</td>\n",
              "      <td>0.306435</td>\n",
              "      <td>0.890320</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        loss  accuracy  val_loss  val_accuracy  epoch\n",
              "0   0.947643  0.588987  0.596058      0.667407      0\n",
              "1   0.572074  0.697074  0.521553      0.738011      1\n",
              "2   0.521695  0.739980  0.492051      0.753108      2\n",
              "3   0.489236  0.765472  0.459737      0.776643      3\n",
              "4   0.460256  0.780532  0.418451      0.804618      4\n",
              "5   0.451644  0.789552  0.418835      0.809503      5\n",
              "6   0.427562  0.805396  0.471136      0.774867      6\n",
              "7   0.407920  0.817162  0.418548      0.804174      7\n",
              "8   0.393523  0.820300  0.404676      0.816163      8\n",
              "9   0.382904  0.826026  0.406405      0.813499      9\n",
              "10  0.369551  0.832536  0.397592      0.815275     10\n",
              "11  0.352915  0.842576  0.366613      0.835258     11\n",
              "12  0.342766  0.848145  0.347679      0.851243     12\n",
              "13  0.341976  0.847439  0.345873      0.846803     13\n",
              "14  0.341970  0.848459  0.355959      0.847691     14\n",
              "15  0.320052  0.862107  0.335133      0.853908     15\n",
              "16  0.315778  0.863440  0.388180      0.829485     16\n",
              "17  0.313839  0.860224  0.305692      0.869449     17\n",
              "18  0.304565  0.867676  0.309005      0.861901     18\n",
              "19  0.291412  0.875520  0.323846      0.860568     19\n",
              "20  0.283353  0.879912  0.312917      0.866785     20\n",
              "21  0.275262  0.882265  0.327702      0.851687     21\n",
              "22  0.267534  0.886109  0.313132      0.856572     22\n",
              "23  0.259465  0.888305  0.303587      0.871670     23\n",
              "24  0.254037  0.894345  0.324417      0.865009     24\n",
              "25  0.256177  0.894737  0.296011      0.871226     25\n",
              "26  0.245260  0.896227  0.291893      0.878330     26\n",
              "27  0.239210  0.902737  0.290029      0.880107     27\n",
              "28  0.238666  0.901247  0.275823      0.881439     28\n",
              "29  0.230305  0.908620  0.317048      0.878330     29\n",
              "30  0.227051  0.906738  0.285950      0.880107     30\n",
              "31  0.217583  0.910738  0.285679      0.876998     31\n",
              "32  0.220343  0.909248  0.280150      0.886767     32\n",
              "33  0.209756  0.911758  0.290600      0.885435     33\n",
              "34  0.213324  0.908777  0.284946      0.890320     34\n",
              "35  0.201624  0.917484  0.276093      0.883659     35\n",
              "36  0.202590  0.918739  0.283062      0.888544     36\n",
              "37  0.193660  0.923053  0.292412      0.875222     37\n",
              "38  0.190260  0.924543  0.276388      0.898313     38\n",
              "39  0.180728  0.928230  0.276527      0.888988     39\n",
              "40  0.179936  0.926582  0.290371      0.884103     40\n",
              "41  0.172866  0.929249  0.312000      0.888099     41\n",
              "42  0.166418  0.936544  0.283487      0.895648     42\n",
              "43  0.171907  0.929092  0.279611      0.887655     43\n",
              "44  0.156161  0.936309  0.261063      0.896092     44\n",
              "45  0.162054  0.935603  0.306567      0.888099     45\n",
              "46  0.157808  0.937564  0.282274      0.892984     46\n",
              "47  0.152172  0.939838  0.278782      0.895648     47\n",
              "48  0.153886  0.940701  0.290206      0.895648     48\n",
              "49  0.148738  0.941799  0.306435      0.890320     49"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOUO_l7_lbcR",
        "outputId": "3991ea2f-2a80-4e66-a879-bb3127f2afb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "loss, accuracy = model.evaluate(test_augmented_iterator, verbose=2)\n",
        "\n",
        "print('Test accuracy', accuracy)\n",
        "print('Test Loss', loss)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18/18 - 22s - loss: 0.2949 - accuracy: 0.8841\n",
            "Test accuracy 0.884103000164032\n",
            "Test Loss 0.29485538601875305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfi_JvxElZVQ",
        "outputId": "f959d5cc-92ff-4ba2-c740-3cdfef26d285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plot_loss(history_deep_model)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNuWhtaeQSXA"
      },
      "source": [
        "**Kita ingin mencari loss yang terendah, loss ini terhadap val_lossnya**\n",
        "\n",
        "Berdasarkan grafik epoch terhadap loss pada model CNN dengan deeper layer dan image augmentation, terlihat bahwa garis loss nya terus menurun sementara garis val_loss nya menurun namun mulai stabil setelah epoch sekitaran 28. Terlihat juga bahwa perbedaan jarak antara loss dan val_lossnya dengan 50 epoch ini tidak besar yang berarti model yang kita buat ini tidak overfit. Oleh karena itu, nilai val_loss terendah diperoleh saat epochnya sekitar 45, dimana diperoleh **val_loss: 0.2611** dan **val_accuracy: 0.8961**"
      ]
    }
  ]
}