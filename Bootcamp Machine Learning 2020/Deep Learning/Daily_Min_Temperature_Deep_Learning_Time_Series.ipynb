{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Daily Min Temperature Deep Learning Time Series.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HLZjjbX55Xk"
      },
      "source": [
        "# **Daily Min Temperature Deep Learning Time Series**\n",
        "\n",
        "**Benedictus Bayu Pramudhito**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnk6TyOGv5Fa"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XHGR7poxXXd",
        "outputId": "630f5d69-301e-46c5-ae54-2b4fce09d5c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "import os\n",
        "print('\\n'.join(os.listdir(r'./drive/My Drive/Colab Test (Bootcamp ML 2020)')))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bottle.csv\n",
            "References\n",
            "Untitled0.ipynb\n",
            "Week_1_Python_1.ipynb\n",
            "heart.csv\n",
            "pima-indians-diabetes.csv\n",
            "sonar.csv\n",
            "housing.csv\n",
            "winequality-white.csv\n",
            "auto.csv\n",
            "sales.csv\n",
            "birth.csv\n",
            "shampoo.csv\n",
            "daily-min-temperatures.csv\n",
            "Quiz\n",
            "customer.csv\n",
            "iris.csv\n",
            "iris (1).csv\n",
            "Bootcamp Images\n",
            "parkinsons\n",
            "numbers_dataset\n",
            "bottle-updated.csv\n",
            "auto-mpg.csv\n",
            "auto (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgJ5pb5mxxG7"
      },
      "source": [
        "def parser(x):\n",
        "  return datetime.strptime(x, '%Y-%m-%d')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8VrPWNBxfEC",
        "outputId": "a0330eb6-c399-4067-8f7a-20454002c801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "df = pd.read_csv(r'./drive/My Drive/Colab Test (Bootcamp ML 2020)/daily-min-temperatures.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date\n",
              "1981-01-01    20.7\n",
              "1981-01-02    17.9\n",
              "1981-01-03    18.8\n",
              "1981-01-04    14.6\n",
              "1981-01-05    15.8\n",
              "Name: Temp, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_brXDRR0x-fa",
        "outputId": "fae9a21f-c297-4980-854d-8d9b62f3ac17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "df.plot()\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD8CAYAAABuHP8oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2dd3gWxfbHvycFQg0tdEKo0ouEDgpSRLArV9FrV/Sq14b6w861XLle+8Xee0FUVBSliYD03qWF3nsLJHnn98fum2zevGV3dnbm3TfzeZ48ecvue2bLnD1z5sw5xBiDRqPRaPxHkuoGaDQajYYPrcA1Go3Gp2gFrtFoND5FK3CNRqPxKVqBazQajU/RClyj0Wh8SopMYTVq1GBZWVkyRWo0Go3vWbhw4T7GWEbo51IVeFZWFhYsWCBTpEaj0fgeItoc7nPtQtFoNBqfohW4RqPR+BStwDUajcanaAWu0Wg0PkUrcI1Go/EpWoE7IK8ggPV7jqluhkaj0QDQCtwRz0xYjf4vTsf2QydVN0Wj0Wi0AnfCvE0HAAAHj59W3BKNRqPRCtwRuvSFRqOJJ7QC54BIdQs0Go1GK3CNRqPxLVqBazQajU+JqcCJqAERTSOiVUS0kojuNj8fRUTbiWiJ+TfY++ZqNBqNJoidbIT5AEYwxhYRUSUAC4lokvndS4yx571rXnwwbuE2TFq1W3UzNBqNphgxLXDG2E7G2CLz9VEAqwHU87phoRw+mYefl+9Ebl4Bvlu8DYzJiwkZMXYpJq7cJVWmRqPRxMJRPnAiygLQEcBcAD0B3ElE1wJYAMNKPxhmn+EAhgNAZmYmd0Pv+XIxpq3di34tamLKmj2oXqEszmpeIr+5FAg6DEWj0ajH9iQmEVUEMA7APYyxIwDeANAEQAcAOwG8EG4/xtjbjLFsxlh2Rga/wv1j3T4AwJQ1ewAAR3PzuX9Lo9FoEgFbCpyIUmEo788YY98CAGNsN2OsgDEWAPAOgC7eNRMoCGj3hUaj0VixE4VCAN4DsJox9qLl8zqWzS4BsEJ88yLDFK6L1At5NBpNPGDHB94TwDUAlhPREvOzhwEMI6IOMFaY5wC41ZMWahKaT+dsRsWyKbi4o/R5cY3G98RU4IyxmUDYWbufxTdHU9p49Htj4KYVuPfM2bgf7etXQbkyyaqbohGEXomp0ZQCth08gSvfnoMHxy1T3RSNQLQC12hKAcGorb92HVXWhmlr9iBr5ARs2X9CWRsSDa3AHaDX8Wj8jsoJ+HGLtgEAFm8tsVxEw4lvFbhKZaqjUDR+Ix6Mj2ATSHcgYfhWgWs0GvcwxvDy5L/klAk0NbhW3+LQClyjKcWs23MML09eh9s/Xei5rODaDW2Ai0MrcAeoXDyk0XhBcIXzqfyANJk6l5A4tALXaBKcj2fnYPCrM8J+J9M3Hg9++ETDtwpcxb2gLQeNH3l8/MrC1/EwgRgHTUgYfKvAVbB2t7oYWo3G7zA9iSkcrcA5SJJsQmzYeww/LdshVaYmMVGpPANMT2KKRitwH9Dvhem48/PFqpuhjEtfn4XP525R3YyEwKo8xy/ZHtE37nErFMhMTLQCj0F+QfjZ+QnLdmLbQb0kWAaLthzCw98tV92MhOPuL5cUvpbhG9dzmOLxrQJ/fdp6KXI+n1fS8mMMuOPzRbj4tVlS2hDks7mbMeztOVJlahKXJMmGcKEPPIzcI7l5EY0lkTw3cQ2e/HGV53Jk4VsFvkZSUp4jJ/NKfBaMB9937LSUNgR55LsVmL1xv1SZmsTFOpcjR5eziLLajfoN93291PMWvP77Brw/a5PncmThWwUuCx27Gj8cOnEaF42ZqbPZucBq/RZT4BKt8Ujumh+W6ol6p2gFzoFW6mqYsHwnlm47jDemb1DdlIRAdjRIpH7DPOxQS7YewroEDv/VCpyDf/24MvZGHrLnSK5S+YlOIMCk+GNVYF2MlizZCV6YjTD0cw8Nootfm4UBL/3hnQDFaAUeg3D31pyNB6S3w8rbf2xUKj/Rue6DeWj6yC+qm+E5yZJdKCxCHLi1jx04Lndeye/4WoF/Omez6iYoobQuhJDlupqxbp8cQQqw3jvJyYos8BCxAcuF3XVYjy6d4GsFHiyIW9p4Z0bizKLz4PYBdiq/ANP/2iumMQ7YeuAE1saInlq/5xg27j0mpT0pSWq6f2hOIeuDubQaJ7z4WoFrNDw8+/MaXPf+PCzdekiq3N7PTcO5L0f3x/Z/cTrOeWG6lPakJlvDCCUs5IkwgrJa4FqBO8MXCnxBjjqfs444UYs1QkHUpdhgWriHwsT4lyZUTWJGe1bojJ/O8IUCH78kfuND8xI0WiFe0A9QsVjVo9XalTqJab6ftnYPPp+7pZgFroq1u47ixd/Wqm6GY3yhwON5WBUPN58oAgGGGev2YsX2w6qbUkix4bXCdiQMROj8zGTc8vGC4h9LbYIh7YYP5uPh75aj1eO/WpsnhY5P/lbs/eVv/IlXp67HidP5wmT8sHQHskZOwP5jp4T9ZigxFTgRNSCiaUS0iohWEtHd5ufViGgSEa0z/1f1rJEKNXhpKqP2wZ85uOa9eTj/fzOF3shuYBFea/jZe/QUJq3arUx+PDyID54o7j7LCxgjaZEunM/MKDkv6wjYscDzAYxgjLUC0A3AHUTUCsBIAFMYY80ATDHfa3zMBkv0w2mJNRKjEW6AEw8KwA0FAYaskRPw7ozo8fwBm9s5QeW5sxOeqap9Xgyky5VJBgDk5hWI/3GTmAqcMbaTMbbIfH0UwGoA9QBcBOAjc7OPAFzsWSPj2YeSQFjPcrx4hlS6qD6bu9kTd1Lw4fjfX6P7XINW4X8mrhHeBiDE2pTYx6KJktnVT+UX4LmJa4qNNkXKTzEnifMLvLuHU5xsTERZADoCmAugFmNsp/nVLgC1IuwzHMBwAMjMzORqpEr9HUt/xIui04jnke+MdQY5o4d48vux7muvIzJU9avdR7zzCTvh87lb8PrvG7Bx73FPfj/o6w94qCNsT2ISUUUA4wDcwxg7Yv2OGdPLYZvJGHubMZbNGMvOyMjga6Q2wKUQDwOd2Rv2Y7LFP1vsAanwaTl3435lfuNEMxLuHxs5bayXyi6UYATZxJW7cMoDl2FyoQL37qBsKXAiSoWhvD9jjH1rfrybiOqY39cBsMebJsZHJe1InMoLYPuhk6qbIQSrxRd6y+05motjp7yf2Bz2zhzcbImQsE4iy+zcoVzx9pwSkRu8qJwYj+OuBEB9VJdI8cE4e6UKnAzt+R6A1YyxFy1f/QDgOvP1dQDGi29esA1e/XJsYp36u79ajJ6jpyZE9jrreZ4TUjiiyzNTcK6CrG5WpT3GrMK0QdJSc68oqs4e/caWqehld7HZG8IXJlE92hB5zoP9qcBDy8OOBd4TwDUAziGiJebfYACjAQwgonUA+pvvvWlkHJsNv681cmoUqL7zBBMuH4eKkYZ1Jebeo4bvNFESHtm9rXnurNP5gcLzVUxmhNeyGfZO+NKAAcaw49BJT3OEB/FqjuFobh4On8wr1FteHkrMSUzG2ExEvtb9xDYnPPGrvotIhCXA1gdlvDyPwjUjTprGjd32u7kGd32xGBNX7rI9ARsvNtLqnUdx/9ilePz8VrixVyPp8kXc9+3+9RsYAy5sXxeAt+fWFysx49kCLw3s82Al2d6jp9Dj2SkRv7/irdn4esHWsB1KtZ9UFHbvah5rdOLKXeFlFssBLrsmZmw27TNGfqEuPFmIuLNk3p6+UOBK9XeCKAun5BUE8PncLdh64ASyn54s/PcnrtyFHVFcIXM3HcCD3yyTMpSWjcxj8tv581lzleMTBR4v9kFkRE1+bNp3HINe/gOHTsivTGI9za9OXY+Hv1uOD2bleCPMZk8Nt1midPJo9/XX87firi8Wu5YROn9mlbhpnzfxz27wai7potdmlVgRGVwoZcVvDzx/KHDVDbCBqOv+2rT1WLPrKH5TEHMczo+fm+/dMmA7hPWBS+5jK3eIXY150lQk0ZTFg+OWFd4Dbg7XrkKKFyPJq2u7dOsh/BWSk+S5iSVXwo5buE1YRJmM29QXClxtMis1vDtjI16dsk6qzHCn2TNlafOahlNAsq2kIa/OFPp7T/+0GgBw/LT3D0cnZ+r139fj9d/Xe9YWOwQ8DLmzc9uM+nEV/vHZIm4ZE1eEn3vwCl8o8DgxDqTy1+5jeHHSX6qbAS8eYVv2n8CizQdtbRuuP4vq44dP5mHaGs/Wn0XEaRikm+dV6IRvtL703MS1hVbp5FW7cTRXfsGL4LVV2efdrLi97dOFAlsSG0e5UFQRayn9x7NzEAgwXN9TfNiRrPuIMQbGSsrzMpNZKOGO1YtEPGf9d5rtbe/7ekmJz3o0rS6kHUEf8/xH+iOjUlkhv2kHmQt07Cp/67XfeuAEbv54Afq3rIV3r8v2pF2R8PLcyB5Nyxgp+kKBx/LPPT5+JQB4osBlxOwCwN/emo35OQcxtFP9Yp/LDKcKd5rHLtwmTX44wqUgbVazklAZpyT7+VXOk9lZr3DczM635YD8SU4vz43fJijtoF0oYXj2l9VYtMXeED+IW8thfo4hL/RYZdctjMWD3yxNiLQBVmT3a5nibFvgcXKbebnsXLb6ljEx7AsFLnsS863pG3Hp639Kk/f53C0Rv0uWdOxLtx4qUaUkHF8v2IZFW+RWcw9F9ClJlIVBVoLnyO6xFUtkZsnVsv3QyRLRG17ipd89AS+zPxS4H9bxuLk5vpxfpMBDh7hJESzwaWvFTr5d9NosfGPTXRJngwLXqMxy6BXBS8RzaMwykdhz9FQMdJHEjDGGp35aZTvm/LSnozu5F1r7wE1UDe9WbD9cmAEvFm4uVbTDS4mgLfcqTIofL8NtUVg7WtbICQpbEhnGGNeQvIQSsfETIicSN+47jvdmbsJ7MzfZ2n7RZu9Gd1sOnPDst6PhpSvFFxa4TKw3/FM/rZIjNMoFjmSBq4W/Tcu2ue+gos+IbAvceo+9OmUd1u6K7aJw2kbH1WAsJ7XIAnd/pp0aobuOeJdp8t6vIheS8JInf1zp2US57xW46GGK9efmbjqgrB0ycdp2N8+UC8fM4t9ZAHkFgRKRLbKvnVXai5P+wqWvxz4nTR7+Gat2HIm5XVRhcPbwE/Og9G+/cEvwyPcdO43vFm33RIYvFHi0/vXsL/wFX7+Yt8VxtInXhBo9X83bGnY7kcNcXuvOj5wME1cv3wIv/j7PZqx9pAyD4SjygTs/OKsPXCMGr+4xXyjwaLz9x0bufR/6dnmJaBPe8+yVD/yrBeEVuMi5HqehW17066yRE/DZ3M325LtoQLhdZUehhEo7XRDAydMF2Hk4esGMk6ftl7QrikIJ/3mJ7S2vx0xbF3VbJ/wnTL4ROyRCfn0Z+EKBy42b5ZPmRgdYO4rdTvPyZHHL7J0qMK/COsMlF5JBPIQR/rZqF5ZujT4/wGPFhd7PdhTjryuNpeR7OCbKdx3Oxeu/ry+Uq6oQdGnBFwpcJrxd+ZaP+Ave8qjDPUdPYeqa3cgaOQFHXMbOOrbAPTKODp/0PvdGOPdPHOhv3P3lEtz2afQkSu/N3IRXJjtLcGb30MLN9+yxlGTLsznku/2zhXhu4lqs3+PvuqWi8SpFgC8UuMwOxitrXo79Cc9QeH3Kr0wxi/y67Cyy6nkePC4mx7mb4XW4EdaGvccwNoKrKt54yebIK3iOQg+Xd7TxtM2IrOOnjDmGRKsRG42pa3bjirdme7qKNBK+UOCliR2H5BTsHfrmn4VumOd/dea64LXA/2/cMr4dQ+CxZh4fvwK3fLwg7J53f7kED3yzDFs9iBP+fO4W9H5uavEPZSg3CooqLotX9JyNzgwUP+vvvmdkONr+xg8XYO6mA/hx6Q6PWhQZrcBDkJkpLhzT/9orRc78nIN42RyOfzzb3uRhEF4L2K2rJ0i5Ms7Xn308ezMmrdqNBVFGSr2fs58l0S4Pf7ccWw8Un5yUOqcT8p7XAvdjRMoJB5O+VvIDDO/O2Og4drvE9hIutC9WYtolN68AaanJrn4jTJUlz/Fb30jifOyLiixw8ys3fsg/VyEKiQa4MBeKitDR3UdyUatyGvf+vJPiM9btw4x1+3DidAHu6tfM9n4noxTo8Oqa+8ICt1sfMme/u/SXOw+fRN/nf3f1G6UB1SFePh6dSyNSMiteN63dxVui9PyMdXvR9d9T8JuD2PdQ3E6KHz/lzIIf9WPkeYJP52z2pNqQLxS43QQ3bhXLFW/N8XQpbyR4b/rDCgofA/ztFWbE+dnBKpnQM8UbJus0dPR0fgBDXp3BJQsoKje3JEZoZTTiaXX0ml1H8dPyncJ/1xcKXNbs7o5D0RdSxBs5+51PugUCDF/PdxdxwbuUXpQCP366AN8tVltowg1yK/KIscCdXrvNB05gJc/S/xDcnCnXakPwQNOpRW+HmAqciN4noj1EtMLy2Sgi2k5ES8y/wcJbZiHfJ/k+8woCUkugBXHin/x28XY86DIa5OsFapXn6F/W4N6vlsZdGoRoWBXpiu3uFZt9ucXf8/rAI2XFtHI0N68wVUE8DLb8oTXcYccC/xDAoDCfv8QY62D+/Sy2WcUp8KAuoxdcOGYWWjw20dE+p/MDhdV4ZGB3PiEavOkLRPvOcyVUdfcrkeLAea3io7nRrcf1e46h7ajfsJljVOgVblfYir5fvZg5iqnAGWN/AOBfpSIAuxa46lCn1TvtdY5Jq3bj0e+XI78gYHthhij8nIiqBD46FJnu2OOn8gstYVHumgMxFmGtC6naI+pwedu/7eAJTFgm3ufshAke+LxDcRNGeCcRXQtgAYARjLGwZiQRDQcwHAAyMzO5BBWoiO3zkFs+NkLZPp0TuZSaV4TqvJU7DsuT7SOFKxqZY8jJq4vyj4h6cDjNSy/sUnO2/4Xf3BtGou9XL+5/3knMNwA0AdABwE4AL0TakDH2NmMsmzGWnZHhbIVTEFkelNKgYEKPccirM7l+53R+APd8uRibXYZuukF1OKMT7vw8ep4TrxCVqMvpxPUTP6wUIpe39SLuDD/cXVwKnDG2mzFWwBgLAHgHQBexzSpOVvXywn+z5+ipJT6Lo6gjR+w4dFKqJQ0A8zYdwPdLduChb5dLlWvFTw/cX1bwxzO7YYGg+RWnYYSxXC524U4XLeDesHPITtNQiIZLgRNRHcvbSwCsiLStCO62uRrKyTXb7rOQwWjc/tkibkvaLSqVqI/0tzLcRhwFaVc/XcjvyELW6OyDWfZqfXqFnTDCLwDMBnAGEW0jopsAPEdEy4loGYC+AO71spEpyd6Gq2eNnIAcm1Wz3TJn434pcqxkPz25sGyXSqWXCBOoKsJE44GG1SuoboIjZN1qqu/pmJOYjLFhYT5+z4O2KGXZdu9dEMu2HcKVb8/xXE4o+46dwr5j6qrYe4WKzjPqh5UYfVk76XJV47dHrxgfuOgwQvFn0RcrMe3yzcJt3AUOkomEPbX/75vww9b9gvyCblBpMYiWrOJQ1tioIJ+Q+EyDi7g37PyG6tOSUAr8LXPCg2dJvJtK66FEqmOp+mKLRHXaXUDN+fSDF0j1sD4eEGHt2srB5ECM3ZxOTkgoBR6E5+IlJZHt6uC8eFVL0gmiihZc8948AM7OtR/iajXhOXBM3ejx97V7HO8j4t54azp/wfRwnMrXChwA8OLf2uPhwS0ifs+TRlKGco0HhfPuTLGz5mqPSb7wOLiEShi7cJuyCdxPHBYcAYCfJayCBJzdD04LRNjBlwr80jPro1yUwg08iwgOHPd+ks/tQ6JzVlVBLVGDaOV32Rt/ImvkBE86RiQYgCYP/4zP5jpXKn7nPcEPf7vYWYy0ce8x/LlhH7JGTsCs9ftwJEbuFrvEWl/hxF3VvGYlt80pgW8UeN304pU5op04Hkvh/8Z5vyDFusSZh1jVhlSEKMYDh054X80+SG5eAAUBhmcmrJYm00uc2BTbDqpJVLVq5xGMWxg5A+aB46dxzgvTcdU7cwEAV787V5js8UvE1bkskyJe3fpGgU+9v0+x9/HgjnDKB7NyPP39WCGKpz3wwTnBq8m1WAbaR3/mCJRlCFN5+x0/lY89AgqP3NizkaPtv5hXNDm/79gpYTVOY7H7yCmMGLs04vdHJbUjHE5uaS9m2HyjwEOtz2iTZ/FUiSOeePR7dcvevSRWRIyovBxAURihykiPy974E13+PSXi93ZbVjEthTt9RPbTk9E9Shvs0rRmRXTMrOLqN1TmxHEi2Qu95BsFHkq0sL/cvAC6/XsKpq1xPnsdz1QpX8ZWYv1IyKp4HwkV3ey+r5Z48rsqLfBYseh21YTbYzhuycfOq5oYY/jilm744c6eLlvjDSKVbscG4uewfKvAoxlAu47kYteRXDw9IXKRUb9xe58mePriNo7TelrxwlKJh5jjaH3s28XbPZEZB4ftGh4XyK8uigyHg8EYXberz2+FK83H40B4evlU4fJ9rMBjn7hwdSDW7DqCXv8pmYkw3hl+VmOkl0tF9QplVDelGIEAQ35BAAfjYJWpTKLdf0dz85CbV+BJFXI72FUpPMblrZ8sLPFZXkGAeyK5Ulk3JQnc881t3aN+H0vPqH6O+1aB2wnJCzf8eW3aBmw76Gyl5oc3dMbLV3TAT//s5Wg/kQQt76+GR7/hrOw5mut5VZIjuXkY9eNKdHxqEk7GKHHmlaWkQk1GGwi1HfUbBr38hycr70QiYu0DYwwPjF2Kh7/jm19565ps121wcxi1KqdF/T7e59PUPv5cYOeaFQg4+R0zq6DPGTVd/45bgp0t00Fu9Gvfm1fMX7pLQORCKAHG8PV8I8QrN68A5cpED3X0AhWdLNaq3Zz9J3DMgyrkIuH1xoVGM33vItSudnp0Beo1fneF+VeB2zjx1kpsCzcfQOW0VMedPV4ewMk277TT+YHCeNPtDkcaPFgrrMduos97iwU7ynnuRqWlZGPCO58y8KXpha/d9I9ODcVM6ilN0Kb4lvaVC2Xm//XFgkf7A3DuQrnsjdkY8NIfnrXNa5JsXqliw/bE0ZdRcaNEpj/QR1g7ZPPW9A3F3ufmFTgyUHiVT46l8rwb++b96zsXe9+6bmUXv+acEQOax7x3Yj8c1HYyXynw+lXLo0bFsgBsWuBxYj2LwK6/UqXPLjTKZeqa3Th0wpjcXLf7KFZJLvtmBy8LFXidsfHZX9YUvt5/7BRaPDYRbzpIwBTtnurf0nu3YcWQCcxPburK9Ts8KvSx81vhnzYqfcW7D9xXCtyKnWFTSrL4p2NaKt8pcxuRYNeFsnjLIdw/dqmaG8/SxBOn83HjhwsKsxYOeOkP7Dgs3gcPGH74sQu2lrBIveber5agIMp1lXkJdh8xcvmMX7LdtmWdRCi2ruCNq88sfN2tcXVbv+HmPgv14IhM6SyD39fuUV4oxbcK3M7F3nbwJHaFKA2nt5soH1fv56a52t+uv/La9+fhm4XbcCQ3X/7gznJyR3xtLH1eLqHSUYABD3yzrNAiXbvrKDbuPea53O8Wb8eOQycxe8P+wpGGlaOCEirZgcfaJxDGWxbQyC6bFmqE8a5T4OmjTh4883MOYH8YRX39B/OdCxaMbxV4e5uB/32ed6c4RVlRiVREORJWJRKswk5k1Bz1ktBsdee+/AfOeWF6hK0jc0nHeo73yc0rwLB35uCGD0t2Zt7QOlmMW7QNreumo0VtI0selyIU2SCJFoeTFc1D35yNy97408PW8ONbBd6gmr1wutw8d7G48e0Bi47s2fm5m0pGXfA8AO/pX9I3OaxLJp65pE3Y7UW5i3jOVnDSeK3iUms81muDqvZDUmUQS6dmjZyAd2cYPv77xy4tNAx4jj3Zob8mZ/8JzM85gKyRE3BjmIe1KnyrwGWR5kEKSCkIfPI8dn4rW9vd+slCzzLDDWlbB8M6Z4b9LpIbeuFmh2F8HBo8GKp64nQBHvt+BfIVLd4Jjn7W7DqKVTuOxNja4LNbjElDN88/kX5+OwbHx2Zxh28s6WV57BSeEMofzHj3qXGUY8mn2kkezw9tr7oJXDAwrht7WJcGuLlXUZrRCmWSUSnN/nKBfJdl6S7qUBeds6qF/S7S8URK+H/ZG7Mdyeax5KyLxT6Zsxkz1u1z/BuA2GIdr/9ubzI3Nbl49+dzoYjT4HbEi5JnP6qr6HV+HIa1lQoFPmbqOu597bpqEoF3rs3GExe0xqMWizvA4Miad+u1eeXKjiUUS9Fvh//xgCCjl6ftBYKEn9OilpDfscu4fxSlZJBZoDo1SmSYvbUdYtoRzoXy5t/PLPGZtQShqtw20fC1Am+cYW/W/Pnf/ip8fVhi9RY3rHvmPFf7M+bcIzCgVa0Sedczq5W3VdJKJW7bF+y4tSunYcxVHQEAL9gceYnymDAwNKhWTsyP2aBTw5KjHJ4RiNNTH217Ow/Q0P037D3muA1nN8/AeW1ql/h8UJs6Uff7dVVRJkavJ+btElOBE9H7RLSHiFZYPqtGRJOIaJ35X0mxRieJnYLMXM83xJVJqzqVi1mhKpJo3WS6UT6+qYuj/VQk13f7fDm3dW28cmUH3NWvGc5vVxc5o4fgsk71be0bGgfuZgQy7rYe/Du7IM6fz1HpZyPaKPhQDvLRjV1QKc15aleZpfvsYscC/xDAoJDPRgKYwhhrBmCK+V46GZXKSpf57KVtPZcxKMQ6aFMv3fFvMLiLQnlkcEusevJc1KqcVjjALu9xoqqHzmvBtZ/VBcBTxIOIcFGHelw1C0OtfzfnvGaMzHheE6npVQTmsY72rODNMBrLBTSodW30aloDHRpUwaNDWhb7zos6lVaCK8e9ImbrGWN/AAidzr8IwEfm648AXCy4XXGL3fhz/t9Pxz/6NPFUhh2SkgjlyxiTl8E+cyJGulgAmOSicPOtZxvHXa+q4Uq4f2Dzwu+i9e1Xp6wvfC27cno8TmyJJpqF7sR6H39H9Ko7dp59PKt5U5KT8OnNXfH9HT1xc+/Gxb6LlU423uF9/NRijAUTTe8CIHcGRiFex1bfdnaTiJN4TmCM4WCY1YFec3+U4rN2qVelHJY8PgB39G2KHk1iL+me7OKhYYemNStG/G5hTnHb5rTtPiMAACAASURBVLr350mRK5Nocfb/+tF+vdH2DaIbP7w9K55dQF4vxXCdTpYxxogo4ikkouEAhgNAZmb4OF5NEXYu+KNDWuLpCaujbvPtou3CbmyZUQpBqpSPn8pDFaJUjfHS4q9ftRzW7/E+JYD16k64qxcOncjD6p1Hwn4fypfzt0b51hkiCkzEI18O7+aZ+5HX1NtNRHUAwPwf0fHIGHubMZbNGMvOyMjgFBc/1K3ifMj13eJtsTdywAXt68bc5pmfoyt4J8iwcC6PMGk4YmBz1KpcFu3qO58HEEU0tXLchluJl6oOHmJbD7hP1UAAWtdNR8+mNYp/Ien5bVd/h0aAiGzekscHCPw1g26Nq7uq+RkNXgX+A4DrzNfXARgvpjnxw9e3dscXt3Qr8XnZlGRUNhe22E25ee9XTtwK8WeFyOi/kbI8dmpYDXMf7s8VNSCKeDYMAwGGZdsO4bZPS9aqFIXI6x/NHRMPBbLjaeRnBzthhF8AmA3gDCLaRkQ3ARgNYAARrQPQ33yfUHRpVA3dI/hfg/NWdoZ8uz0oYyYdhyb4+j3O84KIsvLjQAc4IrSIwbOXtkVaahLKpiThhp5ZMfefs2k/LhwzK+Z213ZvyNtEoamJ/3NZOwBAyzriijfEe85uL4npA2eMDYvwVT/BbfENwdCxRPXZhRLsHo0zKmDj3uMxt//EzFehAtl9uU29ysXKyvESbPewLpkY1sX+XNFV78y1td11PbIK84iUlC3vpA3NboCh2Q0ifj/60rb48M+cYrVc/YzXGsLXKzFVEVy8EavM2aETpx1nPRNJBcETJ3YnYpLt1n+zELoClBfZE65uH+KydGfd9MirPEcMPANEReGbocg8o1d2yUQTh9E3bjOOXt8jS+kcixt8W9RYJT2b1sDUNXuiKqpxC7dhxNiluLd/84jbhMO6MOTNv3dC5XL8l+jsMzLw8/JdsTeMgVMl88NS51XK7xvg7DxFwguFGE1Fu03eFbzeXg/mykV5+A5uWwebnh0S8XvZoxqnp6L/i85zv1sZdWFrV/urRCvwMDw8OPqKwNeuOhM7Dp/Ea1PXR9xmhBkP/ftfzlYG5lmSa4SuyAxit0PN23TQkexIBEvTlbEZn85TZipaqJ4TZCsbUXli4tmNK3tUo3oys3GNCti4L7ar0A7RHpwi0C6UMAw/K/pKyHJlktEko6Int3XNSuJWhomq1ze0UwPcelZj3CvISvYSu8rmrWs64T+XuU+LkCi+2mjEuwUuGmuZObd8fKOzXEJO0QrcBXbKMjm5Gb+7vUfEyBcrsi2iMilJeGhwy8Kl9YCxWlIUt50tLnXAnI32ijhkN6yKKyIUiAhyl42q5W4JzpHImCv544G+nssQgYiVyG5wOwKoacnR5HWdUa3AXfDw4JYxFZmTm6FjppKkjraxRitUrSAuLnskZxIrN9i5LvcNaI6c0ZF9wzxYU8be1KsRPr+lG67vkYXrumcJlROOzOp8ue1le3ceCUk4JRu3j9Lz28VeaCcKrcBdULVCGTxxgb1yY4mAtSOLKqKgClXD9BTLxPdj57dCerlUjLqwtee+UldI1uDVKpTBVV3FpN1QlRhuUOvaKCcosioaWoG7REUsuKoJL6tckRa4CpxcNpGTag05rWBRPHReC3w5vOQK40jc1KuRklw4/76krZDRz/8Ncj66E3G537ymE1Y/FZqFWzxagbsklu9S9YSMSIIulM5ZVeM6asIOKgpPAMDVXflXRIrg1rOboFvj2PMsQRhTGyGjIj5b1b3Bg+8VeM7oIY5jrWWyYLOYUD4rZT1OQh+JYD8mIt8rcFV9VOG6Ll9SJ50/KmvTs4MFtiQ+8b0CB9SkO1Upu7rHVT4iUbjoRIl0dZS24wWKrnVykvxJTCturGFe15efMmQkxEKe67pnYdm2w5jKUU4rlMfPdzYp6ffJPEdYkniJeHA9e2lbHD6pps6gnzqpWy49s57jfawJ20TkSmlUw9twutJKQljgVSuUwfvXdxbyWzeaxXztEi9VU3i4qmsmnru8ne3tG5qdcHC76NW77TKsS6bQGHAnOJl8jpQjhAfGgBkP9sXY25wX5OalZW3nmf8KV5iSGAv861v5jlfFg9ZPD/eEUOBBVj15rnSZWT62LC5oVxe9m9WIvaFJvSrlsOapQfh710zf+8DtLMIKIjL1acW0FDSoVh6ds6rZ2v6pi9zn6eAZLbFiFrjrJigpQF4aSCgFbl0pqImNVYfVqmyvg6WlJivPVSECJ6v9bu7VCM8PbS9EbtdG9hR3kGskLPAJBytMmez+t368sxf3vry32vQH+vDL9NGsR0IpcFX4VZ8REffN6nMD3NHS9ZTkJFzeqT5qVHRfrYXn4bf6yUEY2KpWzKLAkeCxoJ0ULYlFW8mhgDUqlnG1hN1P/VkrcAGEK73mlHH/kOcTDZJf4GIG1u8anIPxd/bCExe0QpbkxTjlyiTj7WuzcX0PvhhynktljTi6qIO8peGh8BkY7jSwj/S3VuAiqFXZfQbBTg2dDa2tnNWcr1h0gQvnpsrQTVXUq1ION/RshIGtw6f59RqZLsKKZnrfyuVS8fzQ9ljwaH9pst3iJwvaLdppbGHSvWdx7af6fqnEmUs7wPgVca+mGZifI36Rkl1Skwl5nMUUujXmf1iqZEDLWlz78Tynr+uRhSQiXNO9IVKTk1DNZ8V+3eCnOR5tgVtoVqsS137Bggeq4FXCbuJ7/3lOU+59AeCC9uqG5R/e4C5HcwVFk+VJEpdxpiYn4cZejQone5XpNA65bpvqdv+rukau+SmaUq3AP7u5q5DfqV9VXYKink2rc4d5MfDPuLtVJhU5Rw0//bMX3vz7ma4iBdzW37z17Mau9peNCHcXEeFJjpDGzwX1MSe4fdi42f/mXo3QtCafIchDqVbgrepUFhY77iSuWCTvXdeZW4EHAur82LydpE29dAxqw7eQqGnNipj90Dl8gi2IKsAsC1Ex+3+LUk0+Ej2a2l9nEA7/ODMMZKcFLtU+8ORkEjYxpEoVpqUmc1tYZTxOikUE1KqUhl1Hcj2VY4crOzfA6MvsrzrVlF7c+MBv7+POteiUUm2Bp0apKi+TWEWUY8FrYbWtl164UMOLCIcaFcvi9r7hl8r7zbLi4dKOznOQxDNqlrU7F6pqIU6XRtW0BS4T62IOtzUe3UwIxiqiHI6vb+2O7YdOGLI5ZI6/oyeqmJEFI89rgcGcboloJPtoNl8TGxXFS3hQ1kwFw3BXCpyIcgAcBVAAIJ8xli2iUbII+q1/v78PqroMk/q/QS3w7C9rHO9XvQKf3C6NqgEwwuHc+jhVJZRyQ4WyyTh9wv5CJBWRQqnJSejUsCoWepAT3gkisgkCzkZNcx/uJ8T37o9HhkFAQYIgET6EvoyxDvGivJ3MJQYjKbJqVEB6eXclwm7uzReZICbm1N6Nc233opV8Mm61aL55t4ed4rBy+QPnelM4+bw24Rf13NyrER4a3ALj/tFDeGFkp4jSK3bv1W6Nq6FW5TTUdlGMwY+omAeLDyewQH65m28xjlt49ZGI4BW7HVT2EJgxoEcTd1EIokgv500Nz0in9NHzWxW6qAAjo+GZmXy5THi4p38zXN8jS+hvht6raalJaFG7KGTupSvam9uJu88u71Tf8T6qXD1lHBoVInArkQH4jYgWEtFwEQ1yi9+K7Yq42YL6u2aMlJ1WUaKG1bGIlC/dTxnfRPDL3b3x7e09pclrUbtyYay9qCsdaoGfmVkVb13TqfB98IHldHQUjbOaZzh+EH14g5jaANE4I8yiPxVuOrdnuhdj7EwA5wG4g4hKmL9ENJyIFhDRgr1797oUFxtVioFXD4uwwAvMeO5YbfBKZ0fy45e+bCnxBMM5LWsCAM7mzJUTi2u6NSyW9a/ATG2QqmhNxLJRA/H9HT25V1Tb5cc7e+GrW90nsBOBKwXOGNtu/t8D4DsAJdYoM8beZoxlM8ayMzK8uZGs+GSivJAWAooF5OYVAACaxVgBZrW6RSrX8XeGtywjWflDO9XHfQPkFaLmrQZjh3gsbHFl5wY4p0UtnJlZFTmjh3CnoY3GmqcG4by2xSOXgvNITTyqUhXruVA5LRUdBB1rpHmLWpXLom399GLuMZVwK3AiqkBElYKvAQwEsEJUw3iJ5JLwupI7z2Rkr6Y18Oqwjq5lX2LGG8cqOuDVwstIqQSu6ZYFoGS2xP8ObY+qnNE3PHRxWETB74y+rJ2URVqhdM6qhg9u6IwHzj3DE5lOcrh7RTSX58Ud5Mf9u7nKtQDMJKKlAOYBmMAYmyimWfyEO71fDe+G+SHpMBvHQSm07k2qc+cEsXJll0xsenZwzFl/a/pYry3HCmWScXf/ZgCAj2/sIjwS4wNBNVA1fERyVfY9o6ajakdOkD05eXe/ZiU+i1YD9zKOCVe3cJ9pxthGxlh78681Y+wZkQ3jJdw1TkmmuFyEILJJdkYAfc+oKU5gDLz2LLSpl44Jd/GX6nJLM0XFrK1RHypR0Z1kW+D3DmiOnNFD8NM/i+6zMVedKbUNsUi4MMJwlkFyUpKQyULR5OXLdaAOaFULbeqJK9DrlA+u74wrOBIiRaJ13XRhtSqdMvGes7D+mfMK318oKT3uhLt6Y/mogWG/u39gczx3uZx8Lyq6kyojrE29opJwXoWj8pJ4S+nDXONkik8L/MyG8uKCgxQNb+XPvvVtURN9W4gdBVzeqT5mrtuL75fswKDWtTFx5S6hvx8Jwxosuqdknc3kJEJKhBw+d55TcsjvFSqKHkQT6dciHW5JOAs8nKVdvmxyiYsvy1IJZc1Tgwpf927mfVROKPH3GHNPUHlar3E5ySlfZcXVA/ERaWVtwmVn1seLf/N+JBTJhTJiQHN8epP8vOPxQMJZ4OEsgyYZFZFnKeDbuEYFZGfJf2LXTU+Lm1zSXusbFeF11ktfoWwKTprhlTKIx3BCL7Ge6xckKG8gfHK0a7s3xD/6NBG6eCgc5csk48TpyPfT4LZq6qQmnAUeyTiR4UJZNmogHh3SMuL3Tc0FBoPb1sZTHNVNRJBhrtYsmxIfDxIRhFOezw81RlheV88J3lYyizzHhQWuoBGhVaBa1amMJy9q47nyBoBFjw3A6icHRfz+lSvdhwPzkIAWePjPZUxiVk5LtVWZ5/WrO8Xcxiueu7w9+p6xE23rp8fe2GdYJ7DPapYhJYmUkpFGQjrCYhNqgVdxmYDOCbFGzl6FTsYi4RR4JEtblsUQrT/HQ7dLL5eKK7tkCvmtga1qoYPEBE2RCF7aSmkpJT6ThUxFHg8WuAriYSFPvJFwLpRouK2kbodoHTnROt7b12ZHLCEl06UQPOfdGlcv/Ez2EL+0+cBVECcFtOKKhDsl0frtHX2bokr5VIw8z5vc0LFQpb8fP7+V9IUnqicxZSPzgZWSRKhTinJtByN8QkfX8RIaPKSd+GpWdkk8BR5FTaalJmPJ4wMxsLV3M8bxaIjd2KsRJt13tupmeEY8nHO5LhTC7If6yRNooXktNStQgZI+8DjR33hN4erMxFPgIRe1XQJO1sUjk+9TU0hDNSpdJwNa1UJ2w6pSZX57e0/8OfIcqTKDhEahaErBJOZ3EpPoA9EXdKgIvZJFejl16TVlLqKJ2AYFMt+51qhimDVygjSZFcumCEnAxkNJCzxx+5NdEs8Ct7yeOuLsUjtzXa9KOaXyVSg0FR1apu+7tBK8rqGXt3T27OIkngK3XNXGGer8deFoXVdeIqlZkoe5aakJdyvZIg6M/4QnOMIKfUCXUtusGAnX64IX+f6B8iq+2OGJC1qFzS+cKFRKS8XPd/VWIlulDg3K7lrKikbIJHiOQ9fKqHahdG9cXapRFo6E84EDkcshySCSRda+QRUpS35V0krxzayiOwevd4Nq5ZEzeohUf3RpId8sJaVqtWMkvhiuvi5mfJ2RBCZeYlalIdMsjgM3Rim7ulIpKpZcXF3pc56gFng8om+2RKX40+PJi1qjQYQaoRo+gqUAU5KNXpRZrTy2HDjhWfFkP6EVuGBCoxKqVyiD/cdPS1t00KJ2Jew7dlqOsCjIjM4oMIfYKiOOgv7Ya7tnKZEfLGydiBSEuFA6NKiCf1/SFl1LaREHK1qBe0x6+VRDgUuywSfeU/oW1AQiLLWWQTxEoaic85HBQ4NbgMjIdTP9r70IMIZezWqoblZcoH3gHpMUIYY1UflSwcSOaaApDSsrJZdXCTUrpeHFv3UorLIUDw/NeEErcMGE3lylrWO3rG1EosjsZEotcOkSSy/B66sXTxWhFbjHnMo3SrkFJ2ASHgWHGVTgKnzgRYtMpIsudQQvbyAQfbvShPaBe0TrupWRWa08Fmw+CACoojBXiEyCikxm4qHgJJcKJfrIkFY4mbcc3ZtUj72xxhXB6xvQPpRCtAUumOCt1bNpDbzx906FyqW0WODBo7RTWk4U/VvWAgA0riE/rKxpzYr4cnh3lC+jbSGvoUIXiiaIKwVORIOIaC0RrSeikaIalQgE1VdegTHeC12EkKgEJxTDVRD3imu7N8TSJwYis7qOv05kejQxlq6PiLM0GSrhNhuIKBnAawAGANgGYD4R/cAYWyWqcYlAfkHpssADpgaX6UIhIqSXKypwW1oTayU6ldJSMUFRvp14xc24rwuA9YyxjQBARF8CuAhAqVbgoe65l67ogP9NXVcYApXoBA1vVSW/Vv7rXD2hqCk1uFHg9QBstbzfBqBr6EZENBzAcADIzBRTDT2eCWYn69DAqNY+qE1tDGrjXQm3aCRRkUtDFlXKl8HzQ9ujt6KFFhUUFRvQaFRAvNVMiOhyAIMYYzeb768B0JUxdmekfbKzs9mCBQu45PmJHYdOoq7iggoAcDQ3DwGGYu4FjUbjP4hoIWMsO/RzN+bKdgANLO/rm5+VeuJBeQOGz1Cj0SQubmZ75gNoRkSNiKgMgCsB/CCmWRqNRqOJBbcFzhjLJ6I7AfwKIBnA+4yxlcJaptFoNJqouJrxYYz9DOBnQW3RaDQajQN0wKxGo9H4FK3ANRqNxqdoBa7RaDQ+hTsOnEsY0V4Amzl3rwFgn8Dm+EF2aZOrUrY+5tIh26/H3JAxlhH6oVQF7gYiWhAukD2RZZc2uSpl62MuHbIT7Zi1C0Wj0Wh8ilbgGo1G41P8pMDfLoWyS5tclbL1MZcO2Ql1zL7xgWs0Go2mOH6ywDUajUZjQStwjUaj8SlagWs0Go1P0Qo8DiBSVwSMiJTcAwrlKjnXRFROsfxSVWiutBxvXChwIsow/0tvDxE1I6IzFMhtQUSdAYBJnkkmonZE9HdTdkCi3C5E9JhsuabsbkT0PwCNJMvtRESfAegPyL3WRNSWiC4nonKS5TYjolay5FnktiaiPoCSPlXH/C+1+K3SAoJEVBlGZftziKgvY+wvIkqS0bmJqAqA5wB0A7CfiCYAeIsxdtRjudUAPAWgF4BtRPQngJcYYye8lBvCRwDKE9Faxth8r8+5ea6fglEI+yPzMynX2ZT1AIBrALwDYDsRJTPGCjyWWR3AKADZANoB+N38XIbssgDGAOgMI3VFTyJ6iTG2RZLcrgA2EdFPACYyxrYSEXmlVE3DbwyAcwBsIaJ+AMYzxhZIuLcrAngDwNVE1J4xtlzGNQ6i2gK/FkA+gC8A/AuQY5mZT8mnARQwxtoBeBBAbwB1vZYN4N8wDIT2AO4FcDGA8hLkgohSzOpJUwF8DeBuGI0JeDzkHAPgbMZYV8bY60GZHsoLpRaAGxlj/2OMnZKgQMvBOOYAY6w7gGEALgQASR37bADpjLEOAG4E0ByADAOhN4DKZp8aAaAJgFuJqKzHFnEVABUZYy0AXA1gP4ARRFRRwn12Pozi7i/DUOSyrjEABQqciM4kohbm208APALgGQBNiOg8cxtPhiGm7GbmCX4NhuIGY2w+gLIwrHGv5AaP+T5L4ecuAHYDaO2FXIvsZoBRRcn8uD2ASQAYEQUVCxOpxE25Lc23zwNIIqJUIrqAiB4iosFElCZKXhjZzczXtQB0B7CciAYQ0VgiupOIepjfiz7mZoyxkwBuZozdbX7FYFj+1UTJiiA76Ao8DaCv+boPgHQYo9z6HsstAyDDtLbXAwjAeJhc5IHcRpb7pxqAHkRUgTG2F8A4AAcB3GluK9Q4MWUHC9/+CuBlxth9ADKJ6EpzGzneDcaYlD8YvscJAGYDmAugX8j3NwH4Q5LsvpbvUsz/PwE402O551i+GwwgB4YV/gsMa7i617IBVAXwovn6AgBTYFiLtTySO8D8/B0YltEkAP8EMAfAAwCqeXjMQdmfABgP4AMAQwE8CeBHAM08Ptep5v/OAFYH33t8j/UzP3/VPOY9AG4G8Kl5net7JLcPgGYA3jXPb23zvP/HfF9BkNwss79MgaGoW5mfvw/gMfN1CoB+AL4EUEfguQ6VfUbI95cD2CL6Gkf789QCD3ny3Q9gCTOGlN/DUNhWPgNwnIw6mzCH+l7JviXMLmkwUz26eWLHkHtz8AvG2M+MsSzG2EsA/gvDSqzKK9eB7HwAVYmoIYxhfRcAtRlju3lHPjbl3gvgCcbYAMbY/2CMvDoCqMwj06bs4D32lilrCmNsLIBXAKwH0MMjuTcDAGMsz/w/H8AuAJfyyrMpezyKn+9NAAYyxt4F8CyMUSb3hH0UuT8AuIExtg6GGyETxgNjJgzff2PG2HHefhVG7lzGWD8A0wD8y5ww/RBANyJqzIyR5m4AuXDpnowh+ykiKhw9M8a+gTGv9S9zX09GmFa8dqGkAYUn4TiAPPPzdACrLUMvMMZyATwE4AYiegLAQ0SU7rVsZhRnzgawizG2hYhuBzDcMkTyRK65TfD8zwBQHYDbCVQ7stMAVACw0Pzu7zAUelPG77uLJncFEbVijB1jjI2xdIiZAGrCvW82muxVpktjJowRVjDyZj+AegDcFOF2cp3LA5gFcXMdkWRXhnHMrcxruQ/AIABgRsHxBgC2eSC3EoANRNSCMbYQxoPzAsbYWwAWAyjn0g8elBt0S6wCAMbYGBgGyDAAOwDMgxGYAMbYCgANAZzilGlX9tVEVNOy/cUA7iKiUQBeMV14nuGJAjd9jZMA/JeI/mZeuJkAmhHRYhg3VTKAT4looKVT1wTQBkbI1TeMscMeyz7X3K0NgFZE9CsMf91UZvgxvTzmFGZMHg6B4VZYA+AIj5ViU3YKDBdCNwATAfRkjN0C4DeYox+P5CYD+Mg85iTGGDOP+VcYneGIU7kOZX9GRP1hWKRpRPQ0Ec0GUACO4iI89zYzIozqw3ClcONA9odkzCetBHAZET1JRDNguFP2OL3HbMpNAvAJEQ2EMaVykogugeGqmsMYc6xIw8jNB3AAQEciak9E7QGsgOHaSIYRIFCPiP5HRCtgXN/DgvpUJNmZMHzwQTJgPEj7ABjDGNvtVLYjRPtkADSF4RO7CMaw9XMA95vfnQHgW8u2j8EIoQOMGevxAIZKlP0/8/WDMIa4AyTJfRHGcHYogAUALpZ0zE8AeN7yngAkybrOMDr5EBhW2UUSr/MY83VNGCFu50uS+zKKEsa145XLeZ3/a77ubb6/VNZ1Nl+3B/AngEsEyf0CwO0wrP3HYIyoZsII0/wcwD3mfrVguMYuFHiuY8m+09yvPoA3AVzBK9txW4X8iNExk8zXVwN43fLdjQAOmSc2A4b/saX5XS8A34BTiQiSTQCqSpY71pRbTtUxKzzXKaXtmBXc273d9CsBx+yF3JtMuRnm+8aW7+6AEfUDj66zLdkq/ly7UIjoBhh+tafMj5YDuJKIgiveUgFsNL8/CmO4cRcR3Q1jcmkyjHA2nmGOW9lTmMFByXKnAgBz6KYRJHuyU5mC5E4BioUyypSt6pi55AqQ/SY4+5Wq/mxDbgqADTBGcoAxQQsiGg5DwS4C+FZgipKtBDfaH0BFGLPud5sH0cL8/GUYw45ZMGaj28IIv6kAoCWMMLKPAHTzm2x9zPqY9TErlTsBZsgrgHsAzAfQWdK5FipbxJ/7HwAyzf+jAXxlvk6G8WTuZb5vYF7gMkIbr0i2PmZ9zPqYlcn9EEBZ8315v8t2++fahcKK8iu8DKAREZ3LjBCmw8wI3wKA22BEOQhdYqpKtj5mfcz6mJXJPQFjLQOYoPxBKmW7RuTTAMCtAKZb3neBEVnyM4zFIp49iVTJ1sesj1kfc2LIVS2b509YTUwzxjdARN8A2AkjgH4ygHWMsQ1ChMSZbH3M+pj1MSeGXNWyeRG2kMc88PIwYm2HwcgJMFHGgauSrY9ZH7OXclXKLm1yVcvmRXTGrNthzOQOYBwrr3wqWx+zXPQxa7mJKtsxwlwogNwk/fEiWx9z6ZCtjznx5aqWzYNQBa7RaDQaeaiuyKPRaDQaTrQC12g0Gp+iFbhGo9H4FK3ANQkLERUQ0RIiWklES4loBBUV0Yi0TxYRXSWrjRqNG7QC1yQyJxljHRhjrQEMAHAejNzY0cgCoBW4xhfoKBRNwkJExxhjFS3vG8PIIFcDRrmtT2Bk1AOMpPx/EtEcGBn2NsFI2PQqjCRHfWAU4XiNGaXCNBrlaAWuSVhCFbj52SEYlWSOAggwxnKJqBmALxhj2UTUB0bFmfPN7YcDqMkYe5qIysJILzqUMbZJ6sFoNGEQvRJTo/ELqQDGEFEHGFn1mkfYbiCAdkR0ufk+HUAzmEn9NRqVaAWuKTWYLpQCGMV9nwCwG0btxiQAuZF2A/BPxtivUhqp0ThAT2JqSgVElAGj1NgYZvgN0wHsNJdNXwMjgT9guFYqWXb9FcA/iCjV/J3mRFQBGk0coC1wTSJTjoiWwHCX5MOYtHzR/O51AOOI6FoAE2EUKACAZQAKiGgpjAosr8CITFlk1nncC+BiWQeg0URDT2JqNBqNT9EuFI1Go/EpWoFrNBqNT9EKXKPRaHyKVuAajUbjU7QC12g0Gp+iDicsNAAAABxJREFUFbhGo9H4FK3ANRqNxqdoBa7RaDQ+5f8BMpyQaK3m1XQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIaT27BCyCae"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPDIQXQkyPqn"
      },
      "source": [
        "**Processing Time Series Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1TWlXZYyNQ0"
      },
      "source": [
        "def timeseries_to_supervised(data, lag=1):\n",
        "  dataset = pd.DataFrame(data)\n",
        "  columns = [dataset.shift(i) for i in range(1, lag+1)]\n",
        "  columns.append(dataset)\n",
        "  dataset = pd.concat(columns, axis=1)\n",
        "  return dataset\n",
        "\n",
        "def difference(df, interval=1):\n",
        "  diff = list()\n",
        "  for i in range(interval, len(df)):\n",
        "    value = df[i]- df[i - interval]\n",
        "    diff.append(value)\n",
        "  return pd.Series(diff)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stgPUJRSy0CH",
        "outputId": "606cd329-a93f-4ea3-87b7-dd0df53a03e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "lag = 1\n",
        "\n",
        "raw_values = df.values\n",
        "diff_values = difference(raw_values, 1)\n",
        "\n",
        "diff_values"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      -2.8\n",
              "1       0.9\n",
              "2      -4.2\n",
              "3       1.2\n",
              "4       0.0\n",
              "       ... \n",
              "3644   -0.6\n",
              "3645   -0.4\n",
              "3646   -0.1\n",
              "3647    2.2\n",
              "3648   -2.7\n",
              "Length: 3649, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX4GHdUIy6hi",
        "outputId": "efb134f0-1d73-4632-8913-5dc3c6d259f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "supervised = timeseries_to_supervised(diff_values, lag)\n",
        "supervised"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>-2.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.8</td>\n",
              "      <td>0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.9</td>\n",
              "      <td>-4.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-4.2</td>\n",
              "      <td>1.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3644</th>\n",
              "      <td>1.7</td>\n",
              "      <td>-0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3645</th>\n",
              "      <td>-0.6</td>\n",
              "      <td>-0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3646</th>\n",
              "      <td>-0.4</td>\n",
              "      <td>-0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3647</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>2.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3648</th>\n",
              "      <td>2.2</td>\n",
              "      <td>-2.7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3649 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0    0\n",
              "0     NaN -2.8\n",
              "1    -2.8  0.9\n",
              "2     0.9 -4.2\n",
              "3    -4.2  1.2\n",
              "4     1.2  0.0\n",
              "...   ...  ...\n",
              "3644  1.7 -0.6\n",
              "3645 -0.6 -0.4\n",
              "3646 -0.4 -0.1\n",
              "3647 -0.1  2.2\n",
              "3648  2.2 -2.7\n",
              "\n",
              "[3649 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_FXp0w8zTPf",
        "outputId": "5c0aa59f-a4eb-435b-de91-3d0c3a560211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "supervised_values = supervised.values[lag:,:]\n",
        "supervised_values"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.8,  0.9],\n",
              "       [ 0.9, -4.2],\n",
              "       [-4.2,  1.2],\n",
              "       ...,\n",
              "       [-0.4, -0.1],\n",
              "       [-0.1,  2.2],\n",
              "       [ 2.2, -2.7]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbpPmkzTzZe4"
      },
      "source": [
        "split_percentage = 0.75\n",
        "\n",
        "train_size = int(split_percentage * len(supervised_values))\n",
        "\n",
        "train, test = supervised_values[0:train_size], supervised_values[train_size:len(supervised_values)]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUmGtM7jzqgs"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1)) # Range hasil scaling menjadi angka diantara -1 hingga 1\n",
        "scaler = scaler.fit(train)\n",
        "\n",
        "train_scaled = scaler.transform(train)\n",
        "test_scaled = scaler.transform(test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZnlLbS8z5yO",
        "outputId": "cc2a3085-e09c-4b51-e7dd-9f2bd74aed67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "train_scaled"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.11607143,  0.21428571],\n",
              "       [ 0.21428571, -0.24107143],\n",
              "       [-0.24107143,  0.24107143],\n",
              "       ...,\n",
              "       [-0.16071429,  0.375     ],\n",
              "       [ 0.375     ,  0.125     ],\n",
              "       [ 0.125     , -0.16071429]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT5ykkl-z8x7"
      },
      "source": [
        "## **Baseline Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z93rl-HRz7bV",
        "outputId": "b6d88a1e-2043-435c-cc61-e9c3db908da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "neurons = 1\n",
        "epoch = 1000\n",
        "batch_size = 32\n",
        "\n",
        "X_train, y_train = train_scaled[:, 0:-1], train_scaled[:, -1]\n",
        "X_test, y_test = test_scaled[:, 0:-1], test_scaled[:, -1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(neurons, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "history = model.fit(X_train, y_train, epochs=epoch, batch_size=batch_size, validation_data=(X_test, y_test))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.1013 - val_loss: 0.0741\n",
            "Epoch 2/1000\n",
            "86/86 [==============================] - 0s 839us/step - loss: 0.0691 - val_loss: 0.0589\n",
            "Epoch 3/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0610 - val_loss: 0.0556\n",
            "Epoch 4/1000\n",
            "86/86 [==============================] - 0s 756us/step - loss: 0.0592 - val_loss: 0.0549\n",
            "Epoch 5/1000\n",
            "86/86 [==============================] - 0s 839us/step - loss: 0.0588 - val_loss: 0.0547\n",
            "Epoch 6/1000\n",
            "86/86 [==============================] - 0s 746us/step - loss: 0.0588 - val_loss: 0.0547\n",
            "Epoch 7/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0587 - val_loss: 0.0546\n",
            "Epoch 8/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0587 - val_loss: 0.0546\n",
            "Epoch 9/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0587 - val_loss: 0.0546\n",
            "Epoch 10/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 11/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 12/1000\n",
            "86/86 [==============================] - 0s 872us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 13/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 14/1000\n",
            "86/86 [==============================] - 0s 724us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 15/1000\n",
            "86/86 [==============================] - 0s 861us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 16/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 17/1000\n",
            "86/86 [==============================] - 0s 834us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 18/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 19/1000\n",
            "86/86 [==============================] - 0s 958us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 20/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 21/1000\n",
            "86/86 [==============================] - 0s 882us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 22/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 23/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 24/1000\n",
            "86/86 [==============================] - 0s 920us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 25/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 26/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 27/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 28/1000\n",
            "86/86 [==============================] - 0s 906us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 29/1000\n",
            "86/86 [==============================] - 0s 745us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 30/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 31/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0586 - val_loss: 0.0544\n",
            "Epoch 32/1000\n",
            "86/86 [==============================] - 0s 888us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 33/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 34/1000\n",
            "86/86 [==============================] - 0s 744us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 35/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 36/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 37/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 38/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 39/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 40/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 41/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 42/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 43/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 44/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 45/1000\n",
            "86/86 [==============================] - 0s 918us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 46/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 47/1000\n",
            "86/86 [==============================] - 0s 750us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 48/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 49/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 50/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 51/1000\n",
            "86/86 [==============================] - 0s 749us/step - loss: 0.0586 - val_loss: 0.0544\n",
            "Epoch 52/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 53/1000\n",
            "86/86 [==============================] - 0s 832us/step - loss: 0.0586 - val_loss: 0.0544\n",
            "Epoch 54/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 55/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 56/1000\n",
            "86/86 [==============================] - 0s 753us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 57/1000\n",
            "86/86 [==============================] - 0s 830us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 58/1000\n",
            "86/86 [==============================] - 0s 831us/step - loss: 0.0586 - val_loss: 0.0544\n",
            "Epoch 59/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 60/1000\n",
            "86/86 [==============================] - 0s 929us/step - loss: 0.0586 - val_loss: 0.0544\n",
            "Epoch 61/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 62/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 63/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 64/1000\n",
            "86/86 [==============================] - 0s 901us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 65/1000\n",
            "86/86 [==============================] - 0s 758us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 66/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 67/1000\n",
            "86/86 [==============================] - 0s 962us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 68/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 69/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 70/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 71/1000\n",
            "86/86 [==============================] - 0s 925us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 72/1000\n",
            "86/86 [==============================] - 0s 895us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 73/1000\n",
            "86/86 [==============================] - 0s 950us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 74/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 75/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 76/1000\n",
            "86/86 [==============================] - 0s 899us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 77/1000\n",
            "86/86 [==============================] - 0s 730us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 78/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 79/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 80/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 81/1000\n",
            "86/86 [==============================] - 0s 927us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 82/1000\n",
            "86/86 [==============================] - 0s 848us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 83/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 84/1000\n",
            "86/86 [==============================] - 0s 760us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 85/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 86/1000\n",
            "86/86 [==============================] - 0s 917us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 87/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 88/1000\n",
            "86/86 [==============================] - 0s 920us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 89/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 90/1000\n",
            "86/86 [==============================] - 0s 736us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 91/1000\n",
            "86/86 [==============================] - 0s 749us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 92/1000\n",
            "86/86 [==============================] - 0s 753us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 93/1000\n",
            "86/86 [==============================] - 0s 861us/step - loss: 0.0586 - val_loss: 0.0544\n",
            "Epoch 94/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 95/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 96/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 97/1000\n",
            "86/86 [==============================] - 0s 845us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 98/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 99/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 100/1000\n",
            "86/86 [==============================] - 0s 868us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 101/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 102/1000\n",
            "86/86 [==============================] - 0s 927us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 103/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 104/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 105/1000\n",
            "86/86 [==============================] - 0s 914us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 106/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 107/1000\n",
            "86/86 [==============================] - 0s 922us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 108/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 109/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 110/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 111/1000\n",
            "86/86 [==============================] - 0s 876us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 112/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 113/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 114/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 115/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 116/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 117/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 118/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 119/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 120/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 121/1000\n",
            "86/86 [==============================] - 0s 756us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 122/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0586 - val_loss: 0.0544\n",
            "Epoch 123/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 124/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 125/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 126/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 127/1000\n",
            "86/86 [==============================] - 0s 952us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 128/1000\n",
            "86/86 [==============================] - 0s 758us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 129/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 130/1000\n",
            "86/86 [==============================] - 0s 880us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 131/1000\n",
            "86/86 [==============================] - 0s 859us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 132/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 133/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 134/1000\n",
            "86/86 [==============================] - 0s 747us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 135/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 136/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 137/1000\n",
            "86/86 [==============================] - 0s 991us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 138/1000\n",
            "86/86 [==============================] - 0s 875us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 139/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 140/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 141/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 142/1000\n",
            "86/86 [==============================] - 0s 748us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 143/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 144/1000\n",
            "86/86 [==============================] - 0s 755us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 145/1000\n",
            "86/86 [==============================] - 0s 879us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 146/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 147/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 148/1000\n",
            "86/86 [==============================] - 0s 741us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 149/1000\n",
            "86/86 [==============================] - 0s 749us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 150/1000\n",
            "86/86 [==============================] - 0s 954us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 151/1000\n",
            "86/86 [==============================] - 0s 756us/step - loss: 0.0586 - val_loss: 0.0544\n",
            "Epoch 152/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 153/1000\n",
            "86/86 [==============================] - 0s 870us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 154/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 155/1000\n",
            "86/86 [==============================] - 0s 854us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 156/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 157/1000\n",
            "86/86 [==============================] - 0s 935us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 158/1000\n",
            "86/86 [==============================] - 0s 725us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 159/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 160/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 161/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0586 - val_loss: 0.0544\n",
            "Epoch 162/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 163/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 164/1000\n",
            "86/86 [==============================] - 0s 848us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 165/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 166/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 167/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0586 - val_loss: 0.0544\n",
            "Epoch 168/1000\n",
            "86/86 [==============================] - 0s 834us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 169/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 170/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 171/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 172/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 173/1000\n",
            "86/86 [==============================] - 0s 861us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 174/1000\n",
            "86/86 [==============================] - 0s 760us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 175/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 176/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 177/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 178/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 179/1000\n",
            "86/86 [==============================] - 0s 914us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 180/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 181/1000\n",
            "86/86 [==============================] - 0s 870us/step - loss: 0.0586 - val_loss: 0.0544\n",
            "Epoch 182/1000\n",
            "86/86 [==============================] - 0s 862us/step - loss: 0.0587 - val_loss: 0.0544\n",
            "Epoch 183/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0586 - val_loss: 0.0544\n",
            "Epoch 184/1000\n",
            "86/86 [==============================] - 0s 739us/step - loss: 0.0586 - val_loss: 0.0544\n",
            "Epoch 185/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 186/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 187/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 188/1000\n",
            "86/86 [==============================] - 0s 752us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 189/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 190/1000\n",
            "86/86 [==============================] - 0s 831us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 191/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 192/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 193/1000\n",
            "86/86 [==============================] - 0s 758us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 194/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 195/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 196/1000\n",
            "86/86 [==============================] - 0s 909us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 197/1000\n",
            "86/86 [==============================] - 0s 756us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 198/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 199/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 200/1000\n",
            "86/86 [==============================] - 0s 843us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 201/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 202/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 203/1000\n",
            "86/86 [==============================] - 0s 760us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 204/1000\n",
            "86/86 [==============================] - 0s 926us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 205/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 206/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 207/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 208/1000\n",
            "86/86 [==============================] - 0s 837us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 209/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 210/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 211/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 212/1000\n",
            "86/86 [==============================] - 0s 877us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 213/1000\n",
            "86/86 [==============================] - 0s 742us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 214/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 215/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 216/1000\n",
            "86/86 [==============================] - 0s 932us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 217/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 218/1000\n",
            "86/86 [==============================] - 0s 871us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 219/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 220/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 221/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 222/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 223/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 224/1000\n",
            "86/86 [==============================] - 0s 945us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 225/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 226/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 227/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 228/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 229/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 230/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 231/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 232/1000\n",
            "86/86 [==============================] - 0s 841us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 233/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 234/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 235/1000\n",
            "86/86 [==============================] - 0s 966us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 236/1000\n",
            "86/86 [==============================] - 0s 751us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 237/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 238/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 239/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 240/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 241/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 242/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 243/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 244/1000\n",
            "86/86 [==============================] - 0s 859us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 245/1000\n",
            "86/86 [==============================] - 0s 849us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 246/1000\n",
            "86/86 [==============================] - 0s 904us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 247/1000\n",
            "86/86 [==============================] - 0s 748us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 248/1000\n",
            "86/86 [==============================] - 0s 751us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 249/1000\n",
            "86/86 [==============================] - 0s 940us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 250/1000\n",
            "86/86 [==============================] - 0s 811us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 251/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 252/1000\n",
            "86/86 [==============================] - 0s 749us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 253/1000\n",
            "86/86 [==============================] - 0s 899us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 254/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 255/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 256/1000\n",
            "86/86 [==============================] - 0s 729us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 257/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 258/1000\n",
            "86/86 [==============================] - 0s 862us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 259/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 260/1000\n",
            "86/86 [==============================] - 0s 740us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 261/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 262/1000\n",
            "86/86 [==============================] - 0s 901us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 263/1000\n",
            "86/86 [==============================] - 0s 911us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 264/1000\n",
            "86/86 [==============================] - 0s 841us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 265/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 266/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 267/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 268/1000\n",
            "86/86 [==============================] - 0s 741us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 269/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 270/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 271/1000\n",
            "86/86 [==============================] - 0s 858us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 272/1000\n",
            "86/86 [==============================] - 0s 943us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 273/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 274/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 275/1000\n",
            "86/86 [==============================] - 0s 911us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 276/1000\n",
            "86/86 [==============================] - 0s 840us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 277/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 278/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 279/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 280/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 281/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 282/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 283/1000\n",
            "86/86 [==============================] - 0s 753us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 284/1000\n",
            "86/86 [==============================] - 0s 874us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 285/1000\n",
            "86/86 [==============================] - 0s 941us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 286/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 287/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 288/1000\n",
            "86/86 [==============================] - 0s 754us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 289/1000\n",
            "86/86 [==============================] - 0s 894us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 290/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 291/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 292/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 293/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 294/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 295/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 296/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 297/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 298/1000\n",
            "86/86 [==============================] - 0s 839us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 299/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 300/1000\n",
            "86/86 [==============================] - 0s 854us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 301/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 302/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 303/1000\n",
            "86/86 [==============================] - 0s 925us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 304/1000\n",
            "86/86 [==============================] - 0s 760us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 305/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 306/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 307/1000\n",
            "86/86 [==============================] - 0s 837us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 308/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 309/1000\n",
            "86/86 [==============================] - 0s 748us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 310/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 311/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 312/1000\n",
            "86/86 [==============================] - 0s 973us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 313/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 314/1000\n",
            "86/86 [==============================] - 0s 756us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 315/1000\n",
            "86/86 [==============================] - 0s 748us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 316/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 317/1000\n",
            "86/86 [==============================] - 0s 927us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 318/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 319/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 320/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 321/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 322/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 323/1000\n",
            "86/86 [==============================] - 0s 868us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 324/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 325/1000\n",
            "86/86 [==============================] - 0s 910us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 326/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 327/1000\n",
            "86/86 [==============================] - 0s 922us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 328/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 329/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 330/1000\n",
            "86/86 [==============================] - 0s 838us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 331/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 332/1000\n",
            "86/86 [==============================] - 0s 804us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 333/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 334/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 335/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 336/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 337/1000\n",
            "86/86 [==============================] - 0s 767us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 338/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 339/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 340/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 341/1000\n",
            "86/86 [==============================] - 0s 936us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 342/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 343/1000\n",
            "86/86 [==============================] - 0s 843us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 344/1000\n",
            "86/86 [==============================] - 0s 894us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 345/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 346/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 347/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 348/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 349/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 350/1000\n",
            "86/86 [==============================] - 0s 733us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 351/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 352/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 353/1000\n",
            "86/86 [==============================] - 0s 899us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 354/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 355/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 356/1000\n",
            "86/86 [==============================] - 0s 755us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 357/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 358/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 359/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 360/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 361/1000\n",
            "86/86 [==============================] - 0s 767us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 362/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 363/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 364/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 365/1000\n",
            "86/86 [==============================] - 0s 840us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 366/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 367/1000\n",
            "86/86 [==============================] - 0s 970us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 368/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 369/1000\n",
            "86/86 [==============================] - 0s 842us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 370/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 371/1000\n",
            "86/86 [==============================] - 0s 834us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 372/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 373/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 374/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 375/1000\n",
            "86/86 [==============================] - 0s 834us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 376/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 377/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 378/1000\n",
            "86/86 [==============================] - 0s 832us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 379/1000\n",
            "86/86 [==============================] - 0s 916us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 380/1000\n",
            "86/86 [==============================] - 0s 969us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 381/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 382/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 383/1000\n",
            "86/86 [==============================] - 0s 758us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 384/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 385/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 386/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 387/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 388/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 389/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 390/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 391/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 392/1000\n",
            "86/86 [==============================] - 0s 832us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 393/1000\n",
            "86/86 [==============================] - 0s 849us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 394/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 395/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 396/1000\n",
            "86/86 [==============================] - 0s 843us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 397/1000\n",
            "86/86 [==============================] - 0s 754us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 398/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 399/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 400/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 401/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 402/1000\n",
            "86/86 [==============================] - 0s 937us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 403/1000\n",
            "86/86 [==============================] - 0s 907us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 404/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 405/1000\n",
            "86/86 [==============================] - 0s 848us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 406/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 407/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 408/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 409/1000\n",
            "86/86 [==============================] - 0s 744us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 410/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 411/1000\n",
            "86/86 [==============================] - 0s 755us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 412/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 413/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 414/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 415/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 416/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 417/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 418/1000\n",
            "86/86 [==============================] - 0s 740us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 419/1000\n",
            "86/86 [==============================] - 0s 924us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 420/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 421/1000\n",
            "86/86 [==============================] - 0s 811us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 422/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 423/1000\n",
            "86/86 [==============================] - 0s 840us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 424/1000\n",
            "86/86 [==============================] - 0s 767us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 425/1000\n",
            "86/86 [==============================] - 0s 842us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 426/1000\n",
            "86/86 [==============================] - 0s 750us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 427/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 428/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 429/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 430/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 431/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 432/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 433/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 434/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 435/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 436/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 437/1000\n",
            "86/86 [==============================] - 0s 951us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 438/1000\n",
            "86/86 [==============================] - 0s 743us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 439/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 440/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 441/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 442/1000\n",
            "86/86 [==============================] - 0s 841us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 443/1000\n",
            "86/86 [==============================] - 0s 742us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 444/1000\n",
            "86/86 [==============================] - 0s 742us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 445/1000\n",
            "86/86 [==============================] - 0s 837us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 446/1000\n",
            "86/86 [==============================] - 0s 864us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 447/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 448/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 449/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 450/1000\n",
            "86/86 [==============================] - 0s 858us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 451/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 452/1000\n",
            "86/86 [==============================] - 0s 861us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 453/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 454/1000\n",
            "86/86 [==============================] - 0s 917us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 455/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 456/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 457/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 458/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 459/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 460/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 461/1000\n",
            "86/86 [==============================] - 0s 883us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 462/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 463/1000\n",
            "86/86 [==============================] - 0s 849us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 464/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 465/1000\n",
            "86/86 [==============================] - 0s 912us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 466/1000\n",
            "86/86 [==============================] - 0s 893us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 467/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 468/1000\n",
            "86/86 [==============================] - 0s 830us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 469/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 470/1000\n",
            "86/86 [==============================] - 0s 748us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 471/1000\n",
            "86/86 [==============================] - 0s 908us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 472/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 473/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 474/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 475/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 476/1000\n",
            "86/86 [==============================] - 0s 846us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 477/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 478/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 479/1000\n",
            "86/86 [==============================] - 0s 881us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 480/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 481/1000\n",
            "86/86 [==============================] - 0s 875us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 482/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 483/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 484/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 485/1000\n",
            "86/86 [==============================] - 0s 753us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 486/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 487/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 488/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 489/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 490/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 491/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 492/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 493/1000\n",
            "86/86 [==============================] - 0s 886us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 494/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 495/1000\n",
            "86/86 [==============================] - 0s 760us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 496/1000\n",
            "86/86 [==============================] - 0s 955us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 497/1000\n",
            "86/86 [==============================] - 0s 850us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 498/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 499/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 500/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 501/1000\n",
            "86/86 [==============================] - 0s 912us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 502/1000\n",
            "86/86 [==============================] - 0s 751us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 503/1000\n",
            "86/86 [==============================] - 0s 944us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 504/1000\n",
            "86/86 [==============================] - 0s 867us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 505/1000\n",
            "86/86 [==============================] - 0s 760us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 506/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 507/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 508/1000\n",
            "86/86 [==============================] - 0s 878us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 509/1000\n",
            "86/86 [==============================] - 0s 854us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 510/1000\n",
            "86/86 [==============================] - 0s 811us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 511/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 512/1000\n",
            "86/86 [==============================] - 0s 754us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 513/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 514/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 515/1000\n",
            "86/86 [==============================] - 0s 744us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 516/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 517/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 518/1000\n",
            "86/86 [==============================] - 0s 872us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 519/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 520/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 521/1000\n",
            "86/86 [==============================] - 0s 745us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 522/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 523/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 524/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 525/1000\n",
            "86/86 [==============================] - 0s 916us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 526/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 527/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 528/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 529/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 530/1000\n",
            "86/86 [==============================] - 0s 858us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 531/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 532/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 533/1000\n",
            "86/86 [==============================] - 0s 853us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 534/1000\n",
            "86/86 [==============================] - 0s 918us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 535/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 536/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 537/1000\n",
            "86/86 [==============================] - 0s 883us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 538/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 539/1000\n",
            "86/86 [==============================] - 0s 883us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 540/1000\n",
            "86/86 [==============================] - 0s 874us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 541/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 542/1000\n",
            "86/86 [==============================] - 0s 749us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 543/1000\n",
            "86/86 [==============================] - 0s 930us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 544/1000\n",
            "86/86 [==============================] - 0s 750us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 545/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 546/1000\n",
            "86/86 [==============================] - 0s 885us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 547/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 548/1000\n",
            "86/86 [==============================] - 0s 767us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 549/1000\n",
            "86/86 [==============================] - 0s 945us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 550/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 551/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 552/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 553/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 554/1000\n",
            "86/86 [==============================] - 0s 733us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 555/1000\n",
            "86/86 [==============================] - 0s 743us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 556/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 557/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 558/1000\n",
            "86/86 [==============================] - 0s 879us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 559/1000\n",
            "86/86 [==============================] - 0s 900us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 560/1000\n",
            "86/86 [==============================] - 0s 853us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 561/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 562/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 563/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 564/1000\n",
            "86/86 [==============================] - 0s 750us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 565/1000\n",
            "86/86 [==============================] - 0s 724us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 566/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 567/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 568/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 569/1000\n",
            "86/86 [==============================] - 0s 735us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 570/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 571/1000\n",
            "86/86 [==============================] - 0s 804us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 572/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 573/1000\n",
            "86/86 [==============================] - 0s 736us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 574/1000\n",
            "86/86 [==============================] - 0s 900us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 575/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 576/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 577/1000\n",
            "86/86 [==============================] - 0s 936us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 578/1000\n",
            "86/86 [==============================] - 0s 756us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 579/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 580/1000\n",
            "86/86 [==============================] - 0s 852us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 581/1000\n",
            "86/86 [==============================] - 0s 944us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 582/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 583/1000\n",
            "86/86 [==============================] - 0s 844us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 584/1000\n",
            "86/86 [==============================] - 0s 838us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 585/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 586/1000\n",
            "86/86 [==============================] - 0s 877us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 587/1000\n",
            "86/86 [==============================] - 0s 894us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 588/1000\n",
            "86/86 [==============================] - 0s 760us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 589/1000\n",
            "86/86 [==============================] - 0s 839us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 590/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 591/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 592/1000\n",
            "86/86 [==============================] - 0s 854us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 593/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 594/1000\n",
            "86/86 [==============================] - 0s 887us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 595/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 596/1000\n",
            "86/86 [==============================] - 0s 740us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 597/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 598/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 599/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 600/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 601/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 602/1000\n",
            "86/86 [==============================] - 0s 972us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 603/1000\n",
            "86/86 [==============================] - 0s 743us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 604/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 605/1000\n",
            "86/86 [==============================] - 0s 837us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 606/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 607/1000\n",
            "86/86 [==============================] - 0s 947us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 608/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 609/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 610/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 611/1000\n",
            "86/86 [==============================] - 0s 859us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 612/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 613/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 614/1000\n",
            "86/86 [==============================] - 0s 838us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 615/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 616/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 617/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 618/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 619/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 620/1000\n",
            "86/86 [==============================] - 0s 859us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 621/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 622/1000\n",
            "86/86 [==============================] - 0s 952us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 623/1000\n",
            "86/86 [==============================] - 0s 920us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 624/1000\n",
            "86/86 [==============================] - 0s 830us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 625/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 626/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 627/1000\n",
            "86/86 [==============================] - 0s 831us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 628/1000\n",
            "86/86 [==============================] - 0s 837us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 629/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 630/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 631/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 632/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 633/1000\n",
            "86/86 [==============================] - 0s 858us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 634/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 635/1000\n",
            "86/86 [==============================] - 0s 841us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 636/1000\n",
            "86/86 [==============================] - 0s 832us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 637/1000\n",
            "86/86 [==============================] - 0s 753us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 638/1000\n",
            "86/86 [==============================] - 0s 750us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 639/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 640/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 641/1000\n",
            "86/86 [==============================] - 0s 948us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 642/1000\n",
            "86/86 [==============================] - 0s 804us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 643/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 644/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 645/1000\n",
            "86/86 [==============================] - 0s 756us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 646/1000\n",
            "86/86 [==============================] - 0s 851us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 647/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 648/1000\n",
            "86/86 [==============================] - 0s 933us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 649/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 650/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 651/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 652/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 653/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 654/1000\n",
            "86/86 [==============================] - 0s 866us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 655/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 656/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 657/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 658/1000\n",
            "86/86 [==============================] - 0s 846us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 659/1000\n",
            "86/86 [==============================] - 0s 850us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 660/1000\n",
            "86/86 [==============================] - 0s 828us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 661/1000\n",
            "86/86 [==============================] - 0s 928us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 662/1000\n",
            "86/86 [==============================] - 0s 930us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 663/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 664/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 665/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 666/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 667/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 668/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 669/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 670/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 671/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 672/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 673/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 674/1000\n",
            "86/86 [==============================] - 0s 942us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 675/1000\n",
            "86/86 [==============================] - 0s 890us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 676/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 677/1000\n",
            "86/86 [==============================] - 0s 898us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 678/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 679/1000\n",
            "86/86 [==============================] - 0s 912us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 680/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 681/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 682/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 683/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 684/1000\n",
            "86/86 [==============================] - 0s 752us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 685/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 686/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 687/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 688/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 689/1000\n",
            "86/86 [==============================] - 0s 738us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 690/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 691/1000\n",
            "86/86 [==============================] - 0s 840us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 692/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 693/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 694/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 695/1000\n",
            "86/86 [==============================] - 0s 930us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 696/1000\n",
            "86/86 [==============================] - 0s 899us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 697/1000\n",
            "86/86 [==============================] - 0s 876us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 698/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 699/1000\n",
            "86/86 [==============================] - 0s 828us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 700/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 701/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 702/1000\n",
            "86/86 [==============================] - 0s 912us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 703/1000\n",
            "86/86 [==============================] - 0s 828us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 704/1000\n",
            "86/86 [==============================] - 0s 828us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 705/1000\n",
            "86/86 [==============================] - 0s 736us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 706/1000\n",
            "86/86 [==============================] - 0s 758us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 707/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 708/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 709/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 710/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 711/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 712/1000\n",
            "86/86 [==============================] - 0s 846us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 713/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 714/1000\n",
            "86/86 [==============================] - 0s 908us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 715/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 716/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 717/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 718/1000\n",
            "86/86 [==============================] - 0s 727us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 719/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 720/1000\n",
            "86/86 [==============================] - 0s 941us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 721/1000\n",
            "86/86 [==============================] - 0s 913us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 722/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 723/1000\n",
            "86/86 [==============================] - 0s 767us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 724/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 725/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 726/1000\n",
            "86/86 [==============================] - 0s 837us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 727/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 728/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 729/1000\n",
            "86/86 [==============================] - 0s 750us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 730/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 731/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 732/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 733/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 734/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 735/1000\n",
            "86/86 [==============================] - 0s 902us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 736/1000\n",
            "86/86 [==============================] - 0s 916us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 737/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 738/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 739/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 740/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 741/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 742/1000\n",
            "86/86 [==============================] - 0s 838us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 743/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 744/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 745/1000\n",
            "86/86 [==============================] - 0s 915us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 746/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 747/1000\n",
            "86/86 [==============================] - 0s 956us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 748/1000\n",
            "86/86 [==============================] - 0s 856us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 749/1000\n",
            "86/86 [==============================] - 0s 974us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 750/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 751/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 752/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 753/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 754/1000\n",
            "86/86 [==============================] - 0s 887us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 755/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 756/1000\n",
            "86/86 [==============================] - 0s 950us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 757/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 758/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 759/1000\n",
            "86/86 [==============================] - 0s 740us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 760/1000\n",
            "86/86 [==============================] - 0s 909us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 761/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 762/1000\n",
            "86/86 [==============================] - 0s 847us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 763/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 764/1000\n",
            "86/86 [==============================] - 0s 741us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 765/1000\n",
            "86/86 [==============================] - 0s 732us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 766/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 767/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 768/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 769/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 770/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 771/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 772/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 773/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 774/1000\n",
            "86/86 [==============================] - 0s 863us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 775/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 776/1000\n",
            "86/86 [==============================] - 0s 857us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 777/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 778/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 779/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 780/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 781/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 782/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 783/1000\n",
            "86/86 [==============================] - 0s 758us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 784/1000\n",
            "86/86 [==============================] - 0s 929us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 785/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 786/1000\n",
            "86/86 [==============================] - 0s 811us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 787/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 788/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 789/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 790/1000\n",
            "86/86 [==============================] - 0s 922us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 791/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 792/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 793/1000\n",
            "86/86 [==============================] - 0s 922us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 794/1000\n",
            "86/86 [==============================] - 0s 752us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 795/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 796/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 797/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 798/1000\n",
            "86/86 [==============================] - 0s 843us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 799/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 800/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 801/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 802/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 803/1000\n",
            "86/86 [==============================] - 0s 903us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 804/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 805/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 806/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 807/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 808/1000\n",
            "86/86 [==============================] - 0s 838us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 809/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 810/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 811/1000\n",
            "86/86 [==============================] - 0s 860us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 812/1000\n",
            "86/86 [==============================] - 0s 767us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 813/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 814/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 815/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 816/1000\n",
            "86/86 [==============================] - 0s 945us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 817/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 818/1000\n",
            "86/86 [==============================] - 0s 756us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 819/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 820/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 821/1000\n",
            "86/86 [==============================] - 0s 937us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 822/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 823/1000\n",
            "86/86 [==============================] - 0s 855us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 824/1000\n",
            "86/86 [==============================] - 0s 973us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 825/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 826/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 827/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 828/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 829/1000\n",
            "86/86 [==============================] - 0s 902us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 830/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 831/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 832/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 833/1000\n",
            "86/86 [==============================] - 0s 930us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 834/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 835/1000\n",
            "86/86 [==============================] - 0s 895us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 836/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 837/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 838/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 839/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 840/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 841/1000\n",
            "86/86 [==============================] - 0s 870us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 842/1000\n",
            "86/86 [==============================] - 0s 911us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 843/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 844/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 845/1000\n",
            "86/86 [==============================] - 0s 811us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 846/1000\n",
            "86/86 [==============================] - 0s 967us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 847/1000\n",
            "86/86 [==============================] - 0s 751us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 848/1000\n",
            "86/86 [==============================] - 0s 861us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 849/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 850/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 851/1000\n",
            "86/86 [==============================] - 0s 900us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 852/1000\n",
            "86/86 [==============================] - 0s 749us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 853/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 854/1000\n",
            "86/86 [==============================] - 0s 753us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 855/1000\n",
            "86/86 [==============================] - 0s 861us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 856/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 857/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 858/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 859/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 860/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 861/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 862/1000\n",
            "86/86 [==============================] - 0s 934us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 863/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 864/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 865/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 866/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 867/1000\n",
            "86/86 [==============================] - 0s 834us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 868/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 869/1000\n",
            "86/86 [==============================] - 0s 944us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 870/1000\n",
            "86/86 [==============================] - 0s 898us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 871/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 872/1000\n",
            "86/86 [==============================] - 0s 852us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 873/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 874/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 875/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 876/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 877/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 878/1000\n",
            "86/86 [==============================] - 0s 853us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 879/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 880/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 881/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 882/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 883/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 884/1000\n",
            "86/86 [==============================] - 0s 755us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 885/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 886/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 887/1000\n",
            "86/86 [==============================] - 0s 742us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 888/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 889/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 890/1000\n",
            "86/86 [==============================] - 0s 893us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 891/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 892/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 893/1000\n",
            "86/86 [==============================] - 0s 743us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 894/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 895/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 896/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 897/1000\n",
            "86/86 [==============================] - 0s 957us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 898/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 899/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 900/1000\n",
            "86/86 [==============================] - 0s 838us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 901/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 902/1000\n",
            "86/86 [==============================] - 0s 922us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 903/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 904/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 905/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 906/1000\n",
            "86/86 [==============================] - 0s 900us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 907/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 908/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 909/1000\n",
            "86/86 [==============================] - 0s 916us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 910/1000\n",
            "86/86 [==============================] - 0s 724us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 911/1000\n",
            "86/86 [==============================] - 0s 860us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 912/1000\n",
            "86/86 [==============================] - 0s 754us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 913/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 914/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 915/1000\n",
            "86/86 [==============================] - 0s 839us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 916/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 917/1000\n",
            "86/86 [==============================] - 0s 804us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 918/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 919/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 920/1000\n",
            "86/86 [==============================] - 0s 934us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 921/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 922/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 923/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 924/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 925/1000\n",
            "86/86 [==============================] - 0s 869us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 926/1000\n",
            "86/86 [==============================] - 0s 889us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 927/1000\n",
            "86/86 [==============================] - 0s 804us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 928/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 929/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 930/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 931/1000\n",
            "86/86 [==============================] - 0s 838us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 932/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 933/1000\n",
            "86/86 [==============================] - 0s 748us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 934/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 935/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 936/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 937/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 938/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 939/1000\n",
            "86/86 [==============================] - 0s 843us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 940/1000\n",
            "86/86 [==============================] - 0s 881us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 941/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 942/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 943/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 944/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 945/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 946/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 947/1000\n",
            "86/86 [==============================] - 0s 828us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 948/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 949/1000\n",
            "86/86 [==============================] - 0s 909us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 950/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 951/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 952/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 953/1000\n",
            "86/86 [==============================] - 0s 989us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 954/1000\n",
            "86/86 [==============================] - 0s 753us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 955/1000\n",
            "86/86 [==============================] - 0s 748us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 956/1000\n",
            "86/86 [==============================] - 0s 828us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 957/1000\n",
            "86/86 [==============================] - 0s 746us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 958/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 959/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 960/1000\n",
            "86/86 [==============================] - 0s 749us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 961/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 962/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 963/1000\n",
            "86/86 [==============================] - 0s 880us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 964/1000\n",
            "86/86 [==============================] - 0s 804us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 965/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 966/1000\n",
            "86/86 [==============================] - 0s 838us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 967/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 968/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 969/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 970/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 971/1000\n",
            "86/86 [==============================] - 0s 751us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 972/1000\n",
            "86/86 [==============================] - 0s 854us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 973/1000\n",
            "86/86 [==============================] - 0s 937us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 974/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 975/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 976/1000\n",
            "86/86 [==============================] - 0s 998us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 977/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 978/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 979/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 980/1000\n",
            "86/86 [==============================] - 0s 883us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 981/1000\n",
            "86/86 [==============================] - 0s 843us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 982/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 983/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 984/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 985/1000\n",
            "86/86 [==============================] - 0s 856us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 986/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 987/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 988/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 989/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 990/1000\n",
            "86/86 [==============================] - 0s 965us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 991/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 992/1000\n",
            "86/86 [==============================] - 0s 995us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 993/1000\n",
            "86/86 [==============================] - 0s 840us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 994/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 995/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 996/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 997/1000\n",
            "86/86 [==============================] - 0s 850us/step - loss: 0.0586 - val_loss: 0.0545\n",
            "Epoch 998/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 999/1000\n",
            "86/86 [==============================] - 0s 876us/step - loss: 0.0587 - val_loss: 0.0545\n",
            "Epoch 1000/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0586 - val_loss: 0.0545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ401Yvq04Pw",
        "outputId": "4e3be137-9666-43fc-f411-e916db001f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "loss = model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print('Test loss:', loss)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29/29 - 0s - loss: 0.0545\n",
            "Test loss: 0.054481346160173416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_7P_qhP1VYr"
      },
      "source": [
        "def plot_loss_new(history):\n",
        "  history_df = pd.DataFrame(history.history)\n",
        "\n",
        "  min_loss_index = history_df[history_df['loss']==min(history_df['loss'])].index.values\n",
        "  min_loss = history_df.loc[min_loss_index]['loss']\n",
        "  min_val_loss_index = history_df[history_df['val_loss']==min(history_df['val_loss'])].index.values\n",
        "  min_val_loss = history_df.loc[min_val_loss_index]['val_loss']\n",
        "\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.plot(min_loss_index, min_loss, 'o', c='k', ms=4, label='min loss')\n",
        "  plt.plot(min_val_loss_index, min_val_loss, 'o', c='k', ms=4, label='min val_loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "  print('Minimum Loss             :', min_loss)\n",
        "  print()\n",
        "  print('Minimum Validation Loss  :', min_val_loss)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY6V-4Vp1iF4",
        "outputId": "5d5bcb21-8f9e-4f29-cc2c-820d77ba1520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "history_dataframe = pd.DataFrame(history.history)\n",
        "history_dataframe['epoch'] = history.epoch\n",
        "history_dataframe.sort_values(by='val_loss', ascending=True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>epoch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>0.058687</td>\n",
              "      <td>0.054413</td>\n",
              "      <td>161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>0.058676</td>\n",
              "      <td>0.054413</td>\n",
              "      <td>127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>0.058688</td>\n",
              "      <td>0.054414</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>0.058684</td>\n",
              "      <td>0.054414</td>\n",
              "      <td>153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.058672</td>\n",
              "      <td>0.054414</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.058823</td>\n",
              "      <td>0.054740</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.059173</td>\n",
              "      <td>0.054857</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.060975</td>\n",
              "      <td>0.055561</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.069148</td>\n",
              "      <td>0.058913</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.101275</td>\n",
              "      <td>0.074112</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         loss  val_loss  epoch\n",
              "161  0.058687  0.054413    161\n",
              "127  0.058676  0.054413    127\n",
              "120  0.058688  0.054414    120\n",
              "153  0.058684  0.054414    153\n",
              "97   0.058672  0.054414     97\n",
              "..        ...       ...    ...\n",
              "4    0.058823  0.054740      4\n",
              "3    0.059173  0.054857      3\n",
              "2    0.060975  0.055561      2\n",
              "1    0.069148  0.058913      1\n",
              "0    0.101275  0.074112      0\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzJj_8PV1ukj",
        "outputId": "1d031163-21e5-4890-f559-beb7c34a418f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "plot_loss_new(history)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Minimum Loss             : 593    0.058622\n",
            "Name: loss, dtype: float64\n",
            "\n",
            "Minimum Validation Loss  : 161    0.054413\n",
            "Name: val_loss, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7apEHVz712DB"
      },
      "source": [
        "## **Deeper Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co9xeoUb1xFa",
        "outputId": "0be3abb1-f049-4b1f-c489-98d34303308d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "neurons = 1\n",
        "epoch = 1000\n",
        "batch_size = 32\n",
        "\n",
        "deeper = Sequential()\n",
        "deeper.add(Dense(neurons, activation='relu', input_dim=X_train.shape[1]))\n",
        "deeper.add(Dense(5, activation='relu'))\n",
        "deeper.add(Dense(1))\n",
        "deeper.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "history_deep = deeper.fit(X_train, y_train, epochs=epoch, batch_size=batch_size, validation_data=(X_test, y_test))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0672 - val_loss: 0.0585\n",
            "Epoch 2/1000\n",
            "86/86 [==============================] - 0s 921us/step - loss: 0.0618 - val_loss: 0.0559\n",
            "Epoch 3/1000\n",
            "86/86 [==============================] - 0s 830us/step - loss: 0.0599 - val_loss: 0.0548\n",
            "Epoch 4/1000\n",
            "86/86 [==============================] - 0s 925us/step - loss: 0.0590 - val_loss: 0.0542\n",
            "Epoch 5/1000\n",
            "86/86 [==============================] - 0s 874us/step - loss: 0.0587 - val_loss: 0.0541\n",
            "Epoch 6/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0585 - val_loss: 0.0542\n",
            "Epoch 7/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 8/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 9/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 10/1000\n",
            "86/86 [==============================] - 0s 840us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 11/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 12/1000\n",
            "86/86 [==============================] - 0s 742us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 13/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 14/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 15/1000\n",
            "86/86 [==============================] - 0s 903us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 16/1000\n",
            "86/86 [==============================] - 0s 952us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 17/1000\n",
            "86/86 [==============================] - 0s 894us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 18/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 19/1000\n",
            "86/86 [==============================] - 0s 839us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 20/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0584 - val_loss: 0.0542\n",
            "Epoch 21/1000\n",
            "86/86 [==============================] - 0s 840us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 22/1000\n",
            "86/86 [==============================] - 0s 867us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 23/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 24/1000\n",
            "86/86 [==============================] - 0s 828us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 25/1000\n",
            "86/86 [==============================] - 0s 850us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 26/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 27/1000\n",
            "86/86 [==============================] - 0s 889us/step - loss: 0.0584 - val_loss: 0.0542\n",
            "Epoch 28/1000\n",
            "86/86 [==============================] - 0s 854us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 29/1000\n",
            "86/86 [==============================] - 0s 939us/step - loss: 0.0584 - val_loss: 0.0542\n",
            "Epoch 30/1000\n",
            "86/86 [==============================] - 0s 935us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 31/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 32/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0586 - val_loss: 0.0540\n",
            "Epoch 33/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 34/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 35/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 36/1000\n",
            "86/86 [==============================] - 0s 897us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 37/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 38/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 39/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0583 - val_loss: 0.0544\n",
            "Epoch 40/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0585 - val_loss: 0.0542\n",
            "Epoch 41/1000\n",
            "86/86 [==============================] - 0s 913us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 42/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 43/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 44/1000\n",
            "86/86 [==============================] - 0s 858us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 45/1000\n",
            "86/86 [==============================] - 0s 891us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 46/1000\n",
            "86/86 [==============================] - 0s 749us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 47/1000\n",
            "86/86 [==============================] - 0s 862us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 48/1000\n",
            "86/86 [==============================] - 0s 830us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 49/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 50/1000\n",
            "86/86 [==============================] - 0s 839us/step - loss: 0.0585 - val_loss: 0.0541\n",
            "Epoch 51/1000\n",
            "86/86 [==============================] - 0s 811us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 52/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 53/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 54/1000\n",
            "86/86 [==============================] - 0s 909us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 55/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 56/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 57/1000\n",
            "86/86 [==============================] - 0s 912us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 58/1000\n",
            "86/86 [==============================] - 0s 848us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 59/1000\n",
            "86/86 [==============================] - 0s 840us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 60/1000\n",
            "86/86 [==============================] - 0s 856us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 61/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 62/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 63/1000\n",
            "86/86 [==============================] - 0s 949us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 64/1000\n",
            "86/86 [==============================] - 0s 900us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 65/1000\n",
            "86/86 [==============================] - 0s 920us/step - loss: 0.0585 - val_loss: 0.0541\n",
            "Epoch 66/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 67/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 68/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 69/1000\n",
            "86/86 [==============================] - 0s 848us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 70/1000\n",
            "86/86 [==============================] - 0s 968us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 71/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 72/1000\n",
            "86/86 [==============================] - 0s 920us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 73/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 74/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 75/1000\n",
            "86/86 [==============================] - 0s 937us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 76/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 77/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 78/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 79/1000\n",
            "86/86 [==============================] - 0s 903us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 80/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 81/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0585 - val_loss: 0.0539\n",
            "Epoch 82/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0586 - val_loss: 0.0541\n",
            "Epoch 83/1000\n",
            "86/86 [==============================] - 0s 962us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 84/1000\n",
            "86/86 [==============================] - 0s 994us/step - loss: 0.0584 - val_loss: 0.0542\n",
            "Epoch 85/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 86/1000\n",
            "86/86 [==============================] - 0s 919us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 87/1000\n",
            "86/86 [==============================] - 0s 867us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 88/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 89/1000\n",
            "86/86 [==============================] - 0s 862us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 90/1000\n",
            "86/86 [==============================] - 0s 916us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 91/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 92/1000\n",
            "86/86 [==============================] - 0s 861us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 93/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 94/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0584 - val_loss: 0.0543\n",
            "Epoch 95/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 96/1000\n",
            "86/86 [==============================] - 0s 898us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 97/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 98/1000\n",
            "86/86 [==============================] - 0s 877us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 99/1000\n",
            "86/86 [==============================] - 0s 937us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 100/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 101/1000\n",
            "86/86 [==============================] - 0s 879us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 102/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 103/1000\n",
            "86/86 [==============================] - 0s 830us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 104/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 105/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 106/1000\n",
            "86/86 [==============================] - 0s 851us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 107/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0584 - val_loss: 0.0543\n",
            "Epoch 108/1000\n",
            "86/86 [==============================] - 0s 898us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 109/1000\n",
            "86/86 [==============================] - 0s 839us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 110/1000\n",
            "86/86 [==============================] - 0s 839us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 111/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 112/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 113/1000\n",
            "86/86 [==============================] - 0s 866us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 114/1000\n",
            "86/86 [==============================] - 0s 922us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 115/1000\n",
            "86/86 [==============================] - 0s 856us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 116/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 117/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 118/1000\n",
            "86/86 [==============================] - 0s 843us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 119/1000\n",
            "86/86 [==============================] - 0s 905us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 120/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 121/1000\n",
            "86/86 [==============================] - 0s 900us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 122/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 123/1000\n",
            "86/86 [==============================] - 0s 914us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 124/1000\n",
            "86/86 [==============================] - 0s 864us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 125/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 126/1000\n",
            "86/86 [==============================] - 0s 844us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 127/1000\n",
            "86/86 [==============================] - 0s 932us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 128/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 129/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 130/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 131/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 132/1000\n",
            "86/86 [==============================] - 0s 767us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 133/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0583 - val_loss: 0.0543\n",
            "Epoch 134/1000\n",
            "86/86 [==============================] - 0s 848us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 135/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 136/1000\n",
            "86/86 [==============================] - 0s 874us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 137/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 138/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 139/1000\n",
            "86/86 [==============================] - 0s 912us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 140/1000\n",
            "86/86 [==============================] - 0s 993us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 141/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 142/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 143/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 144/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 145/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 146/1000\n",
            "86/86 [==============================] - 0s 767us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 147/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 148/1000\n",
            "86/86 [==============================] - 0s 914us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 149/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 150/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 151/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 152/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 153/1000\n",
            "86/86 [==============================] - 0s 842us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 154/1000\n",
            "86/86 [==============================] - 0s 831us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 155/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 156/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 157/1000\n",
            "86/86 [==============================] - 0s 959us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 158/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 159/1000\n",
            "86/86 [==============================] - 0s 901us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 160/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 161/1000\n",
            "86/86 [==============================] - 0s 843us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 162/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 163/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 164/1000\n",
            "86/86 [==============================] - 0s 830us/step - loss: 0.0584 - val_loss: 0.0543\n",
            "Epoch 165/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0585 - val_loss: 0.0541\n",
            "Epoch 166/1000\n",
            "86/86 [==============================] - 0s 832us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 167/1000\n",
            "86/86 [==============================] - 0s 925us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 168/1000\n",
            "86/86 [==============================] - 0s 746us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 169/1000\n",
            "86/86 [==============================] - 0s 748us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 170/1000\n",
            "86/86 [==============================] - 0s 828us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 171/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 172/1000\n",
            "86/86 [==============================] - 0s 943us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 173/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 174/1000\n",
            "86/86 [==============================] - 0s 948us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 175/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 176/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 177/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 178/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0583 - val_loss: 0.0545\n",
            "Epoch 179/1000\n",
            "86/86 [==============================] - 0s 943us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 180/1000\n",
            "86/86 [==============================] - 0s 828us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 181/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 182/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 183/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 184/1000\n",
            "86/86 [==============================] - 0s 861us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 185/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 186/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 187/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 188/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 189/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 190/1000\n",
            "86/86 [==============================] - 0s 917us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 191/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 192/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 193/1000\n",
            "86/86 [==============================] - 0s 848us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 194/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 195/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 196/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 197/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 198/1000\n",
            "86/86 [==============================] - 0s 857us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 199/1000\n",
            "86/86 [==============================] - 0s 837us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 200/1000\n",
            "86/86 [==============================] - 0s 916us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 201/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 202/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 203/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 204/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 205/1000\n",
            "86/86 [==============================] - 0s 737us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 206/1000\n",
            "86/86 [==============================] - 0s 906us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 207/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 208/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 209/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 210/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 211/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 212/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 213/1000\n",
            "86/86 [==============================] - 0s 909us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 214/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 215/1000\n",
            "86/86 [==============================] - 0s 976us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 216/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 217/1000\n",
            "86/86 [==============================] - 0s 901us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 218/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 219/1000\n",
            "86/86 [==============================] - 0s 925us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 220/1000\n",
            "86/86 [==============================] - 0s 841us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 221/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 222/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 223/1000\n",
            "86/86 [==============================] - 0s 945us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 224/1000\n",
            "86/86 [==============================] - 0s 901us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 225/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 226/1000\n",
            "86/86 [==============================] - 0s 957us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 227/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 228/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 229/1000\n",
            "86/86 [==============================] - 0s 848us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 230/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 231/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 232/1000\n",
            "86/86 [==============================] - 0s 828us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 233/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 234/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 235/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 236/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 237/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 238/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 239/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 240/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 241/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 242/1000\n",
            "86/86 [==============================] - 0s 842us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 243/1000\n",
            "86/86 [==============================] - 0s 962us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 244/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 245/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 246/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 247/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 248/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 249/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0583 - val_loss: 0.0544\n",
            "Epoch 250/1000\n",
            "86/86 [==============================] - 0s 911us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 251/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 252/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 253/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 254/1000\n",
            "86/86 [==============================] - 0s 916us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 255/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 256/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 257/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 258/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 259/1000\n",
            "86/86 [==============================] - 0s 886us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 260/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 261/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 262/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 263/1000\n",
            "86/86 [==============================] - 0s 932us/step - loss: 0.0585 - val_loss: 0.0543\n",
            "Epoch 264/1000\n",
            "86/86 [==============================] - 0s 864us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 265/1000\n",
            "86/86 [==============================] - 0s 939us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 266/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 267/1000\n",
            "86/86 [==============================] - 0s 869us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 268/1000\n",
            "86/86 [==============================] - 0s 902us/step - loss: 0.0584 - val_loss: 0.0543\n",
            "Epoch 269/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 270/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 271/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 272/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 273/1000\n",
            "86/86 [==============================] - 0s 744us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 274/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 275/1000\n",
            "86/86 [==============================] - 0s 962us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 276/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 277/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 278/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 279/1000\n",
            "86/86 [==============================] - 0s 884us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 280/1000\n",
            "86/86 [==============================] - 0s 873us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 281/1000\n",
            "86/86 [==============================] - 0s 927us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 282/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 283/1000\n",
            "86/86 [==============================] - 0s 754us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 284/1000\n",
            "86/86 [==============================] - 0s 941us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 285/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 286/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 287/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 288/1000\n",
            "86/86 [==============================] - 0s 840us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 289/1000\n",
            "86/86 [==============================] - 0s 830us/step - loss: 0.0585 - val_loss: 0.0541\n",
            "Epoch 290/1000\n",
            "86/86 [==============================] - 0s 832us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 291/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 292/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 293/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 294/1000\n",
            "86/86 [==============================] - 0s 935us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 295/1000\n",
            "86/86 [==============================] - 0s 936us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 296/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 297/1000\n",
            "86/86 [==============================] - 0s 952us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 298/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 299/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 300/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 301/1000\n",
            "86/86 [==============================] - 0s 865us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 302/1000\n",
            "86/86 [==============================] - 0s 939us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 303/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 304/1000\n",
            "86/86 [==============================] - 0s 944us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 305/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 306/1000\n",
            "86/86 [==============================] - 0s 831us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 307/1000\n",
            "86/86 [==============================] - 0s 994us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 308/1000\n",
            "86/86 [==============================] - 0s 963us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 309/1000\n",
            "86/86 [==============================] - 0s 844us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 310/1000\n",
            "86/86 [==============================] - 0s 864us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 311/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 312/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 313/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 314/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 315/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 316/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 317/1000\n",
            "86/86 [==============================] - 0s 872us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 318/1000\n",
            "86/86 [==============================] - 0s 938us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 319/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0584 - val_loss: 0.0542\n",
            "Epoch 320/1000\n",
            "86/86 [==============================] - 0s 838us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 321/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 322/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 323/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 324/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 325/1000\n",
            "86/86 [==============================] - 0s 731us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 326/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0585 - val_loss: 0.0542\n",
            "Epoch 327/1000\n",
            "86/86 [==============================] - 0s 922us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 328/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 329/1000\n",
            "86/86 [==============================] - 0s 832us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 330/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 331/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 332/1000\n",
            "86/86 [==============================] - 0s 863us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 333/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 334/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 335/1000\n",
            "86/86 [==============================] - 0s 844us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 336/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 337/1000\n",
            "86/86 [==============================] - 0s 828us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 338/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 339/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0586 - val_loss: 0.0541\n",
            "Epoch 340/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0585 - val_loss: 0.0541\n",
            "Epoch 341/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 342/1000\n",
            "86/86 [==============================] - 0s 935us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 343/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 344/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 345/1000\n",
            "86/86 [==============================] - 0s 946us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 346/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 347/1000\n",
            "86/86 [==============================] - 0s 883us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 348/1000\n",
            "86/86 [==============================] - 0s 871us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 349/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 350/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 351/1000\n",
            "86/86 [==============================] - 0s 750us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 352/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 353/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 354/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 355/1000\n",
            "86/86 [==============================] - 0s 916us/step - loss: 0.0582 - val_loss: 0.0544\n",
            "Epoch 356/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 357/1000\n",
            "86/86 [==============================] - 0s 844us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 358/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 359/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 360/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 361/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 362/1000\n",
            "86/86 [==============================] - 0s 760us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 363/1000\n",
            "86/86 [==============================] - 0s 830us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 364/1000\n",
            "86/86 [==============================] - 0s 852us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 365/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 366/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 367/1000\n",
            "86/86 [==============================] - 0s 950us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 368/1000\n",
            "86/86 [==============================] - 0s 839us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 369/1000\n",
            "86/86 [==============================] - 0s 905us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 370/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 371/1000\n",
            "86/86 [==============================] - 0s 970us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 372/1000\n",
            "86/86 [==============================] - 0s 845us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 373/1000\n",
            "86/86 [==============================] - 0s 929us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 374/1000\n",
            "86/86 [==============================] - 0s 756us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 375/1000\n",
            "86/86 [==============================] - 0s 832us/step - loss: 0.0584 - val_loss: 0.0542\n",
            "Epoch 376/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 377/1000\n",
            "86/86 [==============================] - 0s 893us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 378/1000\n",
            "86/86 [==============================] - 0s 872us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 379/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 380/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 381/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 382/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 383/1000\n",
            "86/86 [==============================] - 0s 901us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 384/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 385/1000\n",
            "86/86 [==============================] - 0s 974us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 386/1000\n",
            "86/86 [==============================] - 0s 903us/step - loss: 0.0584 - val_loss: 0.0542\n",
            "Epoch 387/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 388/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 389/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 390/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 391/1000\n",
            "86/86 [==============================] - 0s 889us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 392/1000\n",
            "86/86 [==============================] - 0s 898us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 393/1000\n",
            "86/86 [==============================] - 0s 837us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 394/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 395/1000\n",
            "86/86 [==============================] - 0s 940us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 396/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 397/1000\n",
            "86/86 [==============================] - 0s 754us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 398/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 399/1000\n",
            "86/86 [==============================] - 0s 887us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 400/1000\n",
            "86/86 [==============================] - 0s 849us/step - loss: 0.0584 - val_loss: 0.0543\n",
            "Epoch 401/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 402/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 403/1000\n",
            "86/86 [==============================] - 0s 828us/step - loss: 0.0584 - val_loss: 0.0542\n",
            "Epoch 404/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 405/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 406/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 407/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 408/1000\n",
            "86/86 [==============================] - 0s 947us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 409/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 410/1000\n",
            "86/86 [==============================] - 0s 890us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 411/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 412/1000\n",
            "86/86 [==============================] - 0s 880us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 413/1000\n",
            "86/86 [==============================] - 0s 980us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 414/1000\n",
            "86/86 [==============================] - 0s 844us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 415/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 416/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 417/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 418/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 419/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 420/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 421/1000\n",
            "86/86 [==============================] - 0s 990us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 422/1000\n",
            "86/86 [==============================] - 0s 840us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 423/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 424/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 425/1000\n",
            "86/86 [==============================] - 0s 884us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 426/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 427/1000\n",
            "86/86 [==============================] - 0s 831us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 428/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 429/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 430/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 431/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 432/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 433/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 434/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 435/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 436/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 437/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 438/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 439/1000\n",
            "86/86 [==============================] - 0s 949us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 440/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 441/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 442/1000\n",
            "86/86 [==============================] - 0s 831us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 443/1000\n",
            "86/86 [==============================] - 0s 908us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 444/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 445/1000\n",
            "86/86 [==============================] - 0s 831us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 446/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 447/1000\n",
            "86/86 [==============================] - 0s 870us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 448/1000\n",
            "86/86 [==============================] - 0s 909us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 449/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 450/1000\n",
            "86/86 [==============================] - 0s 831us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 451/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 452/1000\n",
            "86/86 [==============================] - 0s 993us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 453/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 454/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 455/1000\n",
            "86/86 [==============================] - 0s 863us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 456/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 457/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 458/1000\n",
            "86/86 [==============================] - 0s 958us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 459/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 460/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 461/1000\n",
            "86/86 [==============================] - 0s 859us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 462/1000\n",
            "86/86 [==============================] - 0s 892us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 463/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0584 - val_loss: 0.0542\n",
            "Epoch 464/1000\n",
            "86/86 [==============================] - 0s 832us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 465/1000\n",
            "86/86 [==============================] - 0s 841us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 466/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 467/1000\n",
            "86/86 [==============================] - 0s 862us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 468/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 469/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 470/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 471/1000\n",
            "86/86 [==============================] - 0s 877us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 472/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 473/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 474/1000\n",
            "86/86 [==============================] - 0s 746us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 475/1000\n",
            "86/86 [==============================] - 0s 902us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 476/1000\n",
            "86/86 [==============================] - 0s 845us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 477/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 478/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 479/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 480/1000\n",
            "86/86 [==============================] - 0s 845us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 481/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 482/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 483/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 484/1000\n",
            "86/86 [==============================] - 0s 754us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 485/1000\n",
            "86/86 [==============================] - 0s 886us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 486/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 487/1000\n",
            "86/86 [==============================] - 0s 958us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 488/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 489/1000\n",
            "86/86 [==============================] - 0s 855us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 490/1000\n",
            "86/86 [==============================] - 0s 811us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 491/1000\n",
            "86/86 [==============================] - 0s 884us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 492/1000\n",
            "86/86 [==============================] - 0s 888us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 493/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 494/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 495/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 496/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 497/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 498/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 499/1000\n",
            "86/86 [==============================] - 0s 842us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 500/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 501/1000\n",
            "86/86 [==============================] - 0s 988us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 502/1000\n",
            "86/86 [==============================] - 0s 869us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 503/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 504/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 505/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 506/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 507/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 508/1000\n",
            "86/86 [==============================] - 0s 846us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 509/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 510/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 511/1000\n",
            "86/86 [==============================] - 0s 933us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 512/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 513/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 514/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 515/1000\n",
            "86/86 [==============================] - 0s 950us/step - loss: 0.0585 - val_loss: 0.0541\n",
            "Epoch 516/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 517/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 518/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 519/1000\n",
            "86/86 [==============================] - 0s 929us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 520/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 521/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 522/1000\n",
            "86/86 [==============================] - 0s 838us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 523/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 524/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 525/1000\n",
            "86/86 [==============================] - 0s 839us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 526/1000\n",
            "86/86 [==============================] - 0s 898us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 527/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 528/1000\n",
            "86/86 [==============================] - 0s 839us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 529/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 530/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 531/1000\n",
            "86/86 [==============================] - 0s 899us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 532/1000\n",
            "86/86 [==============================] - 0s 929us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 533/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 534/1000\n",
            "86/86 [==============================] - 0s 844us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 535/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 536/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 537/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 538/1000\n",
            "86/86 [==============================] - 0s 937us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 539/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 540/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 541/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 542/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 543/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 544/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 545/1000\n",
            "86/86 [==============================] - 0s 877us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 546/1000\n",
            "86/86 [==============================] - 0s 746us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 547/1000\n",
            "86/86 [==============================] - 0s 914us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 548/1000\n",
            "86/86 [==============================] - 0s 877us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 549/1000\n",
            "86/86 [==============================] - 0s 758us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 550/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 551/1000\n",
            "86/86 [==============================] - 0s 919us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 552/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 553/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 554/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 555/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 556/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0585 - val_loss: 0.0541\n",
            "Epoch 557/1000\n",
            "86/86 [==============================] - 0s 853us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 558/1000\n",
            "86/86 [==============================] - 0s 898us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 559/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 560/1000\n",
            "86/86 [==============================] - 0s 834us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 561/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 562/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 563/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 564/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 565/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 566/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0584 - val_loss: 0.0542\n",
            "Epoch 567/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 568/1000\n",
            "86/86 [==============================] - 0s 925us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 569/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 570/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 571/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 572/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 573/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 574/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 575/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 576/1000\n",
            "86/86 [==============================] - 0s 834us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 577/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 578/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 579/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 580/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 581/1000\n",
            "86/86 [==============================] - 0s 902us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 582/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 583/1000\n",
            "86/86 [==============================] - 0s 932us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 584/1000\n",
            "86/86 [==============================] - 0s 754us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 585/1000\n",
            "86/86 [==============================] - 0s 841us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 586/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0583 - val_loss: 0.0544\n",
            "Epoch 587/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 588/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 589/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 590/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 591/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 592/1000\n",
            "86/86 [==============================] - 0s 856us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 593/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 594/1000\n",
            "86/86 [==============================] - 0s 886us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 595/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 596/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 597/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 598/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 599/1000\n",
            "86/86 [==============================] - 0s 933us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 600/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 601/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 602/1000\n",
            "86/86 [==============================] - 0s 895us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 603/1000\n",
            "86/86 [==============================] - 0s 848us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 604/1000\n",
            "86/86 [==============================] - 0s 894us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 605/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 606/1000\n",
            "86/86 [==============================] - 0s 760us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 607/1000\n",
            "86/86 [==============================] - 0s 882us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 608/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 609/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 610/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 611/1000\n",
            "86/86 [==============================] - 0s 892us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 612/1000\n",
            "86/86 [==============================] - 0s 947us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 613/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 614/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 615/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 616/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 617/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 618/1000\n",
            "86/86 [==============================] - 0s 872us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 619/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 620/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 621/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0586 - val_loss: 0.0540\n",
            "Epoch 622/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 623/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 624/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 625/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0585 - val_loss: 0.0541\n",
            "Epoch 626/1000\n",
            "86/86 [==============================] - 0s 992us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 627/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 628/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0585 - val_loss: 0.0543\n",
            "Epoch 629/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 630/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 631/1000\n",
            "86/86 [==============================] - 0s 978us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 632/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 633/1000\n",
            "86/86 [==============================] - 0s 943us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 634/1000\n",
            "86/86 [==============================] - 0s 843us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 635/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 636/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 637/1000\n",
            "86/86 [==============================] - 0s 751us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 638/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 639/1000\n",
            "86/86 [==============================] - 0s 937us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 640/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 641/1000\n",
            "86/86 [==============================] - 0s 860us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 642/1000\n",
            "86/86 [==============================] - 0s 910us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 643/1000\n",
            "86/86 [==============================] - 0s 884us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 644/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 645/1000\n",
            "86/86 [==============================] - 0s 942us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 646/1000\n",
            "86/86 [==============================] - 0s 910us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 647/1000\n",
            "86/86 [==============================] - 0s 846us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 648/1000\n",
            "86/86 [==============================] - 0s 755us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 649/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 650/1000\n",
            "86/86 [==============================] - 0s 882us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 651/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 652/1000\n",
            "86/86 [==============================] - 0s 918us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 653/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 654/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 655/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 656/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 657/1000\n",
            "86/86 [==============================] - 0s 937us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 658/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 659/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 660/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 661/1000\n",
            "86/86 [==============================] - 0s 734us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 662/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 663/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 664/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 665/1000\n",
            "86/86 [==============================] - 0s 891us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 666/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 667/1000\n",
            "86/86 [==============================] - 0s 872us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 668/1000\n",
            "86/86 [==============================] - 0s 756us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 669/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0586 - val_loss: 0.0540\n",
            "Epoch 670/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 671/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 672/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 673/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 674/1000\n",
            "86/86 [==============================] - 0s 982us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 675/1000\n",
            "86/86 [==============================] - 0s 830us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 676/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 677/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 678/1000\n",
            "86/86 [==============================] - 0s 886us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 679/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 680/1000\n",
            "86/86 [==============================] - 0s 961us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 681/1000\n",
            "86/86 [==============================] - 0s 842us/step - loss: 0.0585 - val_loss: 0.0542\n",
            "Epoch 682/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 683/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 684/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 685/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 686/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 687/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 688/1000\n",
            "86/86 [==============================] - 0s 748us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 689/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 690/1000\n",
            "86/86 [==============================] - 0s 859us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 691/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 692/1000\n",
            "86/86 [==============================] - 0s 945us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 693/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 694/1000\n",
            "86/86 [==============================] - 0s 881us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 695/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 696/1000\n",
            "86/86 [==============================] - 0s 900us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 697/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 698/1000\n",
            "86/86 [==============================] - 0s 863us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 699/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 700/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 701/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 702/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 703/1000\n",
            "86/86 [==============================] - 0s 948us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 704/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 705/1000\n",
            "86/86 [==============================] - 0s 916us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 706/1000\n",
            "86/86 [==============================] - 0s 832us/step - loss: 0.0585 - val_loss: 0.0541\n",
            "Epoch 707/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 708/1000\n",
            "86/86 [==============================] - 0s 752us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 709/1000\n",
            "86/86 [==============================] - 0s 982us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 710/1000\n",
            "86/86 [==============================] - 0s 922us/step - loss: 0.0585 - val_loss: 0.0541\n",
            "Epoch 711/1000\n",
            "86/86 [==============================] - 0s 844us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 712/1000\n",
            "86/86 [==============================] - 0s 839us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 713/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 714/1000\n",
            "86/86 [==============================] - 0s 750us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 715/1000\n",
            "86/86 [==============================] - 0s 832us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 716/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 717/1000\n",
            "86/86 [==============================] - 0s 756us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 718/1000\n",
            "86/86 [==============================] - 0s 871us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 719/1000\n",
            "86/86 [==============================] - 0s 898us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 720/1000\n",
            "86/86 [==============================] - 0s 930us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 721/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 722/1000\n",
            "86/86 [==============================] - 0s 845us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 723/1000\n",
            "86/86 [==============================] - 0s 847us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 724/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 725/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 726/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 727/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 728/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 729/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 730/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 731/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 732/1000\n",
            "86/86 [==============================] - 0s 830us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 733/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 734/1000\n",
            "86/86 [==============================] - 0s 957us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 735/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 736/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 737/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 738/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 739/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 740/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 741/1000\n",
            "86/86 [==============================] - 0s 942us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 742/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 743/1000\n",
            "86/86 [==============================] - 0s 743us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 744/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 745/1000\n",
            "86/86 [==============================] - 0s 979us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 746/1000\n",
            "86/86 [==============================] - 0s 919us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 747/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 748/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 749/1000\n",
            "86/86 [==============================] - 0s 866us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 750/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 751/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 752/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 753/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 754/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 755/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 756/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 757/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 758/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 759/1000\n",
            "86/86 [==============================] - 0s 965us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 760/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 761/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 762/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 763/1000\n",
            "86/86 [==============================] - 0s 840us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 764/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 765/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 766/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 767/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 768/1000\n",
            "86/86 [==============================] - 0s 758us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 769/1000\n",
            "86/86 [==============================] - 0s 834us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 770/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 771/1000\n",
            "86/86 [==============================] - 0s 832us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 772/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 773/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 774/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 775/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 776/1000\n",
            "86/86 [==============================] - 0s 895us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 777/1000\n",
            "86/86 [==============================] - 0s 958us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 778/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 779/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 780/1000\n",
            "86/86 [==============================] - 0s 862us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 781/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 782/1000\n",
            "86/86 [==============================] - 0s 862us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 783/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 784/1000\n",
            "86/86 [==============================] - 0s 741us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 785/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 786/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 787/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 788/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 789/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 790/1000\n",
            "86/86 [==============================] - 0s 936us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 791/1000\n",
            "86/86 [==============================] - 0s 834us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 792/1000\n",
            "86/86 [==============================] - 0s 904us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 793/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 794/1000\n",
            "86/86 [==============================] - 0s 884us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 795/1000\n",
            "86/86 [==============================] - 0s 847us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 796/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 797/1000\n",
            "86/86 [==============================] - 0s 752us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 798/1000\n",
            "86/86 [==============================] - 0s 934us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 799/1000\n",
            "86/86 [==============================] - 0s 838us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 800/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 801/1000\n",
            "86/86 [==============================] - 0s 750us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 802/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 803/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0584 - val_loss: 0.0543\n",
            "Epoch 804/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 805/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 806/1000\n",
            "86/86 [==============================] - 0s 996us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 807/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 808/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 809/1000\n",
            "86/86 [==============================] - 0s 921us/step - loss: 0.0582 - val_loss: 0.0545\n",
            "Epoch 810/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 811/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 812/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 813/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 814/1000\n",
            "86/86 [==============================] - 0s 758us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 815/1000\n",
            "86/86 [==============================] - 0s 870us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 816/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 817/1000\n",
            "86/86 [==============================] - 0s 864us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 818/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 819/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 820/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 821/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 822/1000\n",
            "86/86 [==============================] - 0s 927us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 823/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 824/1000\n",
            "86/86 [==============================] - 0s 877us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 825/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 826/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 827/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 828/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 829/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 830/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 831/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 832/1000\n",
            "86/86 [==============================] - 0s 755us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 833/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 834/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 835/1000\n",
            "86/86 [==============================] - 0s 875us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 836/1000\n",
            "86/86 [==============================] - 0s 886us/step - loss: 0.0586 - val_loss: 0.0540\n",
            "Epoch 837/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 838/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 839/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 840/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 841/1000\n",
            "86/86 [==============================] - 0s 754us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 842/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 843/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 844/1000\n",
            "86/86 [==============================] - 0s 892us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 845/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 846/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 847/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 848/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 849/1000\n",
            "86/86 [==============================] - 0s 890us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 850/1000\n",
            "86/86 [==============================] - 0s 751us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 851/1000\n",
            "86/86 [==============================] - 0s 921us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 852/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 853/1000\n",
            "86/86 [==============================] - 0s 954us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 854/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 855/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 856/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 857/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 858/1000\n",
            "86/86 [==============================] - 0s 831us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 859/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 860/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 861/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 862/1000\n",
            "86/86 [==============================] - 0s 828us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 863/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 864/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 865/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 866/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 867/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 868/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 869/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 870/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 871/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 872/1000\n",
            "86/86 [==============================] - 0s 873us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 873/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0585 - val_loss: 0.0542\n",
            "Epoch 874/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 875/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 876/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 877/1000\n",
            "86/86 [==============================] - 0s 747us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 878/1000\n",
            "86/86 [==============================] - 0s 755us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 879/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 880/1000\n",
            "86/86 [==============================] - 0s 830us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 881/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 882/1000\n",
            "86/86 [==============================] - 0s 848us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 883/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 884/1000\n",
            "86/86 [==============================] - 0s 832us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 885/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 886/1000\n",
            "86/86 [==============================] - 0s 944us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 887/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 888/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 889/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 890/1000\n",
            "86/86 [==============================] - 0s 845us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 891/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 892/1000\n",
            "86/86 [==============================] - 0s 843us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 893/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 894/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 895/1000\n",
            "86/86 [==============================] - 0s 883us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 896/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 897/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 898/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 899/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 900/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 901/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 902/1000\n",
            "86/86 [==============================] - 0s 741us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 903/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 904/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 905/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 906/1000\n",
            "86/86 [==============================] - 0s 811us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 907/1000\n",
            "86/86 [==============================] - 0s 838us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 908/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 909/1000\n",
            "86/86 [==============================] - 0s 840us/step - loss: 0.0583 - val_loss: 0.0543\n",
            "Epoch 910/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 911/1000\n",
            "86/86 [==============================] - 0s 755us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 912/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 913/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 914/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 915/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 916/1000\n",
            "86/86 [==============================] - 0s 902us/step - loss: 0.0586 - val_loss: 0.0540\n",
            "Epoch 917/1000\n",
            "86/86 [==============================] - 0s 1000us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 918/1000\n",
            "86/86 [==============================] - 0s 997us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 919/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 920/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 921/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 922/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 923/1000\n",
            "86/86 [==============================] - 0s 854us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 924/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 925/1000\n",
            "86/86 [==============================] - 0s 834us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 926/1000\n",
            "86/86 [==============================] - 0s 876us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 927/1000\n",
            "86/86 [==============================] - 0s 989us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 928/1000\n",
            "86/86 [==============================] - 0s 900us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 929/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 930/1000\n",
            "86/86 [==============================] - 0s 831us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 931/1000\n",
            "86/86 [==============================] - 0s 875us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 932/1000\n",
            "86/86 [==============================] - 0s 948us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 933/1000\n",
            "86/86 [==============================] - 0s 940us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 934/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 935/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 936/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 937/1000\n",
            "86/86 [==============================] - 0s 961us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 938/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0584 - val_loss: 0.0541\n",
            "Epoch 939/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 940/1000\n",
            "86/86 [==============================] - 0s 842us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 941/1000\n",
            "86/86 [==============================] - 0s 753us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 942/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 943/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 944/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 945/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 946/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 947/1000\n",
            "86/86 [==============================] - 0s 903us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 948/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 949/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 950/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 951/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 952/1000\n",
            "86/86 [==============================] - 0s 925us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 953/1000\n",
            "86/86 [==============================] - 0s 901us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 954/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 955/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 956/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 957/1000\n",
            "86/86 [==============================] - 0s 882us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 958/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 959/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 960/1000\n",
            "86/86 [==============================] - 0s 923us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 961/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 962/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 963/1000\n",
            "86/86 [==============================] - 0s 843us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 964/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 965/1000\n",
            "86/86 [==============================] - 0s 743us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 966/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 967/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 968/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 969/1000\n",
            "86/86 [==============================] - 0s 869us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 970/1000\n",
            "86/86 [==============================] - 0s 845us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 971/1000\n",
            "86/86 [==============================] - 0s 752us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 972/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 973/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 974/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 975/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 976/1000\n",
            "86/86 [==============================] - 0s 927us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 977/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 978/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 979/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 980/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 981/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 982/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 983/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 984/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 985/1000\n",
            "86/86 [==============================] - 0s 850us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 986/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 987/1000\n",
            "86/86 [==============================] - 0s 975us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 988/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 989/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 990/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 991/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 992/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 993/1000\n",
            "86/86 [==============================] - 0s 842us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 994/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 995/1000\n",
            "86/86 [==============================] - 0s 767us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 996/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 997/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 998/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 999/1000\n",
            "86/86 [==============================] - 0s 811us/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 1000/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0584 - val_loss: 0.0541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcMsD4Ri2ZHa",
        "outputId": "2a84f782-d187-4169-d071-eee45928ab55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "history_deep_df = pd.DataFrame(history_deep.history)\n",
        "history_deep_df['epoch'] = history_deep.epoch\n",
        "history_deep_df.sort_values(by='val_loss', ascending=True)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>epoch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>0.058426</td>\n",
              "      <td>0.053939</td>\n",
              "      <td>109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>0.058435</td>\n",
              "      <td>0.053939</td>\n",
              "      <td>113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.058396</td>\n",
              "      <td>0.053940</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>0.058445</td>\n",
              "      <td>0.053941</td>\n",
              "      <td>140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>0.058376</td>\n",
              "      <td>0.053941</td>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>808</th>\n",
              "      <td>0.058209</td>\n",
              "      <td>0.054475</td>\n",
              "      <td>808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>0.058295</td>\n",
              "      <td>0.054500</td>\n",
              "      <td>177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.059884</td>\n",
              "      <td>0.054755</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.061773</td>\n",
              "      <td>0.055899</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.067168</td>\n",
              "      <td>0.058476</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         loss  val_loss  epoch\n",
              "109  0.058426  0.053939    109\n",
              "113  0.058435  0.053939    113\n",
              "101  0.058396  0.053940    101\n",
              "140  0.058445  0.053941    140\n",
              "130  0.058376  0.053941    130\n",
              "..        ...       ...    ...\n",
              "808  0.058209  0.054475    808\n",
              "177  0.058295  0.054500    177\n",
              "2    0.059884  0.054755      2\n",
              "1    0.061773  0.055899      1\n",
              "0    0.067168  0.058476      0\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eyfogu3O3CUY",
        "outputId": "6a25c0cb-fbc7-4087-daa1-15d5e54bf6b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "plot_loss_new(history_deep)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Minimum Loss             : 354    0.0582\n",
            "Name: loss, dtype: float64\n",
            "\n",
            "Minimum Validation Loss  : 109    0.053939\n",
            "Name: val_loss, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4E6h---3JPi"
      },
      "source": [
        "## **Wider Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLRUHtqM3F4E",
        "outputId": "be1cdd07-983c-41cd-c34e-59b362204a06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "neurons = 50\n",
        "epoch = 1000\n",
        "batch_size = 32\n",
        "\n",
        "wider = Sequential()\n",
        "wider.add(Dense(neurons, activation='relu', input_dim=X_train.shape[1]))\n",
        "wider.add(Dense(1))\n",
        "wider.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "history_wide = wider.fit(X_train, y_train, epochs=epoch, batch_size=batch_size, validation_data=(X_test, y_test))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0623 - val_loss: 0.0542\n",
            "Epoch 2/1000\n",
            "86/86 [==============================] - 0s 885us/step - loss: 0.0588 - val_loss: 0.0540\n",
            "Epoch 3/1000\n",
            "86/86 [==============================] - 0s 985us/step - loss: 0.0587 - val_loss: 0.0540\n",
            "Epoch 4/1000\n",
            "86/86 [==============================] - 0s 962us/step - loss: 0.0587 - val_loss: 0.0539\n",
            "Epoch 5/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0549\n",
            "Epoch 6/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 7/1000\n",
            "86/86 [==============================] - 0s 965us/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 8/1000\n",
            "86/86 [==============================] - 0s 968us/step - loss: 0.0582 - val_loss: 0.0538\n",
            "Epoch 9/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 10/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0538\n",
            "Epoch 11/1000\n",
            "86/86 [==============================] - 0s 919us/step - loss: 0.0583 - val_loss: 0.0547\n",
            "Epoch 12/1000\n",
            "86/86 [==============================] - 0s 993us/step - loss: 0.0583 - val_loss: 0.0538\n",
            "Epoch 13/1000\n",
            "86/86 [==============================] - 0s 968us/step - loss: 0.0581 - val_loss: 0.0541\n",
            "Epoch 14/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 15/1000\n",
            "86/86 [==============================] - 0s 969us/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 16/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 17/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0538\n",
            "Epoch 18/1000\n",
            "86/86 [==============================] - 0s 972us/step - loss: 0.0579 - val_loss: 0.0543\n",
            "Epoch 19/1000\n",
            "86/86 [==============================] - 0s 956us/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 20/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 21/1000\n",
            "86/86 [==============================] - 0s 953us/step - loss: 0.0582 - val_loss: 0.0538\n",
            "Epoch 22/1000\n",
            "86/86 [==============================] - 0s 883us/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 23/1000\n",
            "86/86 [==============================] - 0s 844us/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 24/1000\n",
            "86/86 [==============================] - 0s 860us/step - loss: 0.0581 - val_loss: 0.0538\n",
            "Epoch 25/1000\n",
            "86/86 [==============================] - 0s 965us/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 26/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0538\n",
            "Epoch 27/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 28/1000\n",
            "86/86 [==============================] - 0s 968us/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 29/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 30/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 31/1000\n",
            "86/86 [==============================] - 0s 993us/step - loss: 0.0581 - val_loss: 0.0538\n",
            "Epoch 32/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 33/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0537\n",
            "Epoch 34/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0546\n",
            "Epoch 35/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 36/1000\n",
            "86/86 [==============================] - 0s 917us/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 37/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0543\n",
            "Epoch 38/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0538\n",
            "Epoch 39/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 40/1000\n",
            "86/86 [==============================] - 0s 979us/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 41/1000\n",
            "86/86 [==============================] - 0s 957us/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 42/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 43/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 44/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0544\n",
            "Epoch 45/1000\n",
            "86/86 [==============================] - 0s 978us/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 46/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 47/1000\n",
            "86/86 [==============================] - 0s 960us/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 48/1000\n",
            "86/86 [==============================] - 0s 968us/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 49/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 50/1000\n",
            "86/86 [==============================] - 0s 994us/step - loss: 0.0581 - val_loss: 0.0538\n",
            "Epoch 51/1000\n",
            "86/86 [==============================] - 0s 937us/step - loss: 0.0579 - val_loss: 0.0541\n",
            "Epoch 52/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0541\n",
            "Epoch 53/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 54/1000\n",
            "86/86 [==============================] - 0s 872us/step - loss: 0.0580 - val_loss: 0.0540\n",
            "Epoch 55/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0538\n",
            "Epoch 56/1000\n",
            "86/86 [==============================] - 0s 977us/step - loss: 0.0581 - val_loss: 0.0538\n",
            "Epoch 57/1000\n",
            "86/86 [==============================] - 0s 849us/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 58/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 59/1000\n",
            "86/86 [==============================] - 0s 975us/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 60/1000\n",
            "86/86 [==============================] - 0s 993us/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 61/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 62/1000\n",
            "86/86 [==============================] - 0s 890us/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 63/1000\n",
            "86/86 [==============================] - 0s 960us/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 64/1000\n",
            "86/86 [==============================] - 0s 963us/step - loss: 0.0582 - val_loss: 0.0538\n",
            "Epoch 65/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 66/1000\n",
            "86/86 [==============================] - 0s 931us/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 67/1000\n",
            "86/86 [==============================] - 0s 937us/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 68/1000\n",
            "86/86 [==============================] - 0s 954us/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 69/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 70/1000\n",
            "86/86 [==============================] - 0s 990us/step - loss: 0.0578 - val_loss: 0.0543\n",
            "Epoch 71/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 72/1000\n",
            "86/86 [==============================] - 0s 970us/step - loss: 0.0579 - val_loss: 0.0540\n",
            "Epoch 73/1000\n",
            "86/86 [==============================] - 0s 944us/step - loss: 0.0580 - val_loss: 0.0540\n",
            "Epoch 74/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0538\n",
            "Epoch 75/1000\n",
            "86/86 [==============================] - 0s 987us/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 76/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 77/1000\n",
            "86/86 [==============================] - 0s 999us/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 78/1000\n",
            "86/86 [==============================] - 0s 999us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 79/1000\n",
            "86/86 [==============================] - 0s 945us/step - loss: 0.0578 - val_loss: 0.0540\n",
            "Epoch 80/1000\n",
            "86/86 [==============================] - 0s 994us/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 81/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0542\n",
            "Epoch 82/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 83/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 84/1000\n",
            "86/86 [==============================] - 0s 990us/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 85/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 86/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 87/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0540\n",
            "Epoch 88/1000\n",
            "86/86 [==============================] - 0s 996us/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 89/1000\n",
            "86/86 [==============================] - 0s 998us/step - loss: 0.0580 - val_loss: 0.0542\n",
            "Epoch 90/1000\n",
            "86/86 [==============================] - 0s 988us/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 91/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 92/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0540\n",
            "Epoch 93/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 94/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 95/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0537\n",
            "Epoch 96/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 97/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 98/1000\n",
            "86/86 [==============================] - 0s 972us/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 99/1000\n",
            "86/86 [==============================] - 0s 958us/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 100/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0540\n",
            "Epoch 101/1000\n",
            "86/86 [==============================] - 0s 943us/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 102/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 103/1000\n",
            "86/86 [==============================] - 0s 973us/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 104/1000\n",
            "86/86 [==============================] - 0s 988us/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 105/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0541\n",
            "Epoch 106/1000\n",
            "86/86 [==============================] - 0s 957us/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 107/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 108/1000\n",
            "86/86 [==============================] - 0s 952us/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 109/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 110/1000\n",
            "86/86 [==============================] - 0s 944us/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 111/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 112/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 113/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 114/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 115/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 116/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 117/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 118/1000\n",
            "86/86 [==============================] - 0s 870us/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 119/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 120/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 121/1000\n",
            "86/86 [==============================] - 0s 983us/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 122/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 123/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0541\n",
            "Epoch 124/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0538\n",
            "Epoch 125/1000\n",
            "86/86 [==============================] - 0s 962us/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 126/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 127/1000\n",
            "86/86 [==============================] - 0s 969us/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 128/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0540\n",
            "Epoch 129/1000\n",
            "86/86 [==============================] - 0s 999us/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 130/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 131/1000\n",
            "86/86 [==============================] - 0s 999us/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 132/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0542\n",
            "Epoch 133/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0540\n",
            "Epoch 134/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 135/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 136/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0540\n",
            "Epoch 137/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 138/1000\n",
            "86/86 [==============================] - 0s 1000us/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 139/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 140/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 141/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0540\n",
            "Epoch 142/1000\n",
            "86/86 [==============================] - 0s 998us/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 143/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 144/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0541\n",
            "Epoch 145/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0543\n",
            "Epoch 146/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 147/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 148/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 149/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 150/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 151/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 152/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 153/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 154/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 155/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0540\n",
            "Epoch 156/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0538\n",
            "Epoch 157/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 158/1000\n",
            "86/86 [==============================] - 0s 989us/step - loss: 0.0579 - val_loss: 0.0540\n",
            "Epoch 159/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 160/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 161/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 162/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 163/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 164/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 165/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 166/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0540\n",
            "Epoch 167/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 168/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0540\n",
            "Epoch 169/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 170/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0542\n",
            "Epoch 171/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 172/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 173/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0540\n",
            "Epoch 174/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 175/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 176/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 177/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 178/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 179/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 180/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 181/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 182/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 183/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 184/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 185/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0540\n",
            "Epoch 186/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 187/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 188/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 189/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 190/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 191/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 192/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 193/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 194/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 195/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 196/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 197/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0544\n",
            "Epoch 198/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 199/1000\n",
            "86/86 [==============================] - 0s 980us/step - loss: 0.0578 - val_loss: 0.0540\n",
            "Epoch 200/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0541\n",
            "Epoch 201/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 202/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 203/1000\n",
            "86/86 [==============================] - 0s 905us/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 204/1000\n",
            "86/86 [==============================] - 0s 951us/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 205/1000\n",
            "86/86 [==============================] - 0s 912us/step - loss: 0.0578 - val_loss: 0.0540\n",
            "Epoch 206/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 207/1000\n",
            "86/86 [==============================] - 0s 839us/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 208/1000\n",
            "86/86 [==============================] - 0s 974us/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 209/1000\n",
            "86/86 [==============================] - 0s 995us/step - loss: 0.0577 - val_loss: 0.0541\n",
            "Epoch 210/1000\n",
            "86/86 [==============================] - 0s 880us/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 211/1000\n",
            "86/86 [==============================] - 0s 940us/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 212/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 213/1000\n",
            "86/86 [==============================] - 0s 912us/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 214/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0578 - val_loss: 0.0540\n",
            "Epoch 215/1000\n",
            "86/86 [==============================] - 0s 879us/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 216/1000\n",
            "86/86 [==============================] - 0s 841us/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 217/1000\n",
            "86/86 [==============================] - 0s 858us/step - loss: 0.0578 - val_loss: 0.0546\n",
            "Epoch 218/1000\n",
            "86/86 [==============================] - 0s 839us/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 219/1000\n",
            "86/86 [==============================] - 0s 842us/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 220/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 221/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0578 - val_loss: 0.0540\n",
            "Epoch 222/1000\n",
            "86/86 [==============================] - 0s 1000us/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 223/1000\n",
            "86/86 [==============================] - 0s 913us/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 224/1000\n",
            "86/86 [==============================] - 0s 870us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 225/1000\n",
            "86/86 [==============================] - 0s 842us/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 226/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 227/1000\n",
            "86/86 [==============================] - 0s 870us/step - loss: 0.0577 - val_loss: 0.0546\n",
            "Epoch 228/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 229/1000\n",
            "86/86 [==============================] - 0s 983us/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 230/1000\n",
            "86/86 [==============================] - 0s 920us/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 231/1000\n",
            "86/86 [==============================] - 0s 973us/step - loss: 0.0578 - val_loss: 0.0540\n",
            "Epoch 232/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 233/1000\n",
            "86/86 [==============================] - 0s 942us/step - loss: 0.0577 - val_loss: 0.0541\n",
            "Epoch 234/1000\n",
            "86/86 [==============================] - 0s 863us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 235/1000\n",
            "86/86 [==============================] - 0s 952us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 236/1000\n",
            "86/86 [==============================] - 0s 984us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 237/1000\n",
            "86/86 [==============================] - 0s 943us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 238/1000\n",
            "86/86 [==============================] - 0s 861us/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 239/1000\n",
            "86/86 [==============================] - 0s 977us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 240/1000\n",
            "86/86 [==============================] - 0s 868us/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 241/1000\n",
            "86/86 [==============================] - 0s 980us/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 242/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 243/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0578 - val_loss: 0.0541\n",
            "Epoch 244/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0578 - val_loss: 0.0540\n",
            "Epoch 245/1000\n",
            "86/86 [==============================] - 0s 960us/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 246/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 247/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 248/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 249/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 250/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 251/1000\n",
            "86/86 [==============================] - 0s 739us/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 252/1000\n",
            "86/86 [==============================] - 0s 962us/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 253/1000\n",
            "86/86 [==============================] - 0s 767us/step - loss: 0.0577 - val_loss: 0.0541\n",
            "Epoch 254/1000\n",
            "86/86 [==============================] - 0s 935us/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 255/1000\n",
            "86/86 [==============================] - 0s 883us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 256/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 257/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 258/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 259/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0577 - val_loss: 0.0541\n",
            "Epoch 260/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 261/1000\n",
            "86/86 [==============================] - 0s 928us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 262/1000\n",
            "86/86 [==============================] - 0s 855us/step - loss: 0.0576 - val_loss: 0.0538\n",
            "Epoch 263/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0577 - val_loss: 0.0541\n",
            "Epoch 264/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 265/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 266/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 267/1000\n",
            "86/86 [==============================] - 0s 888us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 268/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 269/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 270/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0576 - val_loss: 0.0543\n",
            "Epoch 271/1000\n",
            "86/86 [==============================] - 0s 842us/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 272/1000\n",
            "86/86 [==============================] - 0s 869us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 273/1000\n",
            "86/86 [==============================] - 0s 811us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 274/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 275/1000\n",
            "86/86 [==============================] - 0s 859us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 276/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 277/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 278/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 279/1000\n",
            "86/86 [==============================] - 0s 756us/step - loss: 0.0577 - val_loss: 0.0542\n",
            "Epoch 280/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 281/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 282/1000\n",
            "86/86 [==============================] - 0s 752us/step - loss: 0.0576 - val_loss: 0.0543\n",
            "Epoch 283/1000\n",
            "86/86 [==============================] - 0s 899us/step - loss: 0.0578 - val_loss: 0.0540\n",
            "Epoch 284/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 285/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 286/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 287/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0578 - val_loss: 0.0540\n",
            "Epoch 288/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 289/1000\n",
            "86/86 [==============================] - 0s 906us/step - loss: 0.0578 - val_loss: 0.0540\n",
            "Epoch 290/1000\n",
            "86/86 [==============================] - 0s 932us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 291/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 292/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0579 - val_loss: 0.0540\n",
            "Epoch 293/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 294/1000\n",
            "86/86 [==============================] - 0s 930us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 295/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 296/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 297/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 298/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 299/1000\n",
            "86/86 [==============================] - 0s 916us/step - loss: 0.0577 - val_loss: 0.0541\n",
            "Epoch 300/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0577 - val_loss: 0.0541\n",
            "Epoch 301/1000\n",
            "86/86 [==============================] - 0s 830us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 302/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 303/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 304/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 305/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 306/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0576 - val_loss: 0.0543\n",
            "Epoch 307/1000\n",
            "86/86 [==============================] - 0s 939us/step - loss: 0.0576 - val_loss: 0.0538\n",
            "Epoch 308/1000\n",
            "86/86 [==============================] - 0s 867us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 309/1000\n",
            "86/86 [==============================] - 0s 847us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 310/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 311/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 312/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 313/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0577 - val_loss: 0.0541\n",
            "Epoch 314/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 315/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 316/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 317/1000\n",
            "86/86 [==============================] - 0s 738us/step - loss: 0.0577 - val_loss: 0.0541\n",
            "Epoch 318/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0577 - val_loss: 0.0542\n",
            "Epoch 319/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 320/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 321/1000\n",
            "86/86 [==============================] - 0s 851us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 322/1000\n",
            "86/86 [==============================] - 0s 959us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 323/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 324/1000\n",
            "86/86 [==============================] - 0s 731us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 325/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 326/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 327/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 328/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 329/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 330/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0576 - val_loss: 0.0543\n",
            "Epoch 331/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0577 - val_loss: 0.0543\n",
            "Epoch 332/1000\n",
            "86/86 [==============================] - 0s 838us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 333/1000\n",
            "86/86 [==============================] - 0s 721us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 334/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 335/1000\n",
            "86/86 [==============================] - 0s 970us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 336/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 337/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 338/1000\n",
            "86/86 [==============================] - 0s 740us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 339/1000\n",
            "86/86 [==============================] - 0s 914us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 340/1000\n",
            "86/86 [==============================] - 0s 851us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 341/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 342/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 343/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 344/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 345/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0577 - val_loss: 0.0541\n",
            "Epoch 346/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 347/1000\n",
            "86/86 [==============================] - 0s 811us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 348/1000\n",
            "86/86 [==============================] - 0s 828us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 349/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 350/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 351/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 352/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 353/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 354/1000\n",
            "86/86 [==============================] - 0s 936us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 355/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 356/1000\n",
            "86/86 [==============================] - 0s 830us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 357/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 358/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 359/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 360/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 361/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 362/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 363/1000\n",
            "86/86 [==============================] - 0s 760us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 364/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 365/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 366/1000\n",
            "86/86 [==============================] - 0s 744us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 367/1000\n",
            "86/86 [==============================] - 0s 861us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 368/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 369/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 370/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 371/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 372/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 373/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 374/1000\n",
            "86/86 [==============================] - 0s 944us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 375/1000\n",
            "86/86 [==============================] - 0s 747us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 376/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 377/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 378/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 379/1000\n",
            "86/86 [==============================] - 0s 846us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 380/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 381/1000\n",
            "86/86 [==============================] - 0s 929us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 382/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 383/1000\n",
            "86/86 [==============================] - 0s 897us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 384/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0576 - val_loss: 0.0543\n",
            "Epoch 385/1000\n",
            "86/86 [==============================] - 0s 866us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 386/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 387/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0576 - val_loss: 0.0543\n",
            "Epoch 388/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 389/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 390/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0576 - val_loss: 0.0543\n",
            "Epoch 391/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 392/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0577 - val_loss: 0.0543\n",
            "Epoch 393/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 394/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 395/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0577 - val_loss: 0.0542\n",
            "Epoch 396/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 397/1000\n",
            "86/86 [==============================] - 0s 958us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 398/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0576 - val_loss: 0.0543\n",
            "Epoch 399/1000\n",
            "86/86 [==============================] - 0s 757us/step - loss: 0.0576 - val_loss: 0.0543\n",
            "Epoch 400/1000\n",
            "86/86 [==============================] - 0s 880us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 401/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 402/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 403/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 404/1000\n",
            "86/86 [==============================] - 0s 842us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 405/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 406/1000\n",
            "86/86 [==============================] - 0s 838us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 407/1000\n",
            "86/86 [==============================] - 0s 811us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 408/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 409/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 410/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 411/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0576 - val_loss: 0.0543\n",
            "Epoch 412/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 413/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 414/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 415/1000\n",
            "86/86 [==============================] - 0s 868us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 416/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 417/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 418/1000\n",
            "86/86 [==============================] - 0s 880us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 419/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 420/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 421/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 422/1000\n",
            "86/86 [==============================] - 0s 957us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 423/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 424/1000\n",
            "86/86 [==============================] - 0s 747us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 425/1000\n",
            "86/86 [==============================] - 0s 767us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 426/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 427/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 428/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 429/1000\n",
            "86/86 [==============================] - 0s 919us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 430/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 431/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 432/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 433/1000\n",
            "86/86 [==============================] - 0s 755us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 434/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 435/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 436/1000\n",
            "86/86 [==============================] - 0s 892us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 437/1000\n",
            "86/86 [==============================] - 0s 844us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 438/1000\n",
            "86/86 [==============================] - 0s 890us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 439/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 440/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 441/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 442/1000\n",
            "86/86 [==============================] - 0s 955us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 443/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 444/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 445/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 446/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0576 - val_loss: 0.0538\n",
            "Epoch 447/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 448/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 449/1000\n",
            "86/86 [==============================] - 0s 895us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 450/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 451/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 452/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 453/1000\n",
            "86/86 [==============================] - 0s 731us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 454/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 455/1000\n",
            "86/86 [==============================] - 0s 937us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 456/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0575 - val_loss: 0.0544\n",
            "Epoch 457/1000\n",
            "86/86 [==============================] - 0s 755us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 458/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 459/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 460/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 461/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 462/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 463/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 464/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 465/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 466/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 467/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 468/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 469/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 470/1000\n",
            "86/86 [==============================] - 0s 751us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 471/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 472/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 473/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 474/1000\n",
            "86/86 [==============================] - 0s 882us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 475/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 476/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 477/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 478/1000\n",
            "86/86 [==============================] - 0s 855us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 479/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 480/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 481/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 482/1000\n",
            "86/86 [==============================] - 0s 924us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 483/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 484/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 485/1000\n",
            "86/86 [==============================] - 0s 861us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 486/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 487/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 488/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 489/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 490/1000\n",
            "86/86 [==============================] - 0s 907us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 491/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 492/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 493/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 494/1000\n",
            "86/86 [==============================] - 0s 892us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 495/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 496/1000\n",
            "86/86 [==============================] - 0s 939us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 497/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 498/1000\n",
            "86/86 [==============================] - 0s 863us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 499/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 500/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 501/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 502/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 503/1000\n",
            "86/86 [==============================] - 0s 840us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 504/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 505/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 506/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 507/1000\n",
            "86/86 [==============================] - 0s 842us/step - loss: 0.0576 - val_loss: 0.0543\n",
            "Epoch 508/1000\n",
            "86/86 [==============================] - 0s 760us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 509/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 510/1000\n",
            "86/86 [==============================] - 0s 834us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 511/1000\n",
            "86/86 [==============================] - 0s 993us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 512/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0574 - val_loss: 0.0544\n",
            "Epoch 513/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 514/1000\n",
            "86/86 [==============================] - 0s 758us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 515/1000\n",
            "86/86 [==============================] - 0s 740us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 516/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 517/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 518/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 519/1000\n",
            "86/86 [==============================] - 0s 754us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 520/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 521/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 522/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 523/1000\n",
            "86/86 [==============================] - 0s 846us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 524/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 525/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 526/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 527/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 528/1000\n",
            "86/86 [==============================] - 0s 767us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 529/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0574 - val_loss: 0.0539\n",
            "Epoch 530/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 531/1000\n",
            "86/86 [==============================] - 0s 857us/step - loss: 0.0577 - val_loss: 0.0542\n",
            "Epoch 532/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 533/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0575 - val_loss: 0.0545\n",
            "Epoch 534/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 535/1000\n",
            "86/86 [==============================] - 0s 804us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 536/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 537/1000\n",
            "86/86 [==============================] - 0s 914us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 538/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 539/1000\n",
            "86/86 [==============================] - 0s 845us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 540/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 541/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0576 - val_loss: 0.0544\n",
            "Epoch 542/1000\n",
            "86/86 [==============================] - 0s 744us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 543/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 544/1000\n",
            "86/86 [==============================] - 0s 831us/step - loss: 0.0575 - val_loss: 0.0544\n",
            "Epoch 545/1000\n",
            "86/86 [==============================] - 0s 900us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 546/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0576 - val_loss: 0.0544\n",
            "Epoch 547/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 548/1000\n",
            "86/86 [==============================] - 0s 827us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 549/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 550/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 551/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 552/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 553/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 554/1000\n",
            "86/86 [==============================] - 0s 929us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 555/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 556/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 557/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0575 - val_loss: 0.0544\n",
            "Epoch 558/1000\n",
            "86/86 [==============================] - 0s 940us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 559/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0575 - val_loss: 0.0544\n",
            "Epoch 560/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 561/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 562/1000\n",
            "86/86 [==============================] - 0s 737us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 563/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 564/1000\n",
            "86/86 [==============================] - 0s 893us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 565/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 566/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 567/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 568/1000\n",
            "86/86 [==============================] - 0s 886us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 569/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 570/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0574 - val_loss: 0.0544\n",
            "Epoch 571/1000\n",
            "86/86 [==============================] - 0s 878us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 572/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 573/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 574/1000\n",
            "86/86 [==============================] - 0s 754us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 575/1000\n",
            "86/86 [==============================] - 0s 905us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 576/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 577/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 578/1000\n",
            "86/86 [==============================] - 0s 913us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 579/1000\n",
            "86/86 [==============================] - 0s 852us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 580/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 581/1000\n",
            "86/86 [==============================] - 0s 750us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 582/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 583/1000\n",
            "86/86 [==============================] - 0s 884us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 584/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 585/1000\n",
            "86/86 [==============================] - 0s 938us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 586/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 587/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 588/1000\n",
            "86/86 [==============================] - 0s 760us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 589/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 590/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 591/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 592/1000\n",
            "86/86 [==============================] - 0s 934us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 593/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 594/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 595/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 596/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 597/1000\n",
            "86/86 [==============================] - 0s 932us/step - loss: 0.0575 - val_loss: 0.0545\n",
            "Epoch 598/1000\n",
            "86/86 [==============================] - 0s 868us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 599/1000\n",
            "86/86 [==============================] - 0s 815us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 600/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 601/1000\n",
            "86/86 [==============================] - 0s 828us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 602/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0575 - val_loss: 0.0544\n",
            "Epoch 603/1000\n",
            "86/86 [==============================] - 0s 843us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 604/1000\n",
            "86/86 [==============================] - 0s 894us/step - loss: 0.0575 - val_loss: 0.0544\n",
            "Epoch 605/1000\n",
            "86/86 [==============================] - 0s 865us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 606/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 607/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 608/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 609/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 610/1000\n",
            "86/86 [==============================] - 0s 790us/step - loss: 0.0575 - val_loss: 0.0544\n",
            "Epoch 611/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 612/1000\n",
            "86/86 [==============================] - 0s 944us/step - loss: 0.0575 - val_loss: 0.0545\n",
            "Epoch 613/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 614/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 615/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 616/1000\n",
            "86/86 [==============================] - 0s 746us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 617/1000\n",
            "86/86 [==============================] - 0s 999us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 618/1000\n",
            "86/86 [==============================] - 0s 904us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 619/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0575 - val_loss: 0.0538\n",
            "Epoch 620/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 621/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 622/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 623/1000\n",
            "86/86 [==============================] - 0s 877us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 624/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 625/1000\n",
            "86/86 [==============================] - 0s 905us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 626/1000\n",
            "86/86 [==============================] - 0s 755us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 627/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 628/1000\n",
            "86/86 [==============================] - 0s 768us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 629/1000\n",
            "86/86 [==============================] - 0s 729us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 630/1000\n",
            "86/86 [==============================] - 0s 940us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 631/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 632/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 633/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 634/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 635/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 636/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 637/1000\n",
            "86/86 [==============================] - 0s 908us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 638/1000\n",
            "86/86 [==============================] - 0s 760us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 639/1000\n",
            "86/86 [==============================] - 0s 905us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 640/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 641/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 642/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 643/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 644/1000\n",
            "86/86 [==============================] - 0s 913us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 645/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0575 - val_loss: 0.0538\n",
            "Epoch 646/1000\n",
            "86/86 [==============================] - 0s 748us/step - loss: 0.0576 - val_loss: 0.0543\n",
            "Epoch 647/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0574 - val_loss: 0.0538\n",
            "Epoch 648/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 649/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 650/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 651/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 652/1000\n",
            "86/86 [==============================] - 0s 877us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 653/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0576 - val_loss: 0.0543\n",
            "Epoch 654/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 655/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 656/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0574 - val_loss: 0.0539\n",
            "Epoch 657/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 658/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 659/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 660/1000\n",
            "86/86 [==============================] - 0s 935us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 661/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 662/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 663/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 664/1000\n",
            "86/86 [==============================] - 0s 756us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 665/1000\n",
            "86/86 [==============================] - 0s 745us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 666/1000\n",
            "86/86 [==============================] - 0s 902us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 667/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 668/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 669/1000\n",
            "86/86 [==============================] - 0s 906us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 670/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 671/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 672/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 673/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 674/1000\n",
            "86/86 [==============================] - 0s 842us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 675/1000\n",
            "86/86 [==============================] - 0s 841us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 676/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0575 - val_loss: 0.0546\n",
            "Epoch 677/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 678/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 679/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 680/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 681/1000\n",
            "86/86 [==============================] - 0s 865us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 682/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 683/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 684/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 685/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 686/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 687/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 688/1000\n",
            "86/86 [==============================] - 0s 929us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 689/1000\n",
            "86/86 [==============================] - 0s 753us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 690/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 691/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 692/1000\n",
            "86/86 [==============================] - 0s 854us/step - loss: 0.0574 - val_loss: 0.0545\n",
            "Epoch 693/1000\n",
            "86/86 [==============================] - 0s 832us/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 694/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 695/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 696/1000\n",
            "86/86 [==============================] - 0s 760us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 697/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 698/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 699/1000\n",
            "86/86 [==============================] - 0s 760us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 700/1000\n",
            "86/86 [==============================] - 0s 845us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 701/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 702/1000\n",
            "86/86 [==============================] - 0s 741us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 703/1000\n",
            "86/86 [==============================] - 0s 745us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 704/1000\n",
            "86/86 [==============================] - 0s 995us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 705/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 706/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 707/1000\n",
            "86/86 [==============================] - 0s 885us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 708/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 709/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 710/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 711/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 712/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 713/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 714/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 715/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 716/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0574 - val_loss: 0.0539\n",
            "Epoch 717/1000\n",
            "86/86 [==============================] - 0s 756us/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 718/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 719/1000\n",
            "86/86 [==============================] - 0s 953us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 720/1000\n",
            "86/86 [==============================] - 0s 760us/step - loss: 0.0574 - val_loss: 0.0539\n",
            "Epoch 721/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 722/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 723/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 724/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 725/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 726/1000\n",
            "86/86 [==============================] - 0s 758us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 727/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 728/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 729/1000\n",
            "86/86 [==============================] - 0s 735us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 730/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 731/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 732/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 733/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 734/1000\n",
            "86/86 [==============================] - 0s 769us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 735/1000\n",
            "86/86 [==============================] - 0s 942us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 736/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 737/1000\n",
            "86/86 [==============================] - 0s 761us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 738/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 739/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 740/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 741/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 742/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 743/1000\n",
            "86/86 [==============================] - 0s 921us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 744/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 745/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 746/1000\n",
            "86/86 [==============================] - 0s 893us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 747/1000\n",
            "86/86 [==============================] - 0s 805us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 748/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 749/1000\n",
            "86/86 [==============================] - 0s 804us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 750/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 751/1000\n",
            "86/86 [==============================] - 0s 866us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 752/1000\n",
            "86/86 [==============================] - 0s 843us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 753/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 754/1000\n",
            "86/86 [==============================] - 0s 758us/step - loss: 0.0574 - val_loss: 0.0539\n",
            "Epoch 755/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 756/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 757/1000\n",
            "86/86 [==============================] - 0s 853us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 758/1000\n",
            "86/86 [==============================] - 0s 967us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 759/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 760/1000\n",
            "86/86 [==============================] - 0s 850us/step - loss: 0.0574 - val_loss: 0.0545\n",
            "Epoch 761/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0573 - val_loss: 0.0540\n",
            "Epoch 762/1000\n",
            "86/86 [==============================] - 0s 917us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 763/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 764/1000\n",
            "86/86 [==============================] - 0s 781us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 765/1000\n",
            "86/86 [==============================] - 0s 835us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 766/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 767/1000\n",
            "86/86 [==============================] - 0s 764us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 768/1000\n",
            "86/86 [==============================] - 0s 935us/step - loss: 0.0574 - val_loss: 0.0544\n",
            "Epoch 769/1000\n",
            "86/86 [==============================] - 0s 826us/step - loss: 0.0573 - val_loss: 0.0539\n",
            "Epoch 770/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 771/1000\n",
            "86/86 [==============================] - 0s 842us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 772/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 773/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 774/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 775/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 776/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 777/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0575 - val_loss: 0.0544\n",
            "Epoch 778/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 779/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 780/1000\n",
            "86/86 [==============================] - 0s 789us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 781/1000\n",
            "86/86 [==============================] - 0s 836us/step - loss: 0.0574 - val_loss: 0.0539\n",
            "Epoch 782/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 783/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 784/1000\n",
            "86/86 [==============================] - 0s 758us/step - loss: 0.0574 - val_loss: 0.0538\n",
            "Epoch 785/1000\n",
            "86/86 [==============================] - 0s 744us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 786/1000\n",
            "86/86 [==============================] - 0s 773us/step - loss: 0.0573 - val_loss: 0.0540\n",
            "Epoch 787/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 788/1000\n",
            "86/86 [==============================] - 0s 943us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 789/1000\n",
            "86/86 [==============================] - 0s 895us/step - loss: 0.0574 - val_loss: 0.0544\n",
            "Epoch 790/1000\n",
            "86/86 [==============================] - 0s 828us/step - loss: 0.0574 - val_loss: 0.0539\n",
            "Epoch 791/1000\n",
            "86/86 [==============================] - 0s 792us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 792/1000\n",
            "86/86 [==============================] - 0s 787us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 793/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 794/1000\n",
            "86/86 [==============================] - 0s 741us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 795/1000\n",
            "86/86 [==============================] - 0s 839us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 796/1000\n",
            "86/86 [==============================] - 0s 946us/step - loss: 0.0575 - val_loss: 0.0544\n",
            "Epoch 797/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 798/1000\n",
            "86/86 [==============================] - 0s 919us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 799/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 800/1000\n",
            "86/86 [==============================] - 0s 890us/step - loss: 0.0574 - val_loss: 0.0544\n",
            "Epoch 801/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 802/1000\n",
            "86/86 [==============================] - 0s 859us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 803/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 804/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 805/1000\n",
            "86/86 [==============================] - 0s 894us/step - loss: 0.0574 - val_loss: 0.0544\n",
            "Epoch 806/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 807/1000\n",
            "86/86 [==============================] - 0s 824us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 808/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 809/1000\n",
            "86/86 [==============================] - 0s 752us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 810/1000\n",
            "86/86 [==============================] - 0s 964us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 811/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 812/1000\n",
            "86/86 [==============================] - 0s 832us/step - loss: 0.0574 - val_loss: 0.0545\n",
            "Epoch 813/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 814/1000\n",
            "86/86 [==============================] - 0s 793us/step - loss: 0.0574 - val_loss: 0.0544\n",
            "Epoch 815/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 816/1000\n",
            "86/86 [==============================] - 0s 955us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 817/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0574 - val_loss: 0.0539\n",
            "Epoch 818/1000\n",
            "86/86 [==============================] - 0s 851us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 819/1000\n",
            "86/86 [==============================] - 0s 777us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 820/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 821/1000\n",
            "86/86 [==============================] - 0s 931us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 822/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0575 - val_loss: 0.0544\n",
            "Epoch 823/1000\n",
            "86/86 [==============================] - 0s 746us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 824/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0574 - val_loss: 0.0544\n",
            "Epoch 825/1000\n",
            "86/86 [==============================] - 0s 856us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 826/1000\n",
            "86/86 [==============================] - 0s 829us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 827/1000\n",
            "86/86 [==============================] - 0s 814us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 828/1000\n",
            "86/86 [==============================] - 0s 910us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 829/1000\n",
            "86/86 [==============================] - 0s 850us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 830/1000\n",
            "86/86 [==============================] - 0s 869us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 831/1000\n",
            "86/86 [==============================] - 0s 804us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 832/1000\n",
            "86/86 [==============================] - 0s 884us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 833/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 834/1000\n",
            "86/86 [==============================] - 0s 837us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 835/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 836/1000\n",
            "86/86 [==============================] - 0s 880us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 837/1000\n",
            "86/86 [==============================] - 0s 810us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 838/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 839/1000\n",
            "86/86 [==============================] - 0s 893us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 840/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 841/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 842/1000\n",
            "86/86 [==============================] - 0s 888us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 843/1000\n",
            "86/86 [==============================] - 0s 919us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 844/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 845/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 846/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 847/1000\n",
            "86/86 [==============================] - 0s 750us/step - loss: 0.0574 - val_loss: 0.0544\n",
            "Epoch 848/1000\n",
            "86/86 [==============================] - 0s 845us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 849/1000\n",
            "86/86 [==============================] - 0s 909us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 850/1000\n",
            "86/86 [==============================] - 0s 947us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 851/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0574 - val_loss: 0.0539\n",
            "Epoch 852/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 853/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 854/1000\n",
            "86/86 [==============================] - 0s 785us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 855/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 856/1000\n",
            "86/86 [==============================] - 0s 933us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 857/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 858/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0575 - val_loss: 0.0545\n",
            "Epoch 859/1000\n",
            "86/86 [==============================] - 0s 802us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 860/1000\n",
            "86/86 [==============================] - 0s 975us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 861/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 862/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 863/1000\n",
            "86/86 [==============================] - 0s 746us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 864/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 865/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 866/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 867/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 868/1000\n",
            "86/86 [==============================] - 0s 854us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 869/1000\n",
            "86/86 [==============================] - 0s 941us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 870/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0574 - val_loss: 0.0544\n",
            "Epoch 871/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0573 - val_loss: 0.0540\n",
            "Epoch 872/1000\n",
            "86/86 [==============================] - 0s 801us/step - loss: 0.0574 - val_loss: 0.0544\n",
            "Epoch 873/1000\n",
            "86/86 [==============================] - 0s 845us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 874/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 875/1000\n",
            "86/86 [==============================] - 0s 774us/step - loss: 0.0573 - val_loss: 0.0541\n",
            "Epoch 876/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 877/1000\n",
            "86/86 [==============================] - 0s 766us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 878/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 879/1000\n",
            "86/86 [==============================] - 0s 804us/step - loss: 0.0573 - val_loss: 0.0545\n",
            "Epoch 880/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 881/1000\n",
            "86/86 [==============================] - 0s 857us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 882/1000\n",
            "86/86 [==============================] - 0s 755us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 883/1000\n",
            "86/86 [==============================] - 0s 798us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 884/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 885/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 886/1000\n",
            "86/86 [==============================] - 0s 852us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 887/1000\n",
            "86/86 [==============================] - 0s 755us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 888/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 889/1000\n",
            "86/86 [==============================] - 0s 784us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 890/1000\n",
            "86/86 [==============================] - 0s 755us/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 891/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 892/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 893/1000\n",
            "86/86 [==============================] - 0s 756us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 894/1000\n",
            "86/86 [==============================] - 0s 795us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 895/1000\n",
            "86/86 [==============================] - 0s 844us/step - loss: 0.0574 - val_loss: 0.0539\n",
            "Epoch 896/1000\n",
            "86/86 [==============================] - 0s 743us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 897/1000\n",
            "86/86 [==============================] - 0s 925us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 898/1000\n",
            "86/86 [==============================] - 0s 908us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 899/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 900/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 901/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 902/1000\n",
            "86/86 [==============================] - 0s 919us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 903/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 904/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0573 - val_loss: 0.0541\n",
            "Epoch 905/1000\n",
            "86/86 [==============================] - 0s 770us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 906/1000\n",
            "86/86 [==============================] - 0s 794us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 907/1000\n",
            "86/86 [==============================] - 0s 818us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 908/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 909/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 910/1000\n",
            "86/86 [==============================] - 0s 831us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 911/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0575 - val_loss: 0.0544\n",
            "Epoch 912/1000\n",
            "86/86 [==============================] - 0s 841us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 913/1000\n",
            "86/86 [==============================] - 0s 783us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 914/1000\n",
            "86/86 [==============================] - 0s 791us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 915/1000\n",
            "86/86 [==============================] - 0s 979us/step - loss: 0.0573 - val_loss: 0.0541\n",
            "Epoch 916/1000\n",
            "86/86 [==============================] - 0s 854us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 917/1000\n",
            "86/86 [==============================] - 0s 808us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 918/1000\n",
            "86/86 [==============================] - 0s 845us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 919/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0573 - val_loss: 0.0545\n",
            "Epoch 920/1000\n",
            "86/86 [==============================] - 0s 813us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 921/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 922/1000\n",
            "86/86 [==============================] - 0s 807us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 923/1000\n",
            "86/86 [==============================] - 0s 845us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 924/1000\n",
            "86/86 [==============================] - 0s 896us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 925/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 926/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 927/1000\n",
            "86/86 [==============================] - 0s 797us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 928/1000\n",
            "86/86 [==============================] - 0s 906us/step - loss: 0.0573 - val_loss: 0.0542\n",
            "Epoch 929/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 930/1000\n",
            "86/86 [==============================] - 0s 772us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 931/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 932/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0573 - val_loss: 0.0541\n",
            "Epoch 933/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 934/1000\n",
            "86/86 [==============================] - 0s 754us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 935/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 936/1000\n",
            "86/86 [==============================] - 0s 822us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 937/1000\n",
            "86/86 [==============================] - 0s 888us/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 938/1000\n",
            "86/86 [==============================] - 0s 821us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 939/1000\n",
            "86/86 [==============================] - 0s 850us/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 940/1000\n",
            "86/86 [==============================] - 0s 820us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 941/1000\n",
            "86/86 [==============================] - 0s 771us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 942/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 943/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 944/1000\n",
            "86/86 [==============================] - 0s 817us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 945/1000\n",
            "86/86 [==============================] - 0s 861us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 946/1000\n",
            "86/86 [==============================] - 0s 891us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 947/1000\n",
            "86/86 [==============================] - 0s 788us/step - loss: 0.0573 - val_loss: 0.0541\n",
            "Epoch 948/1000\n",
            "86/86 [==============================] - 0s 779us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 949/1000\n",
            "86/86 [==============================] - 0s 754us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 950/1000\n",
            "86/86 [==============================] - 0s 868us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 951/1000\n",
            "86/86 [==============================] - 0s 926us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 952/1000\n",
            "86/86 [==============================] - 0s 925us/step - loss: 0.0573 - val_loss: 0.0543\n",
            "Epoch 953/1000\n",
            "86/86 [==============================] - 0s 831us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 954/1000\n",
            "86/86 [==============================] - 0s 745us/step - loss: 0.0573 - val_loss: 0.0541\n",
            "Epoch 955/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 956/1000\n",
            "86/86 [==============================] - 0s 921us/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 957/1000\n",
            "86/86 [==============================] - 0s 776us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 958/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 959/1000\n",
            "86/86 [==============================] - 0s 778us/step - loss: 0.0573 - val_loss: 0.0542\n",
            "Epoch 960/1000\n",
            "86/86 [==============================] - 0s 804us/step - loss: 0.0573 - val_loss: 0.0539\n",
            "Epoch 961/1000\n",
            "86/86 [==============================] - 0s 782us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 962/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 963/1000\n",
            "86/86 [==============================] - 0s 984us/step - loss: 0.0575 - val_loss: 0.0545\n",
            "Epoch 964/1000\n",
            "86/86 [==============================] - 0s 934us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 965/1000\n",
            "86/86 [==============================] - 0s 786us/step - loss: 0.0574 - val_loss: 0.0544\n",
            "Epoch 966/1000\n",
            "86/86 [==============================] - 0s 745us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 967/1000\n",
            "86/86 [==============================] - 0s 861us/step - loss: 0.0573 - val_loss: 0.0541\n",
            "Epoch 968/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 969/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 970/1000\n",
            "86/86 [==============================] - 0s 752us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 971/1000\n",
            "86/86 [==============================] - 0s 765us/step - loss: 0.0574 - val_loss: 0.0544\n",
            "Epoch 972/1000\n",
            "86/86 [==============================] - 0s 812us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 973/1000\n",
            "86/86 [==============================] - 0s 775us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 974/1000\n",
            "86/86 [==============================] - 0s 832us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 975/1000\n",
            "86/86 [==============================] - 0s 819us/step - loss: 0.0573 - val_loss: 0.0542\n",
            "Epoch 976/1000\n",
            "86/86 [==============================] - 0s 837us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 977/1000\n",
            "86/86 [==============================] - 0s 763us/step - loss: 0.0573 - val_loss: 0.0540\n",
            "Epoch 978/1000\n",
            "86/86 [==============================] - 0s 948us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 979/1000\n",
            "86/86 [==============================] - 0s 762us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 980/1000\n",
            "86/86 [==============================] - 0s 796us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 981/1000\n",
            "86/86 [==============================] - 0s 888us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 982/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 983/1000\n",
            "86/86 [==============================] - 0s 799us/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 984/1000\n",
            "86/86 [==============================] - 0s 806us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 985/1000\n",
            "86/86 [==============================] - 0s 823us/step - loss: 0.0573 - val_loss: 0.0542\n",
            "Epoch 986/1000\n",
            "86/86 [==============================] - 0s 816us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 987/1000\n",
            "86/86 [==============================] - 0s 759us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 988/1000\n",
            "86/86 [==============================] - 0s 803us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 989/1000\n",
            "86/86 [==============================] - 0s 973us/step - loss: 0.0573 - val_loss: 0.0542\n",
            "Epoch 990/1000\n",
            "86/86 [==============================] - 0s 833us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 991/1000\n",
            "86/86 [==============================] - 0s 948us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 992/1000\n",
            "86/86 [==============================] - 0s 903us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 993/1000\n",
            "86/86 [==============================] - 0s 780us/step - loss: 0.0574 - val_loss: 0.0541\n",
            "Epoch 994/1000\n",
            "86/86 [==============================] - 0s 756us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 995/1000\n",
            "86/86 [==============================] - 0s 809us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 996/1000\n",
            "86/86 [==============================] - 0s 800us/step - loss: 0.0574 - val_loss: 0.0540\n",
            "Epoch 997/1000\n",
            "86/86 [==============================] - 0s 750us/step - loss: 0.0573 - val_loss: 0.0540\n",
            "Epoch 998/1000\n",
            "86/86 [==============================] - 0s 825us/step - loss: 0.0574 - val_loss: 0.0543\n",
            "Epoch 999/1000\n",
            "86/86 [==============================] - 0s 848us/step - loss: 0.0574 - val_loss: 0.0542\n",
            "Epoch 1000/1000\n",
            "86/86 [==============================] - 0s 950us/step - loss: 0.0574 - val_loss: 0.0542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvs2Scm13lYM",
        "outputId": "e804153e-ea47-4685-966d-b7e827d12fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "history_wide_df = pd.DataFrame(history_wide.history)\n",
        "history_wide_df['epoch'] = history_wide.epoch\n",
        "history_wide_df.sort_values(by='val_loss', ascending=True)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>epoch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>0.058183</td>\n",
              "      <td>0.053740</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.057958</td>\n",
              "      <td>0.053744</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.057937</td>\n",
              "      <td>0.053753</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.058068</td>\n",
              "      <td>0.053756</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.058155</td>\n",
              "      <td>0.053756</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>0.057670</td>\n",
              "      <td>0.054584</td>\n",
              "      <td>226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>675</th>\n",
              "      <td>0.057470</td>\n",
              "      <td>0.054604</td>\n",
              "      <td>675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>0.057777</td>\n",
              "      <td>0.054609</td>\n",
              "      <td>216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.058307</td>\n",
              "      <td>0.054694</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.058225</td>\n",
              "      <td>0.054918</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         loss  val_loss  epoch\n",
              "94   0.058183  0.053740     94\n",
              "32   0.057958  0.053744     32\n",
              "70   0.057937  0.053753     70\n",
              "55   0.058068  0.053756     55\n",
              "20   0.058155  0.053756     20\n",
              "..        ...       ...    ...\n",
              "226  0.057670  0.054584    226\n",
              "675  0.057470  0.054604    675\n",
              "216  0.057777  0.054609    216\n",
              "10   0.058307  0.054694     10\n",
              "4    0.058225  0.054918      4\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjOKoM6Z4WEz",
        "outputId": "44321af5-1fee-4de4-de67-340f9286aae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "plot_loss_new(history_wide)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Minimum Loss             : 976    0.057291\n",
            "Name: loss, dtype: float64\n",
            "\n",
            "Minimum Validation Loss  : 94    0.05374\n",
            "Name: val_loss, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eqTzwr94bGR"
      },
      "source": [
        "## **LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDT25I-y4YV6"
      },
      "source": [
        "from tensorflow.keras.layers import LSTM"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VboDo4Tt4iMQ",
        "outputId": "1c7f859a-eb71-49ce-ece4-7f0a9a48f1ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epoch = 1000\n",
        "batch_size = 32\n",
        "\n",
        "X_train_reshaped = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_reshaped = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "lstm = Sequential()\n",
        "lstm.add(LSTM(50, activation='relu', input_dim=X_train.shape[1])) #50 LSTM block\n",
        "lstm.add(Dense(1))\n",
        "lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "history_lstm = lstm.fit(X_train_reshaped, y_train, epochs=epoch, batch_size=batch_size, validation_data=(X_test_reshaped, y_test))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 0.0651 - val_loss: 0.0549\n",
            "Epoch 2/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0591 - val_loss: 0.0541\n",
            "Epoch 3/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0540\n",
            "Epoch 4/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0586 - val_loss: 0.0539\n",
            "Epoch 5/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 6/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 7/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0585 - val_loss: 0.0542\n",
            "Epoch 8/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 9/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0585 - val_loss: 0.0539\n",
            "Epoch 10/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 11/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0583 - val_loss: 0.0542\n",
            "Epoch 12/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 13/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 14/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 15/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0540\n",
            "Epoch 16/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0541\n",
            "Epoch 17/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 18/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 19/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 20/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 21/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0540\n",
            "Epoch 22/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0540\n",
            "Epoch 23/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 24/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 25/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 26/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 27/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0582 - val_loss: 0.0543\n",
            "Epoch 28/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 29/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0540\n",
            "Epoch 30/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 31/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0541\n",
            "Epoch 32/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 33/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 34/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0542\n",
            "Epoch 35/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 36/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 37/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0540\n",
            "Epoch 38/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 39/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0540\n",
            "Epoch 40/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 41/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 42/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0540\n",
            "Epoch 43/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 44/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 45/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0582 - val_loss: 0.0540\n",
            "Epoch 46/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 47/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 48/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0540\n",
            "Epoch 49/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0585 - val_loss: 0.0539\n",
            "Epoch 50/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 51/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 52/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 53/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 54/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 55/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0541\n",
            "Epoch 56/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 57/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 58/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 59/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 60/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 61/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0540\n",
            "Epoch 62/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 63/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 64/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 65/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0541\n",
            "Epoch 66/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 67/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 68/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0540\n",
            "Epoch 69/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 70/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 71/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0540\n",
            "Epoch 72/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 73/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 74/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 75/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 76/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 77/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 78/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0545\n",
            "Epoch 79/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0585 - val_loss: 0.0540\n",
            "Epoch 80/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 81/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 82/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 83/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0541\n",
            "Epoch 84/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0540\n",
            "Epoch 85/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 86/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 87/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0542\n",
            "Epoch 88/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 89/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 90/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0542\n",
            "Epoch 91/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 92/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 93/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0582 - val_loss: 0.0540\n",
            "Epoch 94/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0540\n",
            "Epoch 95/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0542\n",
            "Epoch 96/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0542\n",
            "Epoch 97/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 98/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 99/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 100/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0541\n",
            "Epoch 101/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 102/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0541\n",
            "Epoch 103/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 104/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0545\n",
            "Epoch 105/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0541\n",
            "Epoch 106/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 107/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 108/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 109/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 110/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 111/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0581 - val_loss: 0.0542\n",
            "Epoch 112/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0539\n",
            "Epoch 113/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 114/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 115/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 116/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 117/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 118/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 119/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 120/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 121/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0541\n",
            "Epoch 122/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 123/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 124/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 125/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 126/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 127/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 128/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 129/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 130/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 131/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0541\n",
            "Epoch 132/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0542\n",
            "Epoch 133/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0541\n",
            "Epoch 134/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0540\n",
            "Epoch 135/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 136/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 137/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0540\n",
            "Epoch 138/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0541\n",
            "Epoch 139/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0539\n",
            "Epoch 140/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 141/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 142/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 143/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0581 - val_loss: 0.0541\n",
            "Epoch 144/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0546\n",
            "Epoch 145/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0583 - val_loss: 0.0540\n",
            "Epoch 146/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0541\n",
            "Epoch 147/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0541\n",
            "Epoch 148/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0541\n",
            "Epoch 149/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 150/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0541\n",
            "Epoch 151/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 152/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 153/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 154/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 155/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 156/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0541\n",
            "Epoch 157/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 158/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0541\n",
            "Epoch 159/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 160/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 161/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 162/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 163/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 164/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 165/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 166/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 167/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 168/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 169/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0540\n",
            "Epoch 170/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 171/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 172/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 173/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0541\n",
            "Epoch 174/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 175/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0542\n",
            "Epoch 176/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 177/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0542\n",
            "Epoch 178/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 179/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 180/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 181/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 182/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 183/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 184/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 185/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 186/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 187/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0540\n",
            "Epoch 188/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 189/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0540\n",
            "Epoch 190/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 191/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 192/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0541\n",
            "Epoch 193/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0582 - val_loss: 0.0539\n",
            "Epoch 194/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0543\n",
            "Epoch 195/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 196/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 197/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0540\n",
            "Epoch 198/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 199/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0540\n",
            "Epoch 200/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 201/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 202/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 203/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 204/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 205/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 206/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 207/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0581 - val_loss: 0.0538\n",
            "Epoch 208/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 209/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 210/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0538\n",
            "Epoch 211/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 212/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0540\n",
            "Epoch 213/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0541\n",
            "Epoch 214/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0539\n",
            "Epoch 215/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 216/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 217/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 218/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 219/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 220/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 221/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 222/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 223/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0540\n",
            "Epoch 224/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 225/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 226/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 227/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 228/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 229/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 230/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0541\n",
            "Epoch 231/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 232/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 233/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0541\n",
            "Epoch 234/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 235/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 236/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 237/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 238/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 239/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 240/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 241/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 242/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0539\n",
            "Epoch 243/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 244/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 245/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 246/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0542\n",
            "Epoch 247/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 248/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 249/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 250/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 251/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 252/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 253/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 254/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 255/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 256/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 257/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 258/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 259/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 260/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 261/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 262/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 263/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 264/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 265/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 266/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 267/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 268/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 269/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 270/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 271/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 272/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 273/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 274/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 275/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 276/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 277/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 278/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 279/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 280/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0540\n",
            "Epoch 281/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 282/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0538\n",
            "Epoch 283/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 284/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 285/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 286/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 287/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 288/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 289/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 290/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 291/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 292/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0541\n",
            "Epoch 293/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 294/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 295/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0541\n",
            "Epoch 296/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 297/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 298/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 299/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 300/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 301/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0540\n",
            "Epoch 302/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 303/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 304/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 305/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 306/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 307/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 308/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 309/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0540\n",
            "Epoch 310/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 311/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 312/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 313/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 314/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 315/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 316/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 317/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 318/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 319/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 320/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 321/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 322/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 323/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 324/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 325/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 326/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 327/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 328/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 329/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 330/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 331/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 332/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 333/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 334/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 335/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 336/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 337/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 338/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 339/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 340/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 341/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 342/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 343/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 344/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 345/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 346/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 347/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 348/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 349/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 350/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 351/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 352/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 353/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 354/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 355/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 356/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 357/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 358/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 359/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 360/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 361/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 362/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 363/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 364/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 365/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 366/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 367/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 368/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 369/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 370/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 371/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 372/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 373/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 374/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 375/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 376/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 377/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 378/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 379/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 380/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 381/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 382/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 383/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 384/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 385/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 386/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 387/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 388/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 389/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 390/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 391/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 392/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 393/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 394/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 395/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 396/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 397/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 398/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 399/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 400/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 401/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 402/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 403/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 404/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 405/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 406/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 407/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0541\n",
            "Epoch 408/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 409/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 410/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 411/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 412/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0540\n",
            "Epoch 413/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 414/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 415/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 416/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 417/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 418/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 419/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 420/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0538\n",
            "Epoch 421/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 422/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 423/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 424/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 425/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 426/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 427/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 428/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 429/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 430/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 431/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 432/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 433/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 434/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 435/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 436/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 437/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 438/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 439/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 440/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 441/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 442/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 443/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 444/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 445/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0542\n",
            "Epoch 446/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0579 - val_loss: 0.0539\n",
            "Epoch 447/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 448/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 449/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 450/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 451/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 452/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 453/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 454/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 455/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0540\n",
            "Epoch 456/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 457/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 458/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 459/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 460/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 461/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 462/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 463/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 464/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 465/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 466/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 467/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 468/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 469/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 470/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 471/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 472/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 473/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 474/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 475/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 476/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 477/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 478/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 479/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 480/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 481/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 482/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 483/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 484/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 485/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 486/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 487/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 488/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 489/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 490/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 491/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 492/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 493/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 494/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 495/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 496/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 497/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 498/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 499/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 500/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 501/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 502/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 503/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 504/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 505/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 506/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 507/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 508/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 509/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 510/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 511/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 512/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 513/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 514/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 515/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 516/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 517/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 518/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 519/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 520/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 521/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 522/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 523/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 524/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 525/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 526/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 527/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 528/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 529/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 530/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 531/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 532/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 533/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 534/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 535/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 536/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 537/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 538/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 539/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 540/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 541/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 542/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 543/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 544/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 545/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 546/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 547/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 548/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 549/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 550/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 551/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 552/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 553/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 554/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 555/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 556/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 557/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 558/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 559/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 560/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 561/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 562/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 563/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 564/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 565/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 566/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 567/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 568/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 569/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 570/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0541\n",
            "Epoch 571/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 572/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 573/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 574/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 575/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 576/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 577/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 578/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 579/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 580/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 581/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 582/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 583/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 584/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 585/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 586/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 587/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 588/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 589/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 590/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 591/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 592/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 593/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 594/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 595/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 596/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 597/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 598/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 599/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 600/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 601/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 602/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 603/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 604/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 605/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 606/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 607/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 608/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 609/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 610/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 611/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 612/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 613/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 614/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 615/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 616/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 617/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 618/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 619/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 620/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 621/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 622/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 623/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 624/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 625/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 626/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 627/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 628/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 629/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 630/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 631/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 632/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 633/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 634/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 635/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 636/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 637/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 638/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 639/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 640/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 641/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 642/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 643/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 644/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 645/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 646/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 647/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 648/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 649/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 650/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 651/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 652/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 653/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 654/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 655/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0537\n",
            "Epoch 656/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0537\n",
            "Epoch 657/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 658/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 659/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 660/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 661/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 662/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 663/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0539\n",
            "Epoch 664/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 665/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 666/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 667/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 668/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 669/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 670/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 671/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 672/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 673/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 674/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 675/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 676/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 677/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 678/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 679/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 680/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 681/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 682/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 683/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 684/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 685/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 686/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 687/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 688/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 689/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 690/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 691/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 692/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 693/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 694/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 695/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 696/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 697/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 698/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 699/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 700/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 701/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 702/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 703/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 704/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 705/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 706/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 707/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 708/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 709/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 710/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 711/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 712/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 713/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 714/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 715/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 716/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 717/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 718/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 719/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 720/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 721/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 722/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.0538\n",
            "Epoch 723/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 724/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 725/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 726/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 727/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 728/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 729/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 730/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 731/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 732/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 733/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 734/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 735/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 736/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 737/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 738/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 739/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0538\n",
            "Epoch 740/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 741/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 742/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 743/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 744/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 745/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0538\n",
            "Epoch 746/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 747/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 748/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 749/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0538\n",
            "Epoch 750/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0538\n",
            "Epoch 751/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 752/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 753/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 754/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 755/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 756/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 757/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 758/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 759/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 760/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 761/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 762/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 763/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 764/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 765/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 766/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 767/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 768/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 769/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 770/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 771/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 772/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 773/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 774/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 775/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 776/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 777/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 778/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 779/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 780/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 781/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 782/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 783/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 784/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 785/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 786/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 787/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 788/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 789/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 790/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 791/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0539\n",
            "Epoch 792/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 793/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 794/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 795/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 796/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 797/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 798/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 799/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0577 - val_loss: 0.0540\n",
            "Epoch 800/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 801/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 802/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 803/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 804/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 805/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 806/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 807/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 808/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 809/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 810/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 811/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 812/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 813/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 814/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 815/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 816/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 817/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 818/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 819/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 820/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 821/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 822/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 823/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 824/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 825/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 826/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 827/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 828/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 829/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 830/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 831/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 832/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 833/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 834/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 835/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 836/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 837/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0538\n",
            "Epoch 838/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 839/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 840/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 841/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 842/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 843/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 844/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 845/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 846/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 847/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 848/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 849/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 850/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 851/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 852/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 853/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 854/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 855/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 856/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 857/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 858/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 859/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 860/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 861/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 862/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 863/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 864/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 865/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 866/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 867/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 868/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 869/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 870/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 871/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 872/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 873/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 874/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 875/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 876/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 877/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 878/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 879/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 880/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 881/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 882/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 883/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 884/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 885/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 886/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 887/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 888/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 889/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 890/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0543\n",
            "Epoch 891/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 892/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 893/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 894/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 895/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 896/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 897/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 898/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 899/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 900/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 901/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 902/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 903/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 904/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 905/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 906/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 907/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 908/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 909/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 910/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 911/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 912/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 913/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 914/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 915/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 916/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 917/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 918/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 919/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 920/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 921/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 922/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 923/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 924/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 925/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 926/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 927/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 928/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 929/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 930/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 931/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 932/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 933/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 934/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0538\n",
            "Epoch 935/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 936/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 937/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 938/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 939/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 940/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 941/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0539\n",
            "Epoch 942/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 943/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 944/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 945/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 946/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 947/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 948/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 949/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 950/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 951/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 952/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 953/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 954/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 955/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 956/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 957/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 958/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 959/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 960/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 961/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 962/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 963/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 964/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 965/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 966/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 967/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 968/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 969/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 970/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 971/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 972/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 973/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 974/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 975/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 976/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 977/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 978/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 979/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 980/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 981/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 982/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 983/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 984/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0539\n",
            "Epoch 985/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 986/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n",
            "Epoch 987/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 988/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 989/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 990/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0542\n",
            "Epoch 991/1000\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0541\n",
            "Epoch 992/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 993/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 994/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0540\n",
            "Epoch 995/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0542\n",
            "Epoch 996/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 997/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 998/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0541\n",
            "Epoch 999/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.0543\n",
            "Epoch 1000/1000\n",
            "86/86 [==============================] - 0s 1ms/step - loss: 0.0576 - val_loss: 0.0540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRSpikiH5PNd",
        "outputId": "a39e43aa-bf90-42d3-b896-a1eff5c4ca3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "history_lstm_df = pd.DataFrame(history_lstm.history)\n",
        "history_lstm_df['epoch'] = history_lstm.epoch\n",
        "history_lstm_df.sort_values(by='val_loss', ascending=True)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>epoch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>654</th>\n",
              "      <td>0.057745</td>\n",
              "      <td>0.053743</td>\n",
              "      <td>654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>655</th>\n",
              "      <td>0.057690</td>\n",
              "      <td>0.053749</td>\n",
              "      <td>655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>663</th>\n",
              "      <td>0.057728</td>\n",
              "      <td>0.053759</td>\n",
              "      <td>663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>660</th>\n",
              "      <td>0.057707</td>\n",
              "      <td>0.053759</td>\n",
              "      <td>660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>0.057887</td>\n",
              "      <td>0.053761</td>\n",
              "      <td>276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>0.057958</td>\n",
              "      <td>0.054285</td>\n",
              "      <td>193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>0.058039</td>\n",
              "      <td>0.054479</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>0.057972</td>\n",
              "      <td>0.054538</td>\n",
              "      <td>103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>0.057822</td>\n",
              "      <td>0.054564</td>\n",
              "      <td>143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.065071</td>\n",
              "      <td>0.054869</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         loss  val_loss  epoch\n",
              "654  0.057745  0.053743    654\n",
              "655  0.057690  0.053749    655\n",
              "663  0.057728  0.053759    663\n",
              "660  0.057707  0.053759    660\n",
              "276  0.057887  0.053761    276\n",
              "..        ...       ...    ...\n",
              "193  0.057958  0.054285    193\n",
              "77   0.058039  0.054479     77\n",
              "103  0.057972  0.054538    103\n",
              "143  0.057822  0.054564    143\n",
              "0    0.065071  0.054869      0\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNcRqw5c6Jig",
        "outputId": "33fab368-b853-4fb5-eaf3-4505b61fd307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "plot_loss_new(history_lstm)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Minimum Loss             : 875    0.057459\n",
            "Name: loss, dtype: float64\n",
            "\n",
            "Minimum Validation Loss  : 654    0.053743\n",
            "Name: val_loss, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJTVa3IU6PrT"
      },
      "source": [
        "## **Kesimpulan**\n",
        "\n",
        "Sweet spot val_loss pada tiap model\n",
        "\n",
        "1. Baseline model : epoch 161 dengan val_loss 0.054413\n",
        "2. Deeper Model : epoch 109 dengan val_loss 0.053939\n",
        "3. Wider Model : epoch 94 dengan val_loss 0.05374\n",
        "4. LSTM Model : epoch 652 dengan val_loss 0.053743\n",
        "\n",
        "Arsitektur terbaik adalah arsitektur dengan Deep Learning Wider Model dan Deep Learning LSTM Model karena memiliki nilai val_loss yang paling kecil dibanding model lainnya\n"
      ]
    }
  ]
}